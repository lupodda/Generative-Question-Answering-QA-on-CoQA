{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1d23b6c"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Authors**:\n",
        "* Francesca Boccardi, francesca.boccardi@studio.unibo.it\n",
        "* Luigi Podda, luigi.podda@studio.unibo.it\n",
        "* Matteo Nestola, matteo.nestola@studio.unibo.it\n",
        "\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Keywords**: Transformers, Question Answering, CoQA"
      ],
      "id": "d1d23b6c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd3f451b"
      },
      "source": [
        "## Deadlines\n",
        "\n",
        "* **December 11**, 2022: deadline for having assignments graded by January 11, 2023\n",
        "* **January 11**, 2023: deadline for half-point speed bonus per assignment\n",
        "* **After January 11**, 2023: assignments are still accepted, but there will be no speed bonus"
      ],
      "id": "bd3f451b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11ada8c8"
      },
      "source": [
        "# Overview"
      ],
      "id": "11ada8c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47c07553"
      },
      "source": [
        "### Problem\n",
        "\n",
        "Question Answering (QA) on [CoQA](https://stanfordnlp.github.io/coqa/) dataset: a conversational QA dataset."
      ],
      "id": "47c07553"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4907f8d"
      },
      "source": [
        "### Task\n",
        "\n",
        "Given a question $Q$, a text passage $P$, the task is to generate the answer $A$.<br>\n",
        "$\\rightarrow A$ can be: (i) a free-form text or (ii) unanswerable;\n",
        "\n",
        "**Note**: an question $Q$ can refer to previous dialogue turns. <br>\n",
        "$\\rightarrow$ dialogue history $H$ may be a valuable input to provide the correct answer $A$."
      ],
      "id": "b4907f8d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b3760b5"
      },
      "source": [
        "### Models\n",
        "\n",
        "We are going to experiment with transformer-based models to define the following models:\n",
        "\n",
        "1.  $A = f_\\theta(Q, P)$\n",
        "\n",
        "2. $A = f_\\theta(Q, P, H)$\n",
        "\n",
        "where $f_\\theta$ is the transformer-based model we have to define with $\\theta$ parameters."
      ],
      "id": "9b3760b5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66cfee64"
      },
      "source": [
        "# The CoQA dataset"
      ],
      "id": "66cfee64"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "996fa650"
      },
      "source": [
        "<center>\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=16vrgyfoV42Z2AQX0QY7LHTfrgektEKKh\" width=\"750\"/>\n",
        "</center>"
      ],
      "id": "996fa650"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6e3e7d0"
      },
      "source": [
        "For detailed information about the dataset, feel free to check the original [paper](https://arxiv.org/pdf/1808.07042.pdf).\n",
        "\n"
      ],
      "id": "f6e3e7d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfb6c37e"
      },
      "source": [
        "### Rationales\n",
        "\n",
        "Each QA pair is paired with a rationale $R$: it is a text span extracted from the given text passage $P$. <br>\n",
        "$\\rightarrow$ $R$ is not a requested output, but it can be used as an additional information at training time!"
      ],
      "id": "bfb6c37e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daa786e2"
      },
      "source": [
        "### Dataset Statistics\n",
        "\n",
        "* **127k** QA pairs.\n",
        "* **8k** conversations.\n",
        "* **7** diverse domains: Children's Stories, Literature, Mid/High School Exams, News, Wikipedia, Reddit, Science.\n",
        "* Average conversation length: **15 turns** (i.e., QA pairs).\n",
        "* Almost **half** of CoQA questions refer back to **conversational history**.\n",
        "* Only **train** and **validation** sets are available."
      ],
      "id": "daa786e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d26d68b7"
      },
      "source": [
        "### Dataset snippet\n",
        "\n",
        "The dataset is stored in JSON format. Each dialogue is represented as follows:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"source\": \"mctest\",\n",
        "    \"id\": \"3dr23u6we5exclen4th8uq9rb42tel\",\n",
        "    \"filename\": \"mc160.test.41\",\n",
        "    \"story\": \"Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton.\n",
        "    Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. [...]\" % <-- $P$\n",
        "    \"questions\": [\n",
        "        {\n",
        "            \"input_text\": \"What color was Cotton?\",   % <-- $Q_1$\n",
        "            \"turn_id\": 1\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"Where did she live?\",\n",
        "            \"turn_id\": 2\n",
        "        },\n",
        "        [...]\n",
        "    ],\n",
        "    \"answers\": [\n",
        "        {\n",
        "            \"span_start\": 59,   % <-- $R_1$ start index\n",
        "            \"spand_end\": 93,    % <-- $R_1$ end index\n",
        "            \"span_text\": \"a little white kitten named Cotton\",   % <-- $R_1$\n",
        "            \"input_text\" \"white\",   % <-- $A_1$      \n",
        "            \"turn_id\": 1\n",
        "        },\n",
        "        [...]\n",
        "    ]\n",
        "}\n",
        "```"
      ],
      "id": "d26d68b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72c7558c"
      },
      "source": [
        "### Simplifications\n",
        "\n",
        "Each dialogue also contains an additional field ```additional_answers```. For simplicity, we **ignore** this field and only consider one groundtruth answer $A$ and text rationale $R$.\n",
        "\n",
        "CoQA only contains 1.3% of unanswerable questions. For simplicity, we **ignore** those QA pairs."
      ],
      "id": "72c7558c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions"
      ],
      "metadata": {
        "id": "UEEVjcYD3tnD"
      },
      "id": "UEEVjcYD3tnD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e01cdad7"
      },
      "source": [
        "## [Task 1] Remove unaswerable QA pairs\n",
        "\n",
        "Write your own script to remove unaswerable QA pairs from both train and validation sets."
      ],
      "id": "e01cdad7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57334e0"
      },
      "source": [
        "## [Task 2] Train, Validation and Test splits\n",
        "\n",
        "CoQA only provides a train and validation set since the test set is hidden for evaluation purposes.\n",
        "\n",
        "We'll consider the provided validation set as a test set. <br>\n",
        "$\\rightarrow$ Write your own script to:\n",
        "* Split the train data in train and validation splits (80% train and 20% val)\n",
        "* Perform splits such that a dialogue appears in one split only! (i.e., split at dialogue level)\n",
        "* Perform splitting using the following seed for reproducibility: 42\n",
        "\n",
        "#### Reproducibility Memo\n",
        "\n",
        "Check back tutorial 2 on how to fix a specific random seed for reproducibility!"
      ],
      "id": "f57334e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "230a21de"
      },
      "source": [
        "## [Task 3] Model definition\n",
        "\n",
        "Write your own script to define the following transformer-based models from [huggingface](https://HuggingFace.co/).\n",
        "\n",
        "* [M1] DistilRoBERTa (distilberta-base)\n",
        "* [M2] BERTTiny (bert-tiny)\n",
        "\n",
        "**Note**: Remember to install the ```transformers``` python package!\n",
        "\n",
        "**Note**: We consider small transformer models for computational reasons!"
      ],
      "id": "230a21de"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1e83f28"
      },
      "source": [
        "## [Task 4] Question generation with text passage $P$ and question $Q$\n",
        "\n",
        "We want to define $f_\\theta(P, Q)$.\n",
        "\n",
        "Write your own script to implement $f_\\theta$ for each model: M1 and M2.\n",
        "\n",
        "#### Formulation\n",
        "\n",
        "Consider a dialogue on text passage $P$.\n",
        "\n",
        "For each question $Q_i$ at dialogue turn $i$, your model should take $P$ and $Q_i$ and generate $A_i$."
      ],
      "id": "f1e83f28"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7311ba86"
      },
      "source": [
        "## [Task 5] Question generation with text passage $P$, question $Q$ and dialogue history $H$\n",
        "\n",
        "We want to define $f_\\theta(P, Q, H)$. Write your own script to implement $f_\\theta$ for each model: M1 and M2.\n",
        "\n",
        "#### Formulation\n",
        "\n",
        "Consider a dialogue on text passage $P$.\n",
        "\n",
        "For each question $Q_i$ at dialogue turn $i$, your model should take $P$, $Q_i$, and $H = \\{ Q_0, A_0, \\dots, Q_{i-1}, A_{i-1} \\}$ to generate $A_i$."
      ],
      "id": "7311ba86"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5ac768c"
      },
      "source": [
        "## [Task 6] Train and evaluate $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$\n",
        "\n",
        "Write your own script to train and evaluate your $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$ models.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "* Perform multiple train/evaluation seed runs: [42, 2022, 1337].$^1$\n",
        "* Evaluate your models with the following metrics: SQUAD F1-score.$^2$\n",
        "* Fine-tune each transformer-based models for **3 epochs**.\n",
        "* Report evaluation SQUAD F1-score computed on the validation and test sets.\n",
        "\n",
        "$^1$ Remember what we said about code reproducibility in Tutorial 2!\n",
        "\n",
        "$^2$ You can use ```allennlp``` python package for a quick implementation of SQUAD F1-score: ```from allennlp_models.rc.tools import squad```."
      ],
      "id": "b5ac768c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92c7e98f"
      },
      "source": [
        "## [Task 7] Error Analysis\n",
        "\n",
        "Perform a simple and short error analysis as follows:\n",
        "* Group dialogues by ```source``` and report the worst 5 model errors for each source (w.r.t. SQUAD F1-score).\n",
        "* Inspect observed results and try to provide some comments (e.g., do the models make errors when faced with a particular question type?)$^1$\n",
        "\n",
        "$^1$ Check the [paper](https://arxiv.org/pdf/1808.07042.pdf) for some valuable information about question/answer types (e.g., Table 6, Table 8)"
      ],
      "id": "92c7e98f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1814004"
      },
      "source": [
        "## Assignment Evaluation\n",
        "\n",
        "The following assignment points will be awarded for each task as follows:\n",
        "\n",
        "* Task 1, Pre-processing $\\rightarrow$ 0.5 points.\n",
        "* Task 2, Dataset Splitting $\\rightarrow$ 0.5 points.\n",
        "* Task 3 and 4, Models Definition $\\rightarrow$ 1.0 points.\n",
        "* Task 5 and 6, Models Training and Evaluation $\\rightarrow$ 2.0 points.\n",
        "* Task 7, Analysis $\\rightarrow$ 1.0 points.\n",
        "* Report $\\rightarrow$ 1.0 points.\n",
        "\n",
        "**Total** = 6 points <br>\n",
        "\n",
        "We may award an additional 0.5 points for outstanding submissions.\n",
        "\n",
        "**Speed Bonus** = 0.5 extra points <br>"
      ],
      "id": "f1814004"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20a1b2b9"
      },
      "source": [
        "## Report\n",
        "\n",
        "We apply the rules described in Assignment 1 regarding the report.\n",
        "* Write a clear and concise report following the given overleaf template (**max 2 pages**).\n",
        "* Report validation and test results in a table.$^1$\n",
        "* **Avoid reporting** code snippets or copy-paste terminal outputs $\\rightarrow$ **Provide a clean schema** of what you want to show"
      ],
      "id": "20a1b2b9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0967c209"
      },
      "source": [
        "## Comments and Organization\n",
        "\n",
        "Remember to properly comment your code (it is not necessary to comment each single line) and don't forget to describe your work!\n",
        "\n",
        "Structure your code for readability and maintenance. If you work with Colab, use sections.\n",
        "\n",
        "This allows you to build clean and modular code, as well as easy to read and to debug (notebooks can be quite tricky time to time)."
      ],
      "id": "0967c209"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23929660"
      },
      "source": [
        "## FAQ (READ THIS!)\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Does Task 3 also include data tokenization and conversion step?\n",
        "\n",
        "**Answer:** Yes! These steps are usually straightforward since ```transformers``` also offers a specific tokenizer for each model.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "```\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_text = tokenizer(text)\n",
        "%% Alternatively\n",
        "inputs = tokenizer.tokenize(text, add_special_tokens=True, max_length=min(max_length, 512))\n",
        "input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
        "```\n",
        "\n",
        "**Suggestion**: Hugginface's documentation is full of tutorials and user-friendly APIs.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**Question**: I'm hitting **out of memory error** when training my models, do you have any suggestions?\n",
        "\n",
        "**Answer**: Here are some common workarounds:\n",
        "\n",
        "1. Try decreasing the mini-batch size\n",
        "2. Try applying a different padding strategy (if you are applying padding): e.g. use quantiles instead of maximum sequence length\n",
        "\n",
        "---\n",
        "---"
      ],
      "id": "23929660"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c56a612"
      },
      "source": [
        "## Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it"
      ],
      "id": "9c56a612"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54bac4b9"
      },
      "source": [
        "## The End!\n",
        "\n",
        "Questions?"
      ],
      "id": "54bac4b9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install & import\n"
      ],
      "metadata": {
        "id": "YBnRBeC5NUEQ"
      },
      "id": "YBnRBeC5NUEQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section are installed and imported all the useful libraries and packages."
      ],
      "metadata": {
        "id": "2ipK1HNH0vmx"
      },
      "id": "2ipK1HNH0vmx"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install allennlp_models\n",
        "!pip install jedi\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "WdEx7rbCNTap"
      },
      "id": "WdEx7rbCNTap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from datasets import Dataset\n",
        "import transformers\n",
        "from transformers import EncoderDecoderModel, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from allennlp_models.rc.tools import squad\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xRbtzBzJNlvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ab56fa-84e1-47e7-ca08-2606eaf1de9e"
      },
      "id": "xRbtzBzJNlvO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `set_reproducibility` function allows to make data pipeline reproducible and to train and evaluate the models using multiple and fixed seeds."
      ],
      "metadata": {
        "id": "7klXV8P81ii1"
      },
      "id": "7klXV8P81ii1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fafy9RlQLvYt"
      },
      "outputs": [],
      "source": [
        "def set_reproducibility(seed):\n",
        "\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(seed)"
      ],
      "id": "fafy9RlQLvYt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6643e14"
      },
      "source": [
        "# Data Download\n"
      ],
      "id": "f6643e14"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first download the dataset used in this work, [CoQA-train](https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json) and [CoQA-test](https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json)."
      ],
      "metadata": {
        "id": "UKOsOZShD8tW"
      },
      "id": "UKOsOZShD8tW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "358bac70"
      },
      "outputs": [],
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "def download_data(data_path, url_path, suffix):\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
        "        download_url(url=url_path, output_path=data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "dataset_path = os.path.join(os.getcwd(),\"coqa\")"
      ],
      "id": "358bac70"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f6ab3ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ac7d87-5ca9-4198-dbd5-f9debcc178ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CoQA train data split... (it may take a while)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "coqa-train-v1.0.json: 49.0MB [00:06, 8.01MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed!\n",
            "Downloading CoQA test data split... (it may take a while)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "coqa-dev-v1.0.json: 9.09MB [00:01, 7.07MB/s]                            "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train data\n",
        "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
        "train_path = os.path.join(dataset_path,\"train.json\")\n",
        "\n",
        "# Test data\n",
        "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path=test_url, suffix='test')\n",
        "test_path = os.path.join(dataset_path,\"test.json\")"
      ],
      "id": "5f6ab3ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40e42311"
      },
      "source": [
        "## Data Inspection"
      ],
      "id": "40e42311"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's then check accurately the dataset format, in order to better understand how to retrieve the task inputs and outputs."
      ],
      "metadata": {
        "id": "7QPY2TwmovyR"
      },
      "id": "7QPY2TwmovyR"
    },
    {
      "cell_type": "code",
      "source": [
        "with open(train_path, 'r') as f:\n",
        "  train_data = json.loads(f.read())\n",
        "\n",
        "with open(test_path, 'r') as f:\n",
        "  test_data = json.loads(f.read())"
      ],
      "metadata": {
        "id": "t_8vPqWtP_hJ"
      },
      "id": "t_8vPqWtP_hJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check train data format\n",
        "\n",
        "sample_train = train_data['data'][0]\n",
        "print(json.dumps(sample_train, indent = 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNUu9XBh1kvE",
        "outputId": "97544f94-3640-4083-b6dd-edbc86e84d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"source\": \"wikipedia\",\n",
            "  \"id\": \"3zotghdk5ibi9cex97fepx7jetpso7\",\n",
            "  \"filename\": \"Vatican_Library.txt\",\n",
            "  \"story\": \"The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \\n\\nThe Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \\n\\nIn March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \\n\\nThe Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \\n\\nScholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \\n\\nThe Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.\",\n",
            "  \"questions\": [\n",
            "    {\n",
            "      \"input_text\": \"When was the Vat formally opened?\",\n",
            "      \"turn_id\": 1\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"what is the library for?\",\n",
            "      \"turn_id\": 2\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"for what subjects?\",\n",
            "      \"turn_id\": 3\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"and?\",\n",
            "      \"turn_id\": 4\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"what was started in 2014?\",\n",
            "      \"turn_id\": 5\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"how do scholars divide the library?\",\n",
            "      \"turn_id\": 6\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"how many?\",\n",
            "      \"turn_id\": 7\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"what is the official name of the Vat?\",\n",
            "      \"turn_id\": 8\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"where is it?\",\n",
            "      \"turn_id\": 9\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"how many printed books does it contain?\",\n",
            "      \"turn_id\": 10\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"when were the Secret Archives moved from the rest of the library?\",\n",
            "      \"turn_id\": 11\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"how many items are in this secret collection?\",\n",
            "      \"turn_id\": 12\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"Can anyone use this library?\",\n",
            "      \"turn_id\": 13\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"what must be requested to view?\",\n",
            "      \"turn_id\": 14\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"what must be requested in person or by mail?\",\n",
            "      \"turn_id\": 15\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"of what books?\",\n",
            "      \"turn_id\": 16\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"What is the Vat the library of?\",\n",
            "      \"turn_id\": 17\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"How many books survived the Pre Lateran period?\",\n",
            "      \"turn_id\": 18\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"what is the point of the project started in 2014?\",\n",
            "      \"turn_id\": 19\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"what will this allow?\",\n",
            "      \"turn_id\": 20\n",
            "    }\n",
            "  ],\n",
            "  \"answers\": [\n",
            "    {\n",
            "      \"span_start\": 151,\n",
            "      \"span_end\": 179,\n",
            "      \"span_text\": \"Formally established in 1475\",\n",
            "      \"input_text\": \"It was formally established in 1475\",\n",
            "      \"turn_id\": 1\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 454,\n",
            "      \"span_end\": 494,\n",
            "      \"span_text\": \"he Vatican Library is a research library\",\n",
            "      \"input_text\": \"research\",\n",
            "      \"turn_id\": 2\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 457,\n",
            "      \"span_end\": 511,\n",
            "      \"span_text\": \"Vatican Library is a research library for history, law\",\n",
            "      \"input_text\": \"history, and law\",\n",
            "      \"turn_id\": 3\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 457,\n",
            "      \"span_end\": 545,\n",
            "      \"span_text\": \"Vatican Library is a research library for history, law, philosophy, science and theology\",\n",
            "      \"input_text\": \"philosophy, science and theology\",\n",
            "      \"turn_id\": 4\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 769,\n",
            "      \"span_end\": 879,\n",
            "      \"span_text\": \"March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts\",\n",
            "      \"input_text\": \"a  project\",\n",
            "      \"turn_id\": 5\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 1048,\n",
            "      \"span_end\": 1127,\n",
            "      \"span_text\": \"Scholars have traditionally divided the history of the library into five period\",\n",
            "      \"input_text\": \"into periods\",\n",
            "      \"turn_id\": 6\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 1048,\n",
            "      \"span_end\": 1128,\n",
            "      \"span_text\": \"Scholars have traditionally divided the history of the library into five periods\",\n",
            "      \"input_text\": \"five\",\n",
            "      \"turn_id\": 7\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 4,\n",
            "      \"span_end\": 94,\n",
            "      \"span_text\": \"Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, \",\n",
            "      \"input_text\": \"The Vatican Apostolic Library\",\n",
            "      \"turn_id\": 8\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 94,\n",
            "      \"span_end\": 150,\n",
            "      \"span_text\": \"is the library of the Holy See, located in Vatican City.\",\n",
            "      \"input_text\": \"in Vatican City\",\n",
            "      \"turn_id\": 9\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 328,\n",
            "      \"span_end\": 412,\n",
            "      \"span_text\": \" It has 75,000 codices from throughout history, as well as 1.1 million printed books\",\n",
            "      \"input_text\": \"1.1 million\",\n",
            "      \"turn_id\": 10\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 917,\n",
            "      \"span_end\": 1009,\n",
            "      \"span_text\": \"atican Secret Archives were separated from the library at the beginning of the 17th century;\",\n",
            "      \"input_text\": \"at the beginning of the 17th century;\",\n",
            "      \"turn_id\": 11\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 915,\n",
            "      \"span_end\": 1046,\n",
            "      \"span_text\": \" Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \",\n",
            "      \"input_text\": \"150,000\",\n",
            "      \"turn_id\": 12\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 546,\n",
            "      \"span_end\": 643,\n",
            "      \"span_text\": \" The Vatican Library is open to anyone who can document their qualifications and research needs. \",\n",
            "      \"input_text\": \"anyone who can document their qualifications and research needs.\",\n",
            "      \"turn_id\": 13\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": -1,\n",
            "      \"span_end\": -1,\n",
            "      \"span_text\": \"unknown\",\n",
            "      \"input_text\": \"unknown\",\n",
            "      \"turn_id\": 14,\n",
            "      \"bad_turn\": \"true\"\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 643,\n",
            "      \"span_end\": 764,\n",
            "      \"span_text\": \"Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \",\n",
            "      \"input_text\": \"Photocopies\",\n",
            "      \"turn_id\": 15\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 644,\n",
            "      \"span_end\": 724,\n",
            "      \"span_text\": \"hotocopies for private study of pages from books published between 1801 and 1990\",\n",
            "      \"input_text\": \"only books published between 1801 and 1990\",\n",
            "      \"turn_id\": 16\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 78,\n",
            "      \"span_end\": 125,\n",
            "      \"span_text\": \"simply the Vat, is the library of the Holy See,\",\n",
            "      \"input_text\": \"the Holy See\",\n",
            "      \"turn_id\": 17\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 1192,\n",
            "      \"span_end\": 1384,\n",
            "      \"span_text\": \"Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant\",\n",
            "      \"input_text\": \"a handful of volumes\",\n",
            "      \"turn_id\": 18\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 785,\n",
            "      \"span_end\": 881,\n",
            "      \"span_text\": \"Vatican Library began an initial four-year project of digitising its collection of manuscripts, \",\n",
            "      \"input_text\": \"digitising manuscripts\",\n",
            "      \"turn_id\": 19\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 868,\n",
            "      \"span_end\": 910,\n",
            "      \"span_text\": \"manuscripts, to be made available online. \",\n",
            "      \"input_text\": \"them to be viewed online.\",\n",
            "      \"turn_id\": 20\n",
            "    }\n",
            "  ],\n",
            "  \"name\": \"Vatican_Library.txt\"\n",
            "}\n"
          ]
        }
      ],
      "id": "PNUu9XBh1kvE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check test data format\n",
        "\n",
        "sample_test = test_data['data'][0]\n",
        "print(json.dumps(sample_test, indent = 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTlhAMhM526c",
        "outputId": "5f52cebf-38f7-4b9a-d732-cbe05a512e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"source\": \"mctest\",\n",
            "  \"id\": \"3dr23u6we5exclen4th8uq9rb42tel\",\n",
            "  \"filename\": \"mc160.test.41\",\n",
            "  \"story\": \"Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \\n\\n\\\"What are you doing, Cotton?!\\\" \\n\\n\\\"I only wanted to be more like you\\\". \\n\\nCotton's mommy rubbed her face on Cotton's and said \\\"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\\\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \\n\\n\\\"Don't ever do that again, Cotton!\\\" they all cried. \\\"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\\\" \\n\\nThen Cotton thought, \\\"I change my mind. I like being special\\\".\",\n",
            "  \"questions\": [\n",
            "    {\n",
            "      \"input_text\": \"What color was Cotton?\",\n",
            "      \"turn_id\": 1\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"Where did she live?\",\n",
            "      \"turn_id\": 2\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"Did she live alone?\",\n",
            "      \"turn_id\": 3\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"Who did she live with?\",\n",
            "      \"turn_id\": 4\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"What color were her sisters?\",\n",
            "      \"turn_id\": 5\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"Was Cotton happy that she looked different than the rest of her family?\",\n",
            "      \"turn_id\": 6\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"What did she do to try to make herself the same color as her sisters?\",\n",
            "      \"turn_id\": 7\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"Whose paint was it?\",\n",
            "      \"turn_id\": 8\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"What did Cotton's mother and siblings do when they saw her painted orange?\",\n",
            "      \"turn_id\": 9\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"Where did Cotton's mother put her to clean the paint off?\",\n",
            "      \"turn_id\": 10\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"What did the other cats do when Cotton emerged from the bucket of water?\",\n",
            "      \"turn_id\": 11\n",
            "    },\n",
            "    {\n",
            "      \"input_text\": \"Did they want Cotton to change the color of her fur?\",\n",
            "      \"turn_id\": 12\n",
            "    }\n",
            "  ],\n",
            "  \"answers\": [\n",
            "    {\n",
            "      \"span_start\": 59,\n",
            "      \"span_end\": 93,\n",
            "      \"span_text\": \"a little white kitten named Cotton\",\n",
            "      \"input_text\": \"white\",\n",
            "      \"turn_id\": 1\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 18,\n",
            "      \"span_end\": 80,\n",
            "      \"span_text\": \"in a barn near a farm house, there lived a little white kitten\",\n",
            "      \"input_text\": \"in a barn\",\n",
            "      \"turn_id\": 2\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 196,\n",
            "      \"span_end\": 215,\n",
            "      \"span_text\": \"Cotton wasn't alone\",\n",
            "      \"input_text\": \"no\",\n",
            "      \"turn_id\": 3\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 281,\n",
            "      \"span_end\": 315,\n",
            "      \"span_text\": \"with her mommy and 5 other sisters\",\n",
            "      \"input_text\": \"with her mommy and 5 sisters\",\n",
            "      \"turn_id\": 4\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 428,\n",
            "      \"span_end\": 490,\n",
            "      \"span_text\": \"her sisters were all orange with beautiful white tiger stripes\",\n",
            "      \"input_text\": \"orange and white\",\n",
            "      \"turn_id\": 5\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 512,\n",
            "      \"span_end\": 549,\n",
            "      \"span_text\": \"Being different made Cotton quite sad\",\n",
            "      \"input_text\": \"no\",\n",
            "      \"turn_id\": 6\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 678,\n",
            "      \"span_end\": 716,\n",
            "      \"span_text\": \"she used it to paint herself like them\",\n",
            "      \"input_text\": \"she painted herself\",\n",
            "      \"turn_id\": 7\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 647,\n",
            "      \"span_end\": 676,\n",
            "      \"span_text\": \"the old farmer's orange paint\",\n",
            "      \"input_text\": \"the farmer\",\n",
            "      \"turn_id\": 8\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 718,\n",
            "      \"span_end\": 776,\n",
            "      \"span_text\": \"When her mommy and sisters found her they started laughing\",\n",
            "      \"input_text\": \"they started laughing\",\n",
            "      \"turn_id\": 9\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 1035,\n",
            "      \"span_end\": 1097,\n",
            "      \"span_text\": \"mommy picked her up and dropped her into a big bucket of water\",\n",
            "      \"input_text\": \"a bucket of water\",\n",
            "      \"turn_id\": 10\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 1143,\n",
            "      \"span_end\": 1170,\n",
            "      \"span_text\": \"Her sisters licked her face\",\n",
            "      \"input_text\": \"licked her face\",\n",
            "      \"turn_id\": 11\n",
            "    },\n",
            "    {\n",
            "      \"span_start\": 965,\n",
            "      \"span_end\": 1008,\n",
            "      \"span_text\": \"We would never want you to be any other way\",\n",
            "      \"input_text\": \"no\",\n",
            "      \"turn_id\": 12\n",
            "    }\n",
            "  ],\n",
            "  \"additional_answers\": {\n",
            "    \"0\": [\n",
            "      {\n",
            "        \"span_start\": 68,\n",
            "        \"span_end\": 93,\n",
            "        \"span_text\": \"white kitten named Cotton\",\n",
            "        \"input_text\": \"white\",\n",
            "        \"turn_id\": 1\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 17,\n",
            "        \"span_end\": 93,\n",
            "        \"span_text\": \" in a barn near a farm house, there lived a little white kitten named Cotton\",\n",
            "        \"input_text\": \"in a barn\",\n",
            "        \"turn_id\": 2\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 192,\n",
            "        \"span_end\": 215,\n",
            "        \"span_text\": \"But Cotton wasn't alone\",\n",
            "        \"input_text\": \"no\",\n",
            "        \"turn_id\": 3\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 258,\n",
            "        \"span_end\": 315,\n",
            "        \"span_text\": \"She shared her hay bed with her mommy and 5 other sisters\",\n",
            "        \"input_text\": \"her mommy and 5 other sisters\",\n",
            "        \"turn_id\": 4\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 416,\n",
            "        \"span_end\": 490,\n",
            "        \"span_text\": \"The rest of her sisters were all orange with beautiful white tiger stripes\",\n",
            "        \"input_text\": \"orange with white tiger stripes\",\n",
            "        \"turn_id\": 5\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 512,\n",
            "        \"span_end\": 549,\n",
            "        \"span_text\": \"Being different made Cotton quite sad\",\n",
            "        \"input_text\": \"no\",\n",
            "        \"turn_id\": 6\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 678,\n",
            "        \"span_end\": 716,\n",
            "        \"span_text\": \"she used it to paint herself like them\",\n",
            "        \"input_text\": \"she painted herself\",\n",
            "        \"turn_id\": 7\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 655,\n",
            "        \"span_end\": 676,\n",
            "        \"span_text\": \"farmer's orange paint\",\n",
            "        \"input_text\": \"the farmer's\",\n",
            "        \"turn_id\": 8\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 718,\n",
            "        \"span_end\": 777,\n",
            "        \"span_text\": \"When her mommy and sisters found her they started laughing.\",\n",
            "        \"input_text\": \"they started laughing\",\n",
            "        \"turn_id\": 9\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 1059,\n",
            "        \"span_end\": 1097,\n",
            "        \"span_text\": \"dropped her into a big bucket of water\",\n",
            "        \"input_text\": \"dropped her into a big bucket of water\",\n",
            "        \"turn_id\": 10\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 1143,\n",
            "        \"span_end\": 1171,\n",
            "        \"span_text\": \"Her sisters licked her face \",\n",
            "        \"input_text\": \"licked her face\",\n",
            "        \"turn_id\": 11\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 1209,\n",
            "        \"span_end\": 1244,\n",
            "        \"span_text\": \"\\\"Don't ever do that again, Cotton!\\\"\",\n",
            "        \"input_text\": \"no\",\n",
            "        \"turn_id\": 12\n",
            "      }\n",
            "    ],\n",
            "    \"1\": [\n",
            "      {\n",
            "        \"span_start\": 68,\n",
            "        \"span_end\": 74,\n",
            "        \"span_text\": \"white \",\n",
            "        \"input_text\": \"white\",\n",
            "        \"turn_id\": 1\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 18,\n",
            "        \"span_end\": 27,\n",
            "        \"span_text\": \"in a barn\",\n",
            "        \"input_text\": \"in a barn\",\n",
            "        \"turn_id\": 2\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 202,\n",
            "        \"span_end\": 215,\n",
            "        \"span_text\": \" wasn't alone\",\n",
            "        \"input_text\": \"No\",\n",
            "        \"turn_id\": 3\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 286,\n",
            "        \"span_end\": 315,\n",
            "        \"span_text\": \"her mommy and 5 other sisters\",\n",
            "        \"input_text\": \"her mommy and 5 other sisters\",\n",
            "        \"turn_id\": 4\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 449,\n",
            "        \"span_end\": 456,\n",
            "        \"span_text\": \"orange \",\n",
            "        \"input_text\": \"orange\",\n",
            "        \"turn_id\": 5\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 546,\n",
            "        \"span_end\": 549,\n",
            "        \"span_text\": \"sad\",\n",
            "        \"input_text\": \"No\",\n",
            "        \"turn_id\": 6\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 693,\n",
            "        \"span_end\": 716,\n",
            "        \"span_text\": \"paint herself like them\",\n",
            "        \"input_text\": \"paint herself like them\",\n",
            "        \"turn_id\": 7\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 647,\n",
            "        \"span_end\": 663,\n",
            "        \"span_text\": \"the old farmer's\",\n",
            "        \"input_text\": \"the old farmer's\",\n",
            "        \"turn_id\": 8\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 866,\n",
            "        \"span_end\": 882,\n",
            "        \"span_text\": \" rubbed her face\",\n",
            "        \"input_text\": \"rubbed her face\",\n",
            "        \"turn_id\": 9\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 1071,\n",
            "        \"span_end\": 1097,\n",
            "        \"span_text\": \"into a big bucket of water\",\n",
            "        \"input_text\": \"into a big bucket of water\",\n",
            "        \"turn_id\": 10\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 1154,\n",
            "        \"span_end\": 1170,\n",
            "        \"span_text\": \" licked her face\",\n",
            "        \"input_text\": \"licked her face\",\n",
            "        \"turn_id\": 11\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 974,\n",
            "        \"span_end\": 1008,\n",
            "        \"span_text\": \"never want you to be any other way\",\n",
            "        \"input_text\": \"No\",\n",
            "        \"turn_id\": 12\n",
            "      }\n",
            "    ],\n",
            "    \"2\": [\n",
            "      {\n",
            "        \"span_start\": 68,\n",
            "        \"span_end\": 94,\n",
            "        \"span_text\": \"white kitten named Cotton.\",\n",
            "        \"input_text\": \"white\",\n",
            "        \"turn_id\": 1\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 17,\n",
            "        \"span_end\": 94,\n",
            "        \"span_text\": \" in a barn near a farm house, there lived a little white kitten named Cotton.\",\n",
            "        \"input_text\": \"in a barn near\",\n",
            "        \"turn_id\": 2\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 191,\n",
            "        \"span_end\": 316,\n",
            "        \"span_text\": \" But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters.\",\n",
            "        \"input_text\": \"no\",\n",
            "        \"turn_id\": 3\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 258,\n",
            "        \"span_end\": 316,\n",
            "        \"span_text\": \"She shared her hay bed with her mommy and 5 other sisters.\",\n",
            "        \"input_text\": \"her mommy and 5 other sisters\",\n",
            "        \"turn_id\": 4\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 416,\n",
            "        \"span_end\": 511,\n",
            "        \"span_text\": \"The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy.\",\n",
            "        \"input_text\": \"orange\",\n",
            "        \"turn_id\": 5\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 512,\n",
            "        \"span_end\": 550,\n",
            "        \"span_text\": \"Being different made Cotton quite sad.\",\n",
            "        \"input_text\": \"no\",\n",
            "        \"turn_id\": 6\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 607,\n",
            "        \"span_end\": 718,\n",
            "        \"span_text\": \" So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. \",\n",
            "        \"input_text\": \"paint herself like them\",\n",
            "        \"turn_id\": 7\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 608,\n",
            "        \"span_end\": 676,\n",
            "        \"span_text\": \"So one day, when Cotton found a can of the old farmer's orange paint\",\n",
            "        \"input_text\": \"the farmer's\",\n",
            "        \"turn_id\": 8\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 718,\n",
            "        \"span_end\": 778,\n",
            "        \"span_text\": \"When her mommy and sisters found her they started laughing. \",\n",
            "        \"input_text\": \"started laughing\",\n",
            "        \"turn_id\": 9\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 1011,\n",
            "        \"span_end\": 1098,\n",
            "        \"span_text\": \"And with that, Cotton's mommy picked her up and dropped her into a big bucket of water.\",\n",
            "        \"input_text\": \"a big bucket of water\",\n",
            "        \"turn_id\": 10\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 1142,\n",
            "        \"span_end\": 1207,\n",
            "        \"span_text\": \" Her sisters licked her face until Cotton's fur was all all dry. \",\n",
            "        \"input_text\": \"licked her face\",\n",
            "        \"turn_id\": 11\n",
            "      },\n",
            "      {\n",
            "        \"span_start\": 1209,\n",
            "        \"span_end\": 1260,\n",
            "        \"span_text\": \"\\\"Don't ever do that again, Cotton!\\\" they all cried.\",\n",
            "        \"input_text\": \"no\",\n",
            "        \"turn_id\": 12\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"name\": \"mc160.test.41\"\n",
            "}\n"
          ]
        }
      ],
      "id": "bTlhAMhM526c"
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_train.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFZva1spQCuf",
        "outputId": "a823a5e5-2bd4-4962-ff88-d09554a8d488"
      },
      "id": "JFZva1spQCuf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['source', 'id', 'filename', 'story', 'questions', 'answers', 'name'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing & dataframe creation"
      ],
      "metadata": {
        "id": "1_zFA5GXfxVM"
      },
      "id": "1_zFA5GXfxVM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make data more readable, training and test sets provided by CoQA in `.json` file are converted into dataframes by the `create_dataframe` function. In particular, for each passage the correspondent set of Q&A pairs is unrolled and arranged over different rows, such that each row contains information about a single Q&A pair turn of a specific passage."
      ],
      "metadata": {
        "id": "ycN_s7jy70ok"
      },
      "id": "ycN_s7jy70ok"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf8aq0O6YBxG"
      },
      "outputs": [],
      "source": [
        "def create_dataframe(path):\n",
        "\n",
        "  \"\"\"\n",
        "  Loads and normalizes the .json file whose `path` is passed as input and returns the correspondent dataframe.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(path, 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "\n",
        "  record_path_q = ['data','questions']\n",
        "  meta_q = [['data','source'], ['data','id'], ['data','filename'], ['data','story']]\n",
        "\n",
        "  df_questions = pd.json_normalize(data, record_path = record_path_q, meta = meta_q)\n",
        "\n",
        "  record_path_a = ['data','answers']\n",
        "  meta_a = [['data','id']]\n",
        "\n",
        "  df_answers = pd.json_normalize(data, record_path = record_path_a, meta = meta_a)\n",
        "\n",
        "  df = df_questions.merge(df_answers, on = ['data.id', 'turn_id'])\n",
        "\n",
        "  return df\n",
        "\n",
        "df_train = create_dataframe(train_path)\n",
        "df_test = create_dataframe(test_path)"
      ],
      "id": "Tf8aq0O6YBxG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, columns of the train and test dataframes are renamed and sorted meaningfully."
      ],
      "metadata": {
        "id": "x-PTVl8f8YgA"
      },
      "id": "x-PTVl8f8YgA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foXvdFmnYKnZ"
      },
      "outputs": [],
      "source": [
        "df_train.rename(columns = {'input_text_x':'question',\n",
        "                           'data.story':'passage',\n",
        "                           'data.source' : 'source',\n",
        "                           'data.id':'id',\n",
        "                           'input_text_y':'answer',\n",
        "                           'span_text':'rationale',\n",
        "                           'span_end' : 'rationale_end'}, inplace = True)\n",
        "\n",
        "\n",
        "df_test.rename(columns = {'input_text_x':'question',\n",
        "                           'data.story':'passage',\n",
        "                           'data.source' : 'source',\n",
        "                           'data.id': 'id',\n",
        "                           'input_text_y':'answer',\n",
        "                           'span_text':'rationale',\n",
        "                           'span_end' : 'rationale_end'}, inplace = True)\n",
        "\n",
        "columns = ['id', 'source', 'passage', 'question', 'answer', 'rationale', 'rationale_end']\n",
        "df_train = df_train[columns]\n",
        "\n",
        "columns = ['id', 'source', 'passage', 'question', 'answer', 'rationale', 'rationale_end']\n",
        "df_test = df_test[columns]"
      ],
      "id": "foXvdFmnYKnZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look to the first lines of created dataframes."
      ],
      "metadata": {
        "id": "L75RX5KVA_e9"
      },
      "id": "L75RX5KVA_e9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "tBCjMNGZJ60n",
        "outputId": "34f26c4e-4894-4450-8833-bb226e113bbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               id     source  \\\n",
              "0  3zotghdk5ibi9cex97fepx7jetpso7  wikipedia   \n",
              "1  3zotghdk5ibi9cex97fepx7jetpso7  wikipedia   \n",
              "2  3zotghdk5ibi9cex97fepx7jetpso7  wikipedia   \n",
              "3  3zotghdk5ibi9cex97fepx7jetpso7  wikipedia   \n",
              "4  3zotghdk5ibi9cex97fepx7jetpso7  wikipedia   \n",
              "\n",
              "                                             passage  \\\n",
              "0  The Vatican Apostolic Library (), more commonl...   \n",
              "1  The Vatican Apostolic Library (), more commonl...   \n",
              "2  The Vatican Apostolic Library (), more commonl...   \n",
              "3  The Vatican Apostolic Library (), more commonl...   \n",
              "4  The Vatican Apostolic Library (), more commonl...   \n",
              "\n",
              "                            question                               answer  \\\n",
              "0  When was the Vat formally opened?  It was formally established in 1475   \n",
              "1           what is the library for?                             research   \n",
              "2                 for what subjects?                     history, and law   \n",
              "3                               and?     philosophy, science and theology   \n",
              "4          what was started in 2014?                           a  project   \n",
              "\n",
              "                                           rationale  rationale_end  \n",
              "0                       Formally established in 1475            179  \n",
              "1           he Vatican Library is a research library            494  \n",
              "2  Vatican Library is a research library for hist...            511  \n",
              "3  Vatican Library is a research library for hist...            545  \n",
              "4  March 2014, the Vatican Library began an initi...            879  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59d56be6-f417-44a9-87bd-b4f390946fc0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>rationale</th>\n",
              "      <th>rationale_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>When was the Vat formally opened?</td>\n",
              "      <td>It was formally established in 1475</td>\n",
              "      <td>Formally established in 1475</td>\n",
              "      <td>179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>what is the library for?</td>\n",
              "      <td>research</td>\n",
              "      <td>he Vatican Library is a research library</td>\n",
              "      <td>494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>for what subjects?</td>\n",
              "      <td>history, and law</td>\n",
              "      <td>Vatican Library is a research library for hist...</td>\n",
              "      <td>511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>and?</td>\n",
              "      <td>philosophy, science and theology</td>\n",
              "      <td>Vatican Library is a research library for hist...</td>\n",
              "      <td>545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>what was started in 2014?</td>\n",
              "      <td>a  project</td>\n",
              "      <td>March 2014, the Vatican Library began an initi...</td>\n",
              "      <td>879</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59d56be6-f417-44a9-87bd-b4f390946fc0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59d56be6-f417-44a9-87bd-b4f390946fc0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59d56be6-f417-44a9-87bd-b4f390946fc0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df_train.head()"
      ],
      "id": "tBCjMNGZJ60n"
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "gbeGIF3xxk25",
        "outputId": "9b83d950-8b5e-4bef-d397-30eb07fd5995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               id  source  \\\n",
              "0  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "1  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "2  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "3  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "4  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "\n",
              "                                             passage  \\\n",
              "0  Once upon a time, in a barn near a farm house,...   \n",
              "1  Once upon a time, in a barn near a farm house,...   \n",
              "2  Once upon a time, in a barn near a farm house,...   \n",
              "3  Once upon a time, in a barn near a farm house,...   \n",
              "4  Once upon a time, in a barn near a farm house,...   \n",
              "\n",
              "                       question                        answer  \\\n",
              "0        What color was Cotton?                         white   \n",
              "1           Where did she live?                     in a barn   \n",
              "2           Did she live alone?                            no   \n",
              "3        Who did she live with?  with her mommy and 5 sisters   \n",
              "4  What color were her sisters?              orange and white   \n",
              "\n",
              "                                           rationale  rationale_end  \n",
              "0                 a little white kitten named Cotton             93  \n",
              "1  in a barn near a farm house, there lived a lit...             80  \n",
              "2                                Cotton wasn't alone            215  \n",
              "3                 with her mommy and 5 other sisters            315  \n",
              "4  her sisters were all orange with beautiful whi...            490  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-020aee5a-7a95-4d99-b6ee-07782e196e75\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>rationale</th>\n",
              "      <th>rationale_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>What color was Cotton?</td>\n",
              "      <td>white</td>\n",
              "      <td>a little white kitten named Cotton</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Where did she live?</td>\n",
              "      <td>in a barn</td>\n",
              "      <td>in a barn near a farm house, there lived a lit...</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Did she live alone?</td>\n",
              "      <td>no</td>\n",
              "      <td>Cotton wasn't alone</td>\n",
              "      <td>215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Who did she live with?</td>\n",
              "      <td>with her mommy and 5 sisters</td>\n",
              "      <td>with her mommy and 5 other sisters</td>\n",
              "      <td>315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>What color were her sisters?</td>\n",
              "      <td>orange and white</td>\n",
              "      <td>her sisters were all orange with beautiful whi...</td>\n",
              "      <td>490</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-020aee5a-7a95-4d99-b6ee-07782e196e75')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-020aee5a-7a95-4d99-b6ee-07782e196e75 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-020aee5a-7a95-4d99-b6ee-07782e196e75');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "id": "gbeGIF3xxk25"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For later purposes, dataframes are enriched with the `history` column, which, for each Q&A pair, collects the set of questions and answers of previous turns, in descending temporal order. For simplicity reasons, the passage and the history are already concatenated and placed into the `passage_history` column."
      ],
      "metadata": {
        "id": "ZmgeXshhBRL4"
      },
      "id": "ZmgeXshhBRL4"
    },
    {
      "cell_type": "code",
      "source": [
        "def add_conversation_history(df):\n",
        "\n",
        "  \"\"\"\n",
        "  Adds to `df` the conversation history information.\n",
        "  \"\"\"\n",
        "\n",
        "  history_df = []\n",
        "  last_id = ''\n",
        "\n",
        "  for idx, row in df.iterrows():\n",
        "\n",
        "    if(last_id != row['id']):\n",
        "      history = ''\n",
        "\n",
        "    last_id = row['id']\n",
        "    data = {'id' : row['id'],\n",
        "            'source' : row['source'],\n",
        "            'passage': row['passage'],\n",
        "            'question': row['question'],\n",
        "            'answer': row['answer'],\n",
        "            'history': history,\n",
        "            'passage_history': row['passage'] + ' ' + history,\n",
        "            'rationale' : row['rationale'],\n",
        "            'rationale_end' : row['rationale_end']}\n",
        "\n",
        "    history_df.append(data)\n",
        "    history = row['question'] + ' ' + row['answer'] + '. ' + history\n",
        "  return pd.DataFrame(history_df)\n",
        "\n",
        "df_train = add_conversation_history(df_train)\n",
        "df_test = add_conversation_history(df_test)"
      ],
      "metadata": {
        "id": "U4AONczABwkY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "U4AONczABwkY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing unanswerable Q&A pairs"
      ],
      "metadata": {
        "id": "qocoEe40xG9e"
      },
      "id": "qocoEe40xG9e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoQA also contains *unanswerable* Q&A pairs, i.e. questions whose answer is *unknown*. For simplicity, they are ignored and removed from both the train and the test sets.\n",
        "\n",
        "Let's first check the current number of train Q&A pairs."
      ],
      "metadata": {
        "id": "i5b9TbzgEqi2"
      },
      "id": "i5b9TbzgEqi2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNLBF6fjdokR",
        "outputId": "a016610a-9774-4da4-93f6-9214ee914247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original total number of train Q&A pairs is 108647\n"
          ]
        }
      ],
      "source": [
        "print(\"The original total number of train Q&A pairs is\", df_train.shape[0])"
      ],
      "id": "SNLBF6fjdokR"
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[df_train['answer'] == 'unknown']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "TeOedoX0yaV6",
        "outputId": "295c03f3-936c-40b3-e8a8-4ca4a6cb0d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    id     source  \\\n",
              "13      3zotghdk5ibi9cex97fepx7jetpso7  wikipedia   \n",
              "349     37w3jxsd668na7z8zzydod86x1wwyi        cnn   \n",
              "352     37w3jxsd668na7z8zzydod86x1wwyi        cnn   \n",
              "354     37w3jxsd668na7z8zzydod86x1wwyi        cnn   \n",
              "356     37w3jxsd668na7z8zzydod86x1wwyi        cnn   \n",
              "...                                ...        ...   \n",
              "108230  3itxp059pwj481n0tun9h1qxelrsj7  gutenberg   \n",
              "108408  3zppdn2slvwes6596ncr3q8fi48e9u       race   \n",
              "108581  34j10vatjfyw0aohj8d4a0wwku3qif  gutenberg   \n",
              "108632  3gs6s824sqxty8vusxp27xazutmnw2  wikipedia   \n",
              "108637  31qnsg6a5rtt5m7pens7xklnbwf87b        cnn   \n",
              "\n",
              "                                                  passage  \\\n",
              "13      The Vatican Apostolic Library (), more commonl...   \n",
              "349     (CNN) -- The 54-year-old Michigan tree trimmer...   \n",
              "352     (CNN) -- The 54-year-old Michigan tree trimmer...   \n",
              "354     (CNN) -- The 54-year-old Michigan tree trimmer...   \n",
              "356     (CNN) -- The 54-year-old Michigan tree trimmer...   \n",
              "...                                                   ...   \n",
              "108230  CHAPTER 6 \\n\\nCan piety the discord heal, Or s...   \n",
              "108408  Edith Cavell was born in a little English vill...   \n",
              "108581  CHAPTER XX \\n\\nFAST IN THE ICE \\n\\n\"Well, ther...   \n",
              "108632  Frankfurt, officially Frankfurt am Main (Liter...   \n",
              "108637  (CNN) -- Cristiano Ronaldo provided the perfec...   \n",
              "\n",
              "                                    question   answer  \\\n",
              "13           what must be requested to view?  unknown   \n",
              "349      Were they the relatives of the kid?  unknown   \n",
              "352              Where did he go afterwards?  unknown   \n",
              "354                          Who found them?  unknown   \n",
              "356                Did he have any siblings?  unknown   \n",
              "...                                      ...      ...   \n",
              "108230   Who wanted to end the conversation?  unknown   \n",
              "108408                 What happened to him?  unknown   \n",
              "108581  What sort of official did they meet?  unknown   \n",
              "108632            Is it the world's biggest?  unknown   \n",
              "108637          How many times did he score?  unknown   \n",
              "\n",
              "                                                  history  \\\n",
              "13      Can anyone use this library? anyone who can do...   \n",
              "349     Why? his truck struck a 10-year-old boy. Who b...   \n",
              "352     How bad was he hurt? \"This is a long road ahea...   \n",
              "354     What was his condition? he is able to wiggle h...   \n",
              "356     Who intervened? Mandi Marie Utash. Who found t...   \n",
              "...                                                   ...   \n",
              "108230    Who had no idea the trouble he'd created? Guy.    \n",
              "108408  DId Edith hear about her father? Yes. Where ex...   \n",
              "108581  Did they go? Yes. Who wanted to go to shore? A...   \n",
              "108632  Is there a music fair? yes. And another? the F...   \n",
              "108637  Who began as a home player at Bernabeu? Gareth...   \n",
              "\n",
              "                                          passage_history rationale  \\\n",
              "13      The Vatican Apostolic Library (), more commonl...   unknown   \n",
              "349     (CNN) -- The 54-year-old Michigan tree trimmer...   unknown   \n",
              "352     (CNN) -- The 54-year-old Michigan tree trimmer...   unknown   \n",
              "354     (CNN) -- The 54-year-old Michigan tree trimmer...   unknown   \n",
              "356     (CNN) -- The 54-year-old Michigan tree trimmer...   unknown   \n",
              "...                                                   ...       ...   \n",
              "108230  CHAPTER 6 \\n\\nCan piety the discord heal, Or s...   unknown   \n",
              "108408  Edith Cavell was born in a little English vill...   unknown   \n",
              "108581  CHAPTER XX \\n\\nFAST IN THE ICE \\n\\n\"Well, ther...   unknown   \n",
              "108632  Frankfurt, officially Frankfurt am Main (Liter...   unknown   \n",
              "108637  (CNN) -- Cristiano Ronaldo provided the perfec...   unknown   \n",
              "\n",
              "        rationale_end  \n",
              "13                 -1  \n",
              "349                -1  \n",
              "352                -1  \n",
              "354                -1  \n",
              "356                -1  \n",
              "...               ...  \n",
              "108230             -1  \n",
              "108408             -1  \n",
              "108581             -1  \n",
              "108632             -1  \n",
              "108637             -1  \n",
              "\n",
              "[1371 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26e70cc4-542a-45ba-8e04-c3a1758dbf7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>history</th>\n",
              "      <th>passage_history</th>\n",
              "      <th>rationale</th>\n",
              "      <th>rationale_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>what must be requested to view?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Can anyone use this library? anyone who can do...</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>37w3jxsd668na7z8zzydod86x1wwyi</td>\n",
              "      <td>cnn</td>\n",
              "      <td>(CNN) -- The 54-year-old Michigan tree trimmer...</td>\n",
              "      <td>Were they the relatives of the kid?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Why? his truck struck a 10-year-old boy. Who b...</td>\n",
              "      <td>(CNN) -- The 54-year-old Michigan tree trimmer...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>37w3jxsd668na7z8zzydod86x1wwyi</td>\n",
              "      <td>cnn</td>\n",
              "      <td>(CNN) -- The 54-year-old Michigan tree trimmer...</td>\n",
              "      <td>Where did he go afterwards?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>How bad was he hurt? \"This is a long road ahea...</td>\n",
              "      <td>(CNN) -- The 54-year-old Michigan tree trimmer...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>37w3jxsd668na7z8zzydod86x1wwyi</td>\n",
              "      <td>cnn</td>\n",
              "      <td>(CNN) -- The 54-year-old Michigan tree trimmer...</td>\n",
              "      <td>Who found them?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>What was his condition? he is able to wiggle h...</td>\n",
              "      <td>(CNN) -- The 54-year-old Michigan tree trimmer...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>37w3jxsd668na7z8zzydod86x1wwyi</td>\n",
              "      <td>cnn</td>\n",
              "      <td>(CNN) -- The 54-year-old Michigan tree trimmer...</td>\n",
              "      <td>Did he have any siblings?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Who intervened? Mandi Marie Utash. Who found t...</td>\n",
              "      <td>(CNN) -- The 54-year-old Michigan tree trimmer...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108230</th>\n",
              "      <td>3itxp059pwj481n0tun9h1qxelrsj7</td>\n",
              "      <td>gutenberg</td>\n",
              "      <td>CHAPTER 6 \\n\\nCan piety the discord heal, Or s...</td>\n",
              "      <td>Who wanted to end the conversation?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Who had no idea the trouble he'd created? Guy.</td>\n",
              "      <td>CHAPTER 6 \\n\\nCan piety the discord heal, Or s...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108408</th>\n",
              "      <td>3zppdn2slvwes6596ncr3q8fi48e9u</td>\n",
              "      <td>race</td>\n",
              "      <td>Edith Cavell was born in a little English vill...</td>\n",
              "      <td>What happened to him?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>DId Edith hear about her father? Yes. Where ex...</td>\n",
              "      <td>Edith Cavell was born in a little English vill...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108581</th>\n",
              "      <td>34j10vatjfyw0aohj8d4a0wwku3qif</td>\n",
              "      <td>gutenberg</td>\n",
              "      <td>CHAPTER XX \\n\\nFAST IN THE ICE \\n\\n\"Well, ther...</td>\n",
              "      <td>What sort of official did they meet?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Did they go? Yes. Who wanted to go to shore? A...</td>\n",
              "      <td>CHAPTER XX \\n\\nFAST IN THE ICE \\n\\n\"Well, ther...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108632</th>\n",
              "      <td>3gs6s824sqxty8vusxp27xazutmnw2</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Frankfurt, officially Frankfurt am Main (Liter...</td>\n",
              "      <td>Is it the world's biggest?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Is there a music fair? yes. And another? the F...</td>\n",
              "      <td>Frankfurt, officially Frankfurt am Main (Liter...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108637</th>\n",
              "      <td>31qnsg6a5rtt5m7pens7xklnbwf87b</td>\n",
              "      <td>cnn</td>\n",
              "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
              "      <td>How many times did he score?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Who began as a home player at Bernabeu? Gareth...</td>\n",
              "      <td>(CNN) -- Cristiano Ronaldo provided the perfec...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1371 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26e70cc4-542a-45ba-8e04-c3a1758dbf7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-26e70cc4-542a-45ba-8e04-c3a1758dbf7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-26e70cc4-542a-45ba-8e04-c3a1758dbf7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "id": "TeOedoX0yaV6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "FDs-arw4Z1-A",
        "outputId": "cec9cb39-a8cb-4e9b-ac47-65fd7fd48c94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [id, source, passage, question, answer, history, passage_history, rationale, rationale_end]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d46eeb7c-c6b5-4aae-8344-b66c81f944ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>history</th>\n",
              "      <th>passage_history</th>\n",
              "      <th>rationale</th>\n",
              "      <th>rationale_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d46eeb7c-c6b5-4aae-8344-b66c81f944ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d46eeb7c-c6b5-4aae-8344-b66c81f944ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d46eeb7c-c6b5-4aae-8344-b66c81f944ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Removing unanswerable Q&A pairs train\n",
        "\n",
        "df_train.drop(labels = df_train.index[df_train['answer']=='unknown'], axis = 0, inplace=True)\n",
        "df_train[df_train['answer'] == 'unknown']"
      ],
      "id": "FDs-arw4Z1-A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now check again the number of train Q&A pairs after removing the unanswerable ones."
      ],
      "metadata": {
        "id": "-3RWyz3qGKC4"
      },
      "id": "-3RWyz3qGKC4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1bvSxuxYFWp",
        "outputId": "a5781e40-c399-4393-ef94-8feb8584d02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of train Q&A pairs after removing unanswerable ones is 107276\n"
          ]
        }
      ],
      "source": [
        "print(\"The total number of train Q&A pairs after removing unanswerable ones is\", df_train.shape[0])"
      ],
      "id": "W1bvSxuxYFWp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then repeat the process for the test set."
      ],
      "metadata": {
        "id": "XReeyJsJGb4B"
      },
      "id": "XReeyJsJGb4B"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The original total number of test Q&A pairs is\", df_test.shape[0])"
      ],
      "metadata": {
        "id": "WY5_Jrj0EpOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb9864c9-57ef-4783-b549-a0d188c0fa4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original total number of test Q&A pairs is 7983\n"
          ]
        }
      ],
      "id": "WY5_Jrj0EpOj"
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[df_test['answer'] == 'unknown']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "aX3Q7RnVyfCq",
        "outputId": "56af64da-43b7-4d64-f22f-fab7fa63269a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  id     source  \\\n",
              "20    3azhrg4cu4ktme1zh7c2ro3pn2430d     mctest   \n",
              "22    3azhrg4cu4ktme1zh7c2ro3pn2430d     mctest   \n",
              "130   3tu5zicbrd13b4c4am1dxb2igltq8b       race   \n",
              "229   3w2lolrxlbfni6t5wqngs6le78akrc  gutenberg   \n",
              "287   3txd01zld4hukwwjfsv5q0j2ixa4ul  gutenberg   \n",
              "...                              ...        ...   \n",
              "6789  38bquhla9w0fbh1spajsdo8dm42mok  wikipedia   \n",
              "6972  3k9fobbf2hjdnejvoji0ymtjvcenlk  gutenberg   \n",
              "7449  3mx2nq3yc9u4xjuey2p2fzokc165xj  wikipedia   \n",
              "7509  3yw4xosqkqldsxz0sac3s2cz64l1u8  wikipedia   \n",
              "7910  3npfyt4izc42dgjyfy8tjwf92engxn       race   \n",
              "\n",
              "                                                passage  \\\n",
              "20    Once there was a beautiful fish named Asta. As...   \n",
              "22    Once there was a beautiful fish named Asta. As...   \n",
              "130   Which country grows the most tea? The answer i...   \n",
              "229   CHAPTER XXII \\n\\nNorthward, along the leeward ...   \n",
              "287   CHAPTER FIFTY FIVE. \\n\\nWAITING. \\n\\nThe lengt...   \n",
              "...                                                 ...   \n",
              "6789  Futsal, (literally \"mini-football\", \"futebol d...   \n",
              "6972  CHAPTER XXIV: A Merry Home-Going \\n\\n\"The Laug...   \n",
              "7449  Guam (i/ˈɡwɑːm/ or /ˈɡwɒm/; Chamorro: Guåhån;[...   \n",
              "7509  The 2008 Summer Olympics torch relay was run f...   \n",
              "7910  Alan worked in an office in the city. He worke...   \n",
              "\n",
              "                                               question   answer  \\\n",
              "20                       What did they do with the note  unknown   \n",
              "22                                    were they excited  unknown   \n",
              "130                  How did his body react to the tea?  unknown   \n",
              "229                               Were there cannibals?  unknown   \n",
              "287                     Why was he excommunicated then?  unknown   \n",
              "...                                                 ...      ...   \n",
              "6789  Does it have the same number of players as reg...  unknown   \n",
              "6972                       Which one was made of paper?  unknown   \n",
              "7449                                 Is it the highest?  unknown   \n",
              "7509                  And did they climb any mountains?  unknown   \n",
              "7910                                    How did he die?  unknown   \n",
              "\n",
              "                                                history  \\\n",
              "20    Who could read the note Asta's papa. Did a lit...   \n",
              "22    did they write back yes. What did they do with...   \n",
              "130   Was he happy with it? yes. When did he take it...   \n",
              "229   Where did the Ariel drop anchor off of? Every ...   \n",
              "287   Was he superstitious? no. Was he a heretic? no...   \n",
              "...                                                 ...   \n",
              "6789  What ground is it played on? a hard court surf...   \n",
              "6972  Who or what was was happy? The Laughing Brook,...   \n",
              "7449  Is it the smallest of it's group of islands? N...   \n",
              "7509  Did they visit any notable landmarks? Panathin...   \n",
              "7910  Why only four days? a farmer died. How long di...   \n",
              "\n",
              "                                        passage_history  \\\n",
              "20    Once there was a beautiful fish named Asta. As...   \n",
              "22    Once there was a beautiful fish named Asta. As...   \n",
              "130   Which country grows the most tea? The answer i...   \n",
              "229   CHAPTER XXII \\n\\nNorthward, along the leeward ...   \n",
              "287   CHAPTER FIFTY FIVE. \\n\\nWAITING. \\n\\nThe lengt...   \n",
              "...                                                 ...   \n",
              "6789  Futsal, (literally \"mini-football\", \"futebol d...   \n",
              "6972  CHAPTER XXIV: A Merry Home-Going \\n\\n\"The Laug...   \n",
              "7449  Guam (i/ˈɡwɑːm/ or /ˈɡwɒm/; Chamorro: Guåhån;[...   \n",
              "7509  The 2008 Summer Olympics torch relay was run f...   \n",
              "7910  Alan worked in an office in the city. He worke...   \n",
              "\n",
              "                                rationale  rationale_end  \n",
              "20                                unknown             -1  \n",
              "22                                unknown             -1  \n",
              "130                               unknown             -1  \n",
              "229                               unknown             -1  \n",
              "287                               unknown             -1  \n",
              "...                                   ...            ...  \n",
              "6789                              unknown             -1  \n",
              "6972  CHAPTER XXIV: A Merry Home-Going \\n             34  \n",
              "7449                              unknown             -1  \n",
              "7509                              unknown             -1  \n",
              "7910                              unknown             -1  \n",
              "\n",
              "[66 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-260e1a81-5c02-4474-aea2-d043e43568a2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>history</th>\n",
              "      <th>passage_history</th>\n",
              "      <th>rationale</th>\n",
              "      <th>rationale_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3azhrg4cu4ktme1zh7c2ro3pn2430d</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once there was a beautiful fish named Asta. As...</td>\n",
              "      <td>What did they do with the note</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Who could read the note Asta's papa. Did a lit...</td>\n",
              "      <td>Once there was a beautiful fish named Asta. As...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3azhrg4cu4ktme1zh7c2ro3pn2430d</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once there was a beautiful fish named Asta. As...</td>\n",
              "      <td>were they excited</td>\n",
              "      <td>unknown</td>\n",
              "      <td>did they write back yes. What did they do with...</td>\n",
              "      <td>Once there was a beautiful fish named Asta. As...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>3tu5zicbrd13b4c4am1dxb2igltq8b</td>\n",
              "      <td>race</td>\n",
              "      <td>Which country grows the most tea? The answer i...</td>\n",
              "      <td>How did his body react to the tea?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Was he happy with it? yes. When did he take it...</td>\n",
              "      <td>Which country grows the most tea? The answer i...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>3w2lolrxlbfni6t5wqngs6le78akrc</td>\n",
              "      <td>gutenberg</td>\n",
              "      <td>CHAPTER XXII \\n\\nNorthward, along the leeward ...</td>\n",
              "      <td>Were there cannibals?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Where did the Ariel drop anchor off of? Every ...</td>\n",
              "      <td>CHAPTER XXII \\n\\nNorthward, along the leeward ...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>3txd01zld4hukwwjfsv5q0j2ixa4ul</td>\n",
              "      <td>gutenberg</td>\n",
              "      <td>CHAPTER FIFTY FIVE. \\n\\nWAITING. \\n\\nThe lengt...</td>\n",
              "      <td>Why was he excommunicated then?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Was he superstitious? no. Was he a heretic? no...</td>\n",
              "      <td>CHAPTER FIFTY FIVE. \\n\\nWAITING. \\n\\nThe lengt...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6789</th>\n",
              "      <td>38bquhla9w0fbh1spajsdo8dm42mok</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Futsal, (literally \"mini-football\", \"futebol d...</td>\n",
              "      <td>Does it have the same number of players as reg...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>What ground is it played on? a hard court surf...</td>\n",
              "      <td>Futsal, (literally \"mini-football\", \"futebol d...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6972</th>\n",
              "      <td>3k9fobbf2hjdnejvoji0ymtjvcenlk</td>\n",
              "      <td>gutenberg</td>\n",
              "      <td>CHAPTER XXIV: A Merry Home-Going \\n\\n\"The Laug...</td>\n",
              "      <td>Which one was made of paper?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Who or what was was happy? The Laughing Brook,...</td>\n",
              "      <td>CHAPTER XXIV: A Merry Home-Going \\n\\n\"The Laug...</td>\n",
              "      <td>CHAPTER XXIV: A Merry Home-Going \\n</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7449</th>\n",
              "      <td>3mx2nq3yc9u4xjuey2p2fzokc165xj</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>Guam (i/ˈɡwɑːm/ or /ˈɡwɒm/; Chamorro: Guåhån;[...</td>\n",
              "      <td>Is it the highest?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Is it the smallest of it's group of islands? N...</td>\n",
              "      <td>Guam (i/ˈɡwɑːm/ or /ˈɡwɒm/; Chamorro: Guåhån;[...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7509</th>\n",
              "      <td>3yw4xosqkqldsxz0sac3s2cz64l1u8</td>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The 2008 Summer Olympics torch relay was run f...</td>\n",
              "      <td>And did they climb any mountains?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Did they visit any notable landmarks? Panathin...</td>\n",
              "      <td>The 2008 Summer Olympics torch relay was run f...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7910</th>\n",
              "      <td>3npfyt4izc42dgjyfy8tjwf92engxn</td>\n",
              "      <td>race</td>\n",
              "      <td>Alan worked in an office in the city. He worke...</td>\n",
              "      <td>How did he die?</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Why only four days? a farmer died. How long di...</td>\n",
              "      <td>Alan worked in an office in the city. He worke...</td>\n",
              "      <td>unknown</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>66 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-260e1a81-5c02-4474-aea2-d043e43568a2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-260e1a81-5c02-4474-aea2-d043e43568a2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-260e1a81-5c02-4474-aea2-d043e43568a2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "id": "aX3Q7RnVyfCq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "C5luH6eEnzBg",
        "outputId": "0728a939-7f57-4531-faf3-5ec528ba3295"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [id, source, passage, question, answer, history, passage_history, rationale, rationale_end]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-751564b4-eaa4-47e3-b3e8-f05df55efa1a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>history</th>\n",
              "      <th>passage_history</th>\n",
              "      <th>rationale</th>\n",
              "      <th>rationale_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-751564b4-eaa4-47e3-b3e8-f05df55efa1a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-751564b4-eaa4-47e3-b3e8-f05df55efa1a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-751564b4-eaa4-47e3-b3e8-f05df55efa1a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Removing unanswerable Q&A pairs test\n",
        "\n",
        "df_test.drop(labels = df_test.index[df_test['answer']=='unknown'], axis = 0, inplace=True)\n",
        "df_test[df_test['answer'] == 'unknown']"
      ],
      "id": "C5luH6eEnzBg"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The total number of test Q&A pairs after removing unanswerable ones is\", df_test.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpB4WeNNes7E",
        "outputId": "0e3e2dda-ddcc-4a99-e72e-1fae670e394f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of test Q&A pairs after removing unanswerable ones is 7917\n"
          ]
        }
      ],
      "id": "hpB4WeNNes7E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspection of created dataframe"
      ],
      "metadata": {
        "id": "ANdntlPqy8_w"
      },
      "id": "ANdntlPqy8_w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better understand the content of created dataframes, let's have a look to a single passage conversation, by printing the text passage and all Q&A turns, each of them paired with a span of text passage supporting the answer, i.e. the rationale *R*."
      ],
      "metadata": {
        "id": "ZW3snTvZHPvf"
      },
      "id": "ZW3snTvZHPvf"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PASSAGE:\\n\")\n",
        "\n",
        "id = df_train['id'][0]\n",
        "story = df_train['passage'][0]\n",
        "\n",
        "print(f\"{story}\")\n",
        "print()\n",
        "\n",
        "print(\"CONVERSATION STRUCTURE:\\n\")\n",
        "\n",
        "for idx, row in df_train.iterrows():\n",
        "\n",
        "  if(id == row['id']):\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    rationale = row['rationale']\n",
        "\n",
        "    print(f\"Turn {idx+1}\")\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    print(f\"R: {rationale}\")\n",
        "    print()\n",
        "\n",
        "  else:\n",
        "    break"
      ],
      "metadata": {
        "id": "l363QtWaiwpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4018ed32-ff31-45b2-b587-fa02ea439afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \n",
            "\n",
            "The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \n",
            "\n",
            "In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \n",
            "\n",
            "The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \n",
            "\n",
            "Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \n",
            "\n",
            "The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: When was the Vat formally opened?\n",
            "A: It was formally established in 1475\n",
            "R: Formally established in 1475\n",
            "\n",
            "Turn 2\n",
            "Q: what is the library for?\n",
            "A: research\n",
            "R: he Vatican Library is a research library\n",
            "\n",
            "Turn 3\n",
            "Q: for what subjects?\n",
            "A: history, and law\n",
            "R: Vatican Library is a research library for history, law\n",
            "\n",
            "Turn 4\n",
            "Q: and?\n",
            "A: philosophy, science and theology\n",
            "R: Vatican Library is a research library for history, law, philosophy, science and theology\n",
            "\n",
            "Turn 5\n",
            "Q: what was started in 2014?\n",
            "A: a  project\n",
            "R: March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts\n",
            "\n",
            "Turn 6\n",
            "Q: how do scholars divide the library?\n",
            "A: into periods\n",
            "R: Scholars have traditionally divided the history of the library into five period\n",
            "\n",
            "Turn 7\n",
            "Q: how many?\n",
            "A: five\n",
            "R: Scholars have traditionally divided the history of the library into five periods\n",
            "\n",
            "Turn 8\n",
            "Q: what is the official name of the Vat?\n",
            "A: The Vatican Apostolic Library\n",
            "R: Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, \n",
            "\n",
            "Turn 9\n",
            "Q: where is it?\n",
            "A: in Vatican City\n",
            "R: is the library of the Holy See, located in Vatican City.\n",
            "\n",
            "Turn 10\n",
            "Q: how many printed books does it contain?\n",
            "A: 1.1 million\n",
            "R:  It has 75,000 codices from throughout history, as well as 1.1 million printed books\n",
            "\n",
            "Turn 11\n",
            "Q: when were the Secret Archives moved from the rest of the library?\n",
            "A: at the beginning of the 17th century;\n",
            "R: atican Secret Archives were separated from the library at the beginning of the 17th century;\n",
            "\n",
            "Turn 12\n",
            "Q: how many items are in this secret collection?\n",
            "A: 150,000\n",
            "R:  Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \n",
            "\n",
            "Turn 13\n",
            "Q: Can anyone use this library?\n",
            "A: anyone who can document their qualifications and research needs.\n",
            "R:  The Vatican Library is open to anyone who can document their qualifications and research needs. \n",
            "\n",
            "Turn 15\n",
            "Q: what must be requested in person or by mail?\n",
            "A: Photocopies\n",
            "R: Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \n",
            "\n",
            "Turn 16\n",
            "Q: of what books?\n",
            "A: only books published between 1801 and 1990\n",
            "R: hotocopies for private study of pages from books published between 1801 and 1990\n",
            "\n",
            "Turn 17\n",
            "Q: What is the Vat the library of?\n",
            "A: the Holy See\n",
            "R: simply the Vat, is the library of the Holy See,\n",
            "\n",
            "Turn 18\n",
            "Q: How many books survived the Pre Lateran period?\n",
            "A: a handful of volumes\n",
            "R: Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant\n",
            "\n",
            "Turn 19\n",
            "Q: what is the point of the project started in 2014?\n",
            "A: digitising manuscripts\n",
            "R: Vatican Library began an initial four-year project of digitising its collection of manuscripts, \n",
            "\n",
            "Turn 20\n",
            "Q: what will this allow?\n",
            "A: them to be viewed online.\n",
            "R: manuscripts, to be made available online. \n",
            "\n"
          ]
        }
      ],
      "id": "l363QtWaiwpk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's print the same conversation, this time visualizing for each Q&A pair the correspondent conversation history *H*."
      ],
      "metadata": {
        "id": "_tl5TegEJgWp"
      },
      "id": "_tl5TegEJgWp"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"PASSAGE:\\n\")\n",
        "\n",
        "id = df_train['id'][0]\n",
        "story = df_train['passage'][0]\n",
        "\n",
        "print(f\"{story}\")\n",
        "print()\n",
        "\n",
        "print(\"CONVERSATION STRUCTURE:\\n\")\n",
        "\n",
        "for idx, row in df_train.iterrows():\n",
        "\n",
        "  if(id == row['id']):\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    history = row['history']\n",
        "\n",
        "    print(f\"Turn {idx+1}\")\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    print(f\"H: {history}\")\n",
        "    print()\n",
        "\n",
        "  else:\n",
        "    break"
      ],
      "metadata": {
        "id": "ybSLqNiKimmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f175fcd-094b-47ee-b27d-ab624b94bed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \n",
            "\n",
            "The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \n",
            "\n",
            "In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \n",
            "\n",
            "The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \n",
            "\n",
            "Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \n",
            "\n",
            "The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: When was the Vat formally opened?\n",
            "A: It was formally established in 1475\n",
            "H: \n",
            "\n",
            "Turn 2\n",
            "Q: what is the library for?\n",
            "A: research\n",
            "H: When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 3\n",
            "Q: for what subjects?\n",
            "A: history, and law\n",
            "H: what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 4\n",
            "Q: and?\n",
            "A: philosophy, science and theology\n",
            "H: for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 5\n",
            "Q: what was started in 2014?\n",
            "A: a  project\n",
            "H: and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 6\n",
            "Q: how do scholars divide the library?\n",
            "A: into periods\n",
            "H: what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 7\n",
            "Q: how many?\n",
            "A: five\n",
            "H: how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 8\n",
            "Q: what is the official name of the Vat?\n",
            "A: The Vatican Apostolic Library\n",
            "H: how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 9\n",
            "Q: where is it?\n",
            "A: in Vatican City\n",
            "H: what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 10\n",
            "Q: how many printed books does it contain?\n",
            "A: 1.1 million\n",
            "H: where is it? in Vatican City. what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 11\n",
            "Q: when were the Secret Archives moved from the rest of the library?\n",
            "A: at the beginning of the 17th century;\n",
            "H: how many printed books does it contain? 1.1 million. where is it? in Vatican City. what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 12\n",
            "Q: how many items are in this secret collection?\n",
            "A: 150,000\n",
            "H: when were the Secret Archives moved from the rest of the library? at the beginning of the 17th century;. how many printed books does it contain? 1.1 million. where is it? in Vatican City. what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 13\n",
            "Q: Can anyone use this library?\n",
            "A: anyone who can document their qualifications and research needs.\n",
            "H: how many items are in this secret collection? 150,000. when were the Secret Archives moved from the rest of the library? at the beginning of the 17th century;. how many printed books does it contain? 1.1 million. where is it? in Vatican City. what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 15\n",
            "Q: what must be requested in person or by mail?\n",
            "A: Photocopies\n",
            "H: what must be requested to view? unknown. Can anyone use this library? anyone who can document their qualifications and research needs.. how many items are in this secret collection? 150,000. when were the Secret Archives moved from the rest of the library? at the beginning of the 17th century;. how many printed books does it contain? 1.1 million. where is it? in Vatican City. what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 16\n",
            "Q: of what books?\n",
            "A: only books published between 1801 and 1990\n",
            "H: what must be requested in person or by mail? Photocopies. what must be requested to view? unknown. Can anyone use this library? anyone who can document their qualifications and research needs.. how many items are in this secret collection? 150,000. when were the Secret Archives moved from the rest of the library? at the beginning of the 17th century;. how many printed books does it contain? 1.1 million. where is it? in Vatican City. what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 17\n",
            "Q: What is the Vat the library of?\n",
            "A: the Holy See\n",
            "H: of what books? only books published between 1801 and 1990. what must be requested in person or by mail? Photocopies. what must be requested to view? unknown. Can anyone use this library? anyone who can document their qualifications and research needs.. how many items are in this secret collection? 150,000. when were the Secret Archives moved from the rest of the library? at the beginning of the 17th century;. how many printed books does it contain? 1.1 million. where is it? in Vatican City. what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 18\n",
            "Q: How many books survived the Pre Lateran period?\n",
            "A: a handful of volumes\n",
            "H: What is the Vat the library of? the Holy See. of what books? only books published between 1801 and 1990. what must be requested in person or by mail? Photocopies. what must be requested to view? unknown. Can anyone use this library? anyone who can document their qualifications and research needs.. how many items are in this secret collection? 150,000. when were the Secret Archives moved from the rest of the library? at the beginning of the 17th century;. how many printed books does it contain? 1.1 million. where is it? in Vatican City. what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 19\n",
            "Q: what is the point of the project started in 2014?\n",
            "A: digitising manuscripts\n",
            "H: How many books survived the Pre Lateran period? a handful of volumes. What is the Vat the library of? the Holy See. of what books? only books published between 1801 and 1990. what must be requested in person or by mail? Photocopies. what must be requested to view? unknown. Can anyone use this library? anyone who can document their qualifications and research needs.. how many items are in this secret collection? 150,000. when were the Secret Archives moved from the rest of the library? at the beginning of the 17th century;. how many printed books does it contain? 1.1 million. where is it? in Vatican City. what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n",
            "Turn 20\n",
            "Q: what will this allow?\n",
            "A: them to be viewed online.\n",
            "H: what is the point of the project started in 2014? digitising manuscripts. How many books survived the Pre Lateran period? a handful of volumes. What is the Vat the library of? the Holy See. of what books? only books published between 1801 and 1990. what must be requested in person or by mail? Photocopies. what must be requested to view? unknown. Can anyone use this library? anyone who can document their qualifications and research needs.. how many items are in this secret collection? 150,000. when were the Secret Archives moved from the rest of the library? at the beginning of the 17th century;. how many printed books does it contain? 1.1 million. where is it? in Vatican City. what is the official name of the Vat? The Vatican Apostolic Library. how many? five. how do scholars divide the library? into periods. what was started in 2014? a  project. and? philosophy, science and theology. for what subjects? history, and law. what is the library for? research. When was the Vat formally opened? It was formally established in 1475. \n",
            "\n"
          ]
        }
      ],
      "id": "ybSLqNiKimmV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/validation splitting\n"
      ],
      "metadata": {
        "id": "92kAux4-gqG5"
      },
      "id": "92kAux4-gqG5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's print the number of different passages contained in the original train and test sets."
      ],
      "metadata": {
        "id": "9cJ5AL86D2rX"
      },
      "id": "9cJ5AL86D2rX"
    },
    {
      "cell_type": "code",
      "source": [
        "tot_stories = len(df_train['id'].unique()) + len(df_test['id'].unique())\n",
        "\n",
        "print(\"Before splitting:\\n\")\n",
        "print(\"Number of train stories:\", len(df_train['id'].unique()))\n",
        "print(\"Number of test stories:\", len(df_test['id'].unique()))\n",
        "print()\n",
        "print(\"The total number of stories in the whole dataset is\", tot_stories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0GbdtCke45V",
        "outputId": "3fe9b82b-b53a-4cac-91e6-0e23f2012492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before splitting:\n",
            "\n",
            "Number of train stories: 7193\n",
            "Number of test stories: 500\n",
            "\n",
            "The total number of stories in the whole dataset is 7693\n"
          ]
        }
      ],
      "id": "y0GbdtCke45V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since CoQA provides only two sets of data, containing $7193$ and $500$ passages respectively, the first one is divided into $80$\\% for train and $20$\\% for validation, while the second one is used as test set."
      ],
      "metadata": {
        "id": "xWHVuyPT4T44"
      },
      "id": "xWHVuyPT4T44"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exTOlJ2ALv7f"
      },
      "outputs": [],
      "source": [
        "splitter = GroupShuffleSplit(train_size=.80, n_splits=1, random_state = 42)\n",
        "split = splitter.split(df_train, groups = df_train['id'])\n",
        "train_inds, val_inds = next(split)\n",
        "\n",
        "df_val = df_train.iloc[val_inds]\n",
        "df_train = df_train.iloc[train_inds]\n",
        "\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_val.reset_index(drop=True, inplace=True)"
      ],
      "id": "exTOlJ2ALv7f"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After splitting:\\n\")\n",
        "print(f\"Number of train stories: {len(df_train['id'].unique())} ({np.round(len(df_train['id'].unique())/(tot_stories)*100, 2)}%)\")\n",
        "print(f\"Number of validation stories: {len(df_val['id'].unique())} ({np.round(len(df_val['id'].unique())/(tot_stories)*100, 2)}%)\")\n",
        "print(f\"Number of test stories: {len(df_test['id'].unique())} ({np.round(len(df_test['id'].unique())/(tot_stories)*100, 2)}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpb1NxJZUkB_",
        "outputId": "20eafff8-c2dd-4872-d569-3085420e8e33"
      },
      "id": "Kpb1NxJZUkB_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After splitting:\n",
            "\n",
            "Number of train stories: 5754 (74.8%)\n",
            "Number of validation stories: 1439 (18.71%)\n",
            "Number of test stories: 500 (6.5%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting the distribution of the number of turns per conversation, it can be seen that the average conversation length is approximately 15 turns for all the sets."
      ],
      "metadata": {
        "id": "H8RK5Otj6KWn"
      },
      "id": "H8RK5Otj6KWn"
    },
    {
      "cell_type": "code",
      "source": [
        "QA_length_train = df_train.groupby(['id'])['question'].count()\n",
        "QA_length_val = df_val.groupby(['id'])['question'].count()\n",
        "QA_length_test = df_test.groupby(['id'])['question'].count()\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(1,3,1)\n",
        "plt.hist(QA_length_train, bins=range(1, 36), color='darkred')\n",
        "plt.xlim((1, 35))\n",
        "plt.title(\"Conversation length distribution - Train\")\n",
        "plt.subplot(1,3,2)\n",
        "plt.hist(QA_length_val, bins=range(1, 36), color='darkred')\n",
        "plt.xlim((1, 35))\n",
        "plt.title(\"Conversation length distribution - Validation\")\n",
        "plt.subplot(1,3,3)\n",
        "plt.hist(QA_length_test, bins=range(1, 36), color='darkred')\n",
        "plt.xlim((1, 35))\n",
        "plt.title(\"Conversation length distribution - Test\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "Yfp2HH7WWsZg",
        "outputId": "cfa77edc-3b79-4f8f-c40f-8df28ec1976d"
      },
      "id": "Yfp2HH7WWsZg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x360 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the histograms below can be seen that most of the passages contain between $200$ and $400$ number of words."
      ],
      "metadata": {
        "id": "mUoMR2_P9SSG"
      },
      "id": "mUoMR2_P9SSG"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(1,3,1)\n",
        "plt.hist(df_train['passage'].map(lambda x: len(str(x).split(\" \"))), color='darkred')\n",
        "plt.xlim((1, 1500))\n",
        "plt.title(\"Passage length - Train\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.hist(df_val['passage'].map(lambda x: len(str(x).split(\" \"))), color='darkred')\n",
        "plt.xlim((1, 1500))\n",
        "plt.title(\"Passage length - Validation\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.hist(df_test['passage'].map(lambda x: len(str(x).split(\" \"))), color='darkred')\n",
        "plt.xlim((1, 1500))\n",
        "plt.title(\"Passage length - Test\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "EfYf3g0_Wu_r",
        "outputId": "02e6edcd-11c4-4049-f82d-3b3daf046d94"
      },
      "id": "EfYf3g0_Wu_r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x360 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When adding the history to passages, the correspondent total length distribution is shifted to the right compared to the previous ones, but most of the samples are still inside the range of [$200$, $400$] words.\n"
      ],
      "metadata": {
        "id": "rH2imSjs-FvV"
      },
      "id": "rH2imSjs-FvV"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(1,3,1)\n",
        "plt.hist(df_train['passage_history'].map(lambda x: len(str(x).split(\" \"))), color='darkred')\n",
        "plt.xlim((1, 1200))\n",
        "plt.title(\"Passage+history length - Train\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.hist(df_val['passage_history'].map(lambda x: len(str(x).split(\" \"))), color='darkred')\n",
        "plt.xlim((1, 1200))\n",
        "plt.title(\"Passage+history length - Validation\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.hist(df_test['passage_history'].map(lambda x: len(str(x).split(\" \"))), color='darkred')\n",
        "plt.xlim((1, 1200))\n",
        "plt.title(\"Passage+history length - Test\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "cvOJru7RWzoX",
        "outputId": "41d64ba4-c4c2-40aa-f962-47afd1495551"
      },
      "id": "cvOJru7RWzoX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x360 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience reasons, let's convert each pandas `Dataframe` into a `Dataset`."
      ],
      "metadata": {
        "id": "-AcWzmGv_4e9"
      },
      "id": "-AcWzmGv_4e9"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = Dataset.from_pandas(df_train)\n",
        "dataset_val = Dataset.from_pandas(df_val)\n",
        "dataset_test = Dataset.from_pandas(df_test)"
      ],
      "metadata": {
        "id": "No0eY6rhtAwn"
      },
      "id": "No0eY6rhtAwn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models implementation"
      ],
      "metadata": {
        "id": "VAqg1ZT7Msrg"
      },
      "id": "VAqg1ZT7Msrg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several approaches to implement transformer-based models for the question answering (QA) task, some of the most famous and used networks are:\n",
        "\n",
        "*   **B**idirectional **E**ncoder **R**epresentations from **T**ransformers (**BERT**);\n",
        "*   **R**obustly **O**ptimized **BERT** (**RoBERTa**).\n",
        "\n",
        "They are both pre-trained transformer models that can be fine-tuned for a variety of natural language processing tasks, including question answering.\n",
        "\n",
        "A key difference is the size and performance of the two models. RoBERTa is a larger model with more parameters, which allows it to achieve higher performance on a variety of tasks. However, this also means that it requires more computational resources and may be slower to train and fine-tune.\n",
        "In summary, both BERT and RoBERTa are powerful models for natural language processing tasks, but RoBERTa is generally considered to be a stronger performer due to its more efficient training technique and larger model size. However, it may also be slower and more resource-intensive to train and fine-tune.\n",
        "\n",
        "In this case, for computational reasons a smaller and lighter version of both model will be used:\n",
        "\n",
        "*   BERTTiny, a smaller version of the original BERT model, with fewer parameters and a shallower architecture\n",
        "*   DistilRoBERTa-base, a smaller, distilled version of the original RoBERTa model that has been \"distilled\" to have fewer parameters while still maintaining a good level of performance\n"
      ],
      "metadata": {
        "id": "AMHYFQMEMvV7"
      },
      "id": "AMHYFQMEMvV7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To deal with the task to solve, the general approach is to define two different sequence-to-sequence transformer-based models, built using BERTTiny and DistilRoBERTa-base both as encoder and decoder.\n",
        "\n",
        "For each type of model, two different versions are implemented:\n",
        "* The baseline $f_\\theta (Q,P)$, which receives as input a question *Q* concatenated with the text passage *P* and generates an answer *A*\n",
        "* The history version $f_\\theta (Q,P,H)$, which receives as input a question *Q* concatenated with the text passage *P* and the conversation history *H*, i.e. the set of questions and answers of previous turns, in descending temporal order, generating an answer *A*.\n",
        "\n",
        "To have a benchmark against which to properly reason on models performances, for both types of models a third version (upper bound version) is implemented, which receives as input a question *Q* and the rationale *R* rather than the passage.\n"
      ],
      "metadata": {
        "id": "fqCSsA1uL5sh"
      },
      "id": "fqCSsA1uL5sh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions definition"
      ],
      "metadata": {
        "id": "Aq0CvJFukyha"
      },
      "id": "Aq0CvJFukyha"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `define_model` function, through the `EncoderDecoderModel` class of [HuggingFace](https://huggingface.co/), loads the pre-trained model whose `model_name` is passed as input, and creates a Seq2Seq architecture setting it both as encoder and decoder. Then, the function makes use of the `tokenizer` for the special tokens configuration: the \\[CLS\\] token is set as decoder start token and \\[SEP\\] token as \\[EOS\\] token encoder. Finally, it specifies beam search decoding parameters."
      ],
      "metadata": {
        "id": "JKpPTpm4OIbG"
      },
      "id": "JKpPTpm4OIbG"
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(model_name, tokenizer):\n",
        "\n",
        "  \"\"\"\n",
        "  Loads the pre-trained weights of `model_name` into an encoder-decoder architecture and sets the desired configuration. Special tokens are set using `tokenizer` ones.\n",
        "  \"\"\"\n",
        "\n",
        "  model = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name)\n",
        "\n",
        "  model.config.decoder.is_decoder = True\n",
        "  model.config.decoder.add_cross_attention = True\n",
        "\n",
        "  model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "  model.config.eos_token_id = tokenizer.sep_token_id\n",
        "  model.config.pad_token_id = tokenizer.pad_token_id\n",
        "  model.config.vocab_size = model.config.encoder.vocab_size\n",
        "\n",
        "  model.config.max_length = 64\n",
        "  model.config.min_length = 1\n",
        "  model.config.no_repeat_ngram_size = 3\n",
        "  model.config.early_stopping = True\n",
        "  model.config.length_penalty = 2.0\n",
        "  model.config.num_beams = 15\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "OIf26Ww8AXY7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OIf26Ww8AXY7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before feeding the models, data must be tokenized and transformed in model inputs, using a proper tokenizer. BERT and RoBERTa encoders accept input sequences maximum $512$ tokens long, so it's important to check if at least the majority of $Q$+$P$ inputs fits this limit. Since questions contribute little to the total length of the input, let's focus on the passages length.\n",
        "\n",
        "The `check_passage_answer_max_length` function computes the maximum passages length of the 95% of training passages. Since answers are used as labels during the training phase, they also need tokenization. Then, the function also provides the maximum answers length as to handle the $90$% of training answers."
      ],
      "metadata": {
        "id": "QW7iClqFUmXX"
      },
      "id": "QW7iClqFUmXX"
    },
    {
      "cell_type": "code",
      "source": [
        "def check_passage_answer_max_length(tokenizer, df_train):\n",
        "\n",
        "  \"\"\"\n",
        "  Computes the maximum passages length of the 95% of training passages and the maximum answers length as to handle the 90% of training answers.\n",
        "  \"\"\"\n",
        "\n",
        "  passage_max_length = np.percentile(df_train.loc[:, ['passage']].applymap(lambda text: len(tokenizer(text).input_ids)), 95)\n",
        "  answer_max_length = np.percentile(df_train.loc[:, ['answer']].applymap(lambda text: len(tokenizer(text).input_ids)), 90)\n",
        "\n",
        "  print('Passage max length: ' + str(passage_max_length))\n",
        "  print('Answer max length: ' + str(answer_max_length))\n",
        "\n",
        "  return passage_max_length, answer_max_length"
      ],
      "metadata": {
        "id": "Of1jD2lABBoK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Of1jD2lABBoK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `tokenize_function` is in charge of tokenizing and preparing the models inputs. Answers are tokenized as `labels`, while, depending on the model version, encoder `input_ids` are prepared in different ways:\n",
        "\n",
        "* Baseline version → the tokenizer encodes the question $Q$ and the passage $P$, using special tokens: [CLS] $Q$ [SEP] $P$ [SEP]\n",
        "* History version → the tokenizer encodes the question $Q$ and the passage $P$ concatenated with the history $H$, using special tokens: [CLS] $Q$ [SEP] $P$+$H$ [SEP]\n",
        "* Upper bound version → the tokenizer encodes the question $Q$ and the rationale $R$, using special tokens: [CLS] $Q$ [SEP] $R$ [SEP]\n",
        "\n",
        "In particular, encoder inputs and labels cannot exceed `encoder_max_length` and `decoder_max_length` respectively. Thus, sequences are truncated or padded accordingly.\n",
        "\n",
        "Finally, in order to ignore the loss of labels padding, the correspondent `input_id` is set to $-100$."
      ],
      "metadata": {
        "id": "79XNrT13cXPi"
      },
      "id": "79XNrT13cXPi"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(batch, history, upper_bound, encoder_max_length, decoder_max_length):\n",
        "\n",
        "  \"\"\"\n",
        "  Transforms the `batch` of data into a batch of model inputs, by properly running a tokenization process. Eventual truncation is performed according to `encoder_max_length` and\n",
        "  `decoder_max_length`. With `history=True` and `upper_bound=True`, input is prepared and tokenized as needed for the history and upper bound model versions respectively.\n",
        "  \"\"\"\n",
        "\n",
        "  if history:\n",
        "    inputs = tokenizer(batch[\"question\"], batch['passage_history'], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "  elif upper_bound:\n",
        "    inputs = tokenizer(batch[\"question\"], batch['rationale'], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "  else:\n",
        "    inputs = tokenizer(batch[\"question\"], batch['passage'], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "\n",
        "  outputs = tokenizer(batch[\"answer\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "\n",
        "  batch[\"input_ids\"] = inputs.input_ids\n",
        "  batch[\"attention_mask\"] = inputs.attention_mask\n",
        "  batch[\"labels\"] = outputs.input_ids.copy()\n",
        "\n",
        "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
        "\n",
        "  return batch"
      ],
      "metadata": {
        "id": "jpgwj8GIAgHL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jpgwj8GIAgHL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `apply_tokenize_function` makes use of the `map` function to transform each training and validation batch into a batch of model inputs, by actually calling the `tokenize_function`. Then, it converts the tokenized data into PyTorch Tensors to be trained on GPU."
      ],
      "metadata": {
        "id": "z-A8_a8hmu77"
      },
      "id": "z-A8_a8hmu77"
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history, ub):\n",
        "\n",
        "  \"\"\"\n",
        "  Applies a tokenization function to `dataset_train` and `dataset_val`, according to `history` and `ub` parameters.\n",
        "  \"\"\"\n",
        "\n",
        "  train_tokenized = dataset_train.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    fn_kwargs = {'history' : history, 'upper_bound': ub, 'encoder_max_length': encoder_max_length, 'decoder_max_length': decoder_max_length},\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"question\", \"passage\", \"answer\", 'history', 'passage_history', 'source', 'rationale', 'rationale_end']\n",
        "  )\n",
        "\n",
        "  val_tokenized = dataset_val.map(\n",
        "      tokenize_function,\n",
        "      batched=True,\n",
        "      fn_kwargs = {'history' : history, 'upper_bound': ub, 'encoder_max_length': encoder_max_length, 'decoder_max_length': decoder_max_length},\n",
        "      batch_size=batch_size,\n",
        "      remove_columns=[\"question\", \"passage\", \"answer\", 'history', 'passage_history', 'source', 'rationale', 'rationale_end']\n",
        "  )\n",
        "\n",
        "  train_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "  val_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "  return train_tokenized, val_tokenized"
      ],
      "metadata": {
        "id": "otokfmhW9cWQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "otokfmhW9cWQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fine-tuning of models is carried out using the `Seq2SeqTrainer` method, which allows to perform generation during evaluation when `predict_with_generate` argument is set to `True` in `Seq2SeqTrainingArguments`. Then, the generated output is passed as predictions to the `compute_metric` function, which can compute on the validation set the desired metric.\n",
        "\n",
        "Before starting the training, the `train_model` function configures the `Seq2SeqTrainingArguments`, by setting a $3$ epochs fine-tuning, using a linear scheduler with $10000$ warmup steps for the AdamW optimizer. Out of several experimental results, this setting was found to be the most effective. Then, `Seq2SeqTrainer` is runned on the tokenized training and validation sets and the model fine-tuning starts.\n",
        "\n",
        "Fine-tuned model and weights are finally saved for later reload.\n"
      ],
      "metadata": {
        "id": "PcyE3LQ8q-E9"
      },
      "id": "PcyE3LQ8q-E9"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_tokenized, val_tokenized, batch_size, save_path_local, save_path_drive, prediction_loss_only):\n",
        "\n",
        "  \"\"\"\n",
        "  Fine-tunes the `model` on `train_tokenized` set, evaluating predictions on `val_tokenized`, and finally saves the best model to `save_path_local` and\n",
        "  `save_path_drive`, locally and on drive respectively. If `prediction_loss_only = True`, when performing evaluation and generating predictions, it only returns the loss.\n",
        "  \"\"\"\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  training_args = Seq2SeqTrainingArguments(\n",
        "      predict_with_generate=True,\n",
        "      num_train_epochs=3,\n",
        "      evaluation_strategy=\"epoch\",\n",
        "      save_strategy=\"epoch\",\n",
        "      per_device_train_batch_size=batch_size,\n",
        "      per_device_eval_batch_size=batch_size,\n",
        "      fp16=True,\n",
        "      warmup_steps = 10000,\n",
        "      output_dir=\"models\",\n",
        "      logging_steps=2,\n",
        "      prediction_loss_only=prediction_loss_only,\n",
        "      report_to = 'none',\n",
        "      load_best_model_at_end = True)\n",
        "\n",
        "  trainer = Seq2SeqTrainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      compute_metrics=compute_metric,\n",
        "      train_dataset=train_tokenized,\n",
        "      eval_dataset=val_tokenized)\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  # Saving the model locally\n",
        "  trainer.save_model(save_path_local)\n",
        "\n",
        "  # Saving the model on drive\n",
        "  trainer.save_model(save_path_drive)"
      ],
      "metadata": {
        "id": "a7GfCNAd-eVd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "a7GfCNAd-eVd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `compute_metric` function evaluates the model on the validation set using the SQuAD F1-score. In particular, it takes as input labels and predictions obtained with generation and decodes both of them. Since `squad.compute_f1` only computes the SQuAD F1-score between pairs of strings, the total one is computed as the average between the scores of all the *(prediction, label)* pairs."
      ],
      "metadata": {
        "id": "9zWCy4MdBErg"
      },
      "id": "9zWCy4MdBErg"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metric(pred):\n",
        "\n",
        "    \"\"\"\n",
        "    Evaluates the model on the validation set using the SQuAD F1-score on `pred` predictions and labels.\n",
        "    \"\"\"\n",
        "\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    squad_score = np.mean([squad.compute_f1(pred_str[i], label_str[i]) for i in range(0,len(pred_str))])\n",
        "\n",
        "    return {\"f1_squad_score\": squad_score}"
      ],
      "metadata": {
        "id": "GhxOrRYYAsIP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GhxOrRYYAsIP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `generate_answers` function first tokenizes and transforms the desired dataset in model inputs. Truncation and padding are handled as for the `tokenize_function`. Then, it runs on the model the `generate` function, and after a decoding process the predicted answers are obtained.\n"
      ],
      "metadata": {
        "id": "bTH8FAtEGCcY"
      },
      "id": "bTH8FAtEGCcY"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answers(batch, history, ub, model, encoder_max_length, decoder_max_length):\n",
        "\n",
        "    \"\"\"\n",
        "    Transforms the `batch` of data into a batch of model inputs by a tokenization process and feeds the `model` for generating the predicted answers.\n",
        "    Eventual truncation is performed according to `encoder_max_length` and `decoder_max_length`.\n",
        "    With `history=True` and `upper_bound=True`, input is prepared and tokenized as needed for the history and upper bound model versions respectively.\n",
        "    \"\"\"\n",
        "\n",
        "    if history:\n",
        "      inputs = tokenizer(batch[\"question\"], batch[\"passage_history\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length, return_tensors=\"pt\")\n",
        "    elif ub:\n",
        "      inputs = tokenizer(batch[\"question\"], batch[\"rationale\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length, return_tensors=\"pt\")\n",
        "    else:\n",
        "      inputs = tokenizer(batch[\"question\"], batch[\"passage\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length, return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = inputs.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    batch[\"pred_answer\"] = output_str\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "J0djxWNGAxKe"
      },
      "execution_count": null,
      "outputs": [],
      "id": "J0djxWNGAxKe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `apply_generate_answers` function makes use of the `map` function to obtain model's predicted answers for each question in the dataset passed as input."
      ],
      "metadata": {
        "id": "0m_-aEDNKC9_"
      },
      "id": "0m_-aEDNKC9_"
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_generate_answers(dataset, model, encoder_max_length, decoder_max_length, batch_size, history, ub):\n",
        "\n",
        "  \"\"\"\n",
        "  Generates `model`'s predicted answer for each question in `dataset`.\n",
        "  \"\"\"\n",
        "\n",
        "  results = dataset.map(generate_answers,\n",
        "                        batched=True,\n",
        "                        fn_kwargs = {'history' : history, 'ub' : ub, 'model': model, 'encoder_max_length': encoder_max_length, 'decoder_max_length': decoder_max_length},\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "oXCHdmSh_vOH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "oXCHdmSh_vOH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `compute_f1_squad` function allows to evaluate models by computing the SQuAD F1-score on results obtained as the average between the SQuAD F1-score of all the *(predicted answer, true answer)* pairs."
      ],
      "metadata": {
        "id": "-1hE6_-5LOA1"
      },
      "id": "-1hE6_-5LOA1"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_f1_squad(results):\n",
        "\n",
        "  \"\"\"\n",
        "  Computes the SQuAD F1-score on `results`.\n",
        "  \"\"\"\n",
        "\n",
        "  squad_score = np.mean([squad.compute_f1(results['pred_answer'][i], results['answer'][i]) for i in range(0,len(results['pred_answer']))])\n",
        "\n",
        "  return squad_score"
      ],
      "metadata": {
        "id": "0q91H9JtAD_M"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0q91H9JtAD_M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For loading saved fine-tuned models, the `reload_model` is used."
      ],
      "metadata": {
        "id": "iIzuyZ0zM07E"
      },
      "id": "iIzuyZ0zM07E"
    },
    {
      "cell_type": "code",
      "source": [
        "def reload_model(path, model_name):\n",
        "\n",
        "  \"\"\"\n",
        "  Loads from `path` the model whose `model_name` is passed as input.\n",
        "  \"\"\"\n",
        "\n",
        "  model = EncoderDecoderModel.from_pretrained(path)\n",
        "  model.to('cuda')\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  return model, tokenizer"
      ],
      "metadata": {
        "id": "3KTLJAl3CF9Z"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3KTLJAl3CF9Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to visually compare predicted and true answers, `show_results` function can be used for printing the passage and the whole conversation of a specific `sample` of data."
      ],
      "metadata": {
        "id": "C7C6sZHMNbE7"
      },
      "id": "C7C6sZHMNbE7"
    },
    {
      "cell_type": "code",
      "source": [
        "def show_results(results, sample):\n",
        "\n",
        "  \"\"\"\n",
        "  Prints the text passage and the whole conversation of a specific `sample`, showing for each Q&A the predicted answer contained in `results`.\n",
        "  \"\"\"\n",
        "\n",
        "  print(f\"PASSAGE:\\n\")\n",
        "\n",
        "  id = results['id'][sample]\n",
        "  passage = results['passage'][sample]\n",
        "\n",
        "  print(f\"{passage}\")\n",
        "  print()\n",
        "\n",
        "  print(\"CONVERSATION STRUCTURE:\\n\")\n",
        "\n",
        "  for idx, row in results.iterrows():\n",
        "\n",
        "    if(id == row['id']):\n",
        "      question = row['question']\n",
        "      answer = row['answer']\n",
        "      pred_answer = row['pred_answer']\n",
        "\n",
        "      print(f\"Turn {idx+1}\")\n",
        "      print(f\"Q: {question}\")\n",
        "      print(f\"A: {answer}\")\n",
        "      print(f\"PA: {pred_answer}\")\n",
        "      print()\n",
        "\n",
        "    else:\n",
        "      break"
      ],
      "metadata": {
        "id": "7AqFq9P_i2LD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7AqFq9P_i2LD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTTiny models"
      ],
      "metadata": {
        "id": "eHrBUUP14zqI"
      },
      "id": "eHrBUUP14zqI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first model to be implemented is BERTTiny-based. Both its baseline and its history version are fine-tuned and evaluated using $3$ different seeds ($42$, $2022$, $1337$)."
      ],
      "metadata": {
        "id": "9Ht3VPkUPRzM"
      },
      "id": "9Ht3VPkUPRzM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n"
      ],
      "metadata": {
        "id": "LUOqCeaztQF6"
      },
      "id": "LUOqCeaztQF6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the BERTTiny tokenizer from HuggingFace.\n",
        "The generic tokenizer class `AutoTokenizer` allows to instantiate a predefined tokenizer hosted inside the pretrained model repo on HuggingFace. This can be done using the `from_pretrained` method and passing the pre-trained model name."
      ],
      "metadata": {
        "id": "Fd4_yR18PGPM"
      },
      "id": "Fd4_yR18PGPM"
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tiny = \"prajjwal1/bert-tiny\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_tiny)\n",
        "passage_max_length, answer_max_length = check_passage_answer_max_length(tokenizer, df_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otfYnhYbsdYu",
        "outputId": "8a023f38-12db-4a7b-f5bf-2023d1b3356e"
      },
      "id": "otfYnhYbsdYu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passage max length: 460.0\n",
            "Answer max length: 9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the `check_passage_answer_max_length` function, it turns out that the maximum passages length of the $95$% of training passages is $460$. Then, the `encoder_max_length` can safely be set to the maximum number of tokens accepted in input by BERT encoders, which is $512$.\n",
        "Also, the maximum answers length as to handle the $90$% of training answers is 9, which can be used as `decoder_max_length`."
      ],
      "metadata": {
        "id": "fXEJRg5rDfM5"
      },
      "id": "fXEJRg5rDfM5"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_max_length = 512\n",
        "decoder_max_length = int(answer_max_length)\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "SPGjWiMqu7KH"
      },
      "id": "SPGjWiMqu7KH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After several experiments, the most effective `batch_size` was found to be $16$."
      ],
      "metadata": {
        "id": "6_UN_XfnRBdg"
      },
      "id": "6_UN_XfnRBdg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $f_\\theta(P, Q)$ implementation\n"
      ],
      "metadata": {
        "id": "cJpiUbwYRxqN"
      },
      "id": "cJpiUbwYRxqN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 42"
      ],
      "metadata": {
        "id": "2HIgfC44Ei4K"
      },
      "id": "2HIgfC44Ei4K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training the model, it's important to set the reproducibility, in this case using a seed equal to $42$. Then the model is defined and the train and validation sets are tokenized."
      ],
      "metadata": {
        "id": "1Fsx2E4yGSGZ"
      },
      "id": "1Fsx2E4yGSGZ"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 42)\n",
        "bert2bert = define_model(bert_tiny, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "KQG5wefhxiiy"
      },
      "id": "KQG5wefhxiiy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's fine-tune the BERTTiny model and save its weights after the training.\n"
      ],
      "metadata": {
        "id": "ZPzO3yr6HEPe"
      },
      "id": "ZPzO3yr6HEPe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa717d6b-8d4e-43f5-bbf8-e192da766d0d",
        "id": "CFt-lkPePsb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 35:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Squad Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.730300</td>\n",
              "      <td>3.637205</td>\n",
              "      <td>0.112784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.184000</td>\n",
              "      <td>3.335953</td>\n",
              "      <td>0.141454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.580000</td>\n",
              "      <td>3.252790</td>\n",
              "      <td>0.150279</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-5364\n",
            "Configuration saved in model/checkpoint-5364/config.json\n",
            "Model weights saved in model/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-10728\n",
            "Configuration saved in model/checkpoint-10728/config.json\n",
            "Model weights saved in model/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-16092\n",
            "Configuration saved in model/checkpoint-16092/config.json\n",
            "Model weights saved in model/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model/checkpoint-16092 (score: 3.2527899742126465).\n",
            "Saving model checkpoint to model/bert-tiny\n",
            "Configuration saved in model/bert-tiny/config.json\n",
            "Model weights saved in model/bert-tiny/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(bert2bert,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/bert-tiny_42',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_42',\n",
        "            prediction_loss_only = False)"
      ],
      "id": "CFt-lkPePsb1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code can be used to load the previously fine-tuned weights."
      ],
      "metadata": {
        "id": "96Qb3JbDIT9J"
      },
      "id": "96Qb3JbDIT9J"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_42', bert_tiny)"
      ],
      "metadata": {
        "id": "0RAX8qb4B1Z5"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0RAX8qb4B1Z5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's use the `apply_generate_answers` function to let the model generating predicted answers on the test set.\n"
      ],
      "metadata": {
        "id": "b8jpVNJ3OXP3"
      },
      "id": "b8jpVNJ3OXP3"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_results = apply_generate_answers(dataset_test, bert2bert, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "1i6MRPjECdsf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1i6MRPjECdsf"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score = compute_f1_squad(bert2bert_results)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34576ece-b061-4b9c-c22b-f2278b8c0df6",
        "id": "63nIQieuQdiU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.14388569014890434\n"
          ]
        }
      ],
      "id": "63nIQieuQdiU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the obtained results it can be seen that BERTTiny has really poor performances.\n",
        "\n",
        "Finally, let's have a visual inspection of the predicted answers for a single passage.\n"
      ],
      "metadata": {
        "id": "Kp4moyDGItq0"
      },
      "id": "Kp4moyDGItq0"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=bert2bert_results), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDe5qzTGDRVX",
        "outputId": "999e68e4-6073-4770-92fc-12ac186866f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: red\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: yes\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: his mother\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: red\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: he wanted to eat a dog\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: his mother\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: a dog\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: a dog\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "VDe5qzTGDRVX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 2022"
      ],
      "metadata": {
        "id": "GMkVJfxAElJa"
      },
      "id": "GMkVJfxAElJa"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 2022)\n",
        "bert2bert = define_model(bert_tiny, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "WjA85UGREqCM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WjA85UGREqCM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWzmsrxJow85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99cd121a-3586-43f4-9c73-3b822af8ad06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 27:29, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Squad Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.693900</td>\n",
              "      <td>3.602125</td>\n",
              "      <td>0.116535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.166400</td>\n",
              "      <td>3.336388</td>\n",
              "      <td>0.144695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.588000</td>\n",
              "      <td>3.251170</td>\n",
              "      <td>0.152582</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-5364\n",
            "Configuration saved in model/checkpoint-5364/config.json\n",
            "Model weights saved in model/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-10728\n",
            "Configuration saved in model/checkpoint-10728/config.json\n",
            "Model weights saved in model/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-16092\n",
            "Configuration saved in model/checkpoint-16092/config.json\n",
            "Model weights saved in model/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model/checkpoint-16092 (score: 3.2511703968048096).\n",
            "Saving model checkpoint to model/bert-tiny\n",
            "Configuration saved in model/bert-tiny/config.json\n",
            "Model weights saved in model/bert-tiny/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(bert2bert,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/bert-tiny_2022',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_2022',\n",
        "            prediction_loss_only = False)"
      ],
      "id": "qWzmsrxJow85"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_2022', bert_tiny)"
      ],
      "metadata": {
        "id": "xO5ApUxbEqCN"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xO5ApUxbEqCN"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_results = apply_generate_answers(dataset_test, bert2bert, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "yFrHRHC1EqCO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yFrHRHC1EqCO"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score = compute_f1_squad(bert2bert_results)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2b4b1a-2197-49c7-8560-93bcf5069883",
        "id": "F5AgU_Qj6J4V"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.14978909467032372\n"
          ]
        }
      ],
      "id": "F5AgU_Qj6J4V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing the seed does not highly influences the model's results."
      ],
      "metadata": {
        "id": "Rld0L3iMh90U"
      },
      "id": "Rld0L3iMh90U"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=bert2bert_results), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp7kKERoFpMs",
        "outputId": "15de737a-d9bc-4a60-eee7-29754fe5254d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: black and white\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: her mother\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: black and white\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: a piggy.\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: his mother\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: a dog\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: a dog\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "Pp7kKERoFpMs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 1337"
      ],
      "metadata": {
        "id": "9fzyGGfMEnE-"
      },
      "id": "9fzyGGfMEnE-"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 1337)\n",
        "bert2bert = define_model(bert_tiny, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "x37ektAzEqr6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "x37ektAzEqr6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f09c56d5-f9ea-4f3d-bbee-19ff5d38190c",
        "id": "GFcvXglC1l_i"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 37:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Squad Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.745900</td>\n",
              "      <td>3.640102</td>\n",
              "      <td>0.111210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.197700</td>\n",
              "      <td>3.340055</td>\n",
              "      <td>0.142545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.622800</td>\n",
              "      <td>3.253203</td>\n",
              "      <td>0.150962</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-5364\n",
            "Configuration saved in model/checkpoint-5364/config.json\n",
            "Model weights saved in model/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-10728\n",
            "Configuration saved in model/checkpoint-10728/config.json\n",
            "Model weights saved in model/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-16092\n",
            "Configuration saved in model/checkpoint-16092/config.json\n",
            "Model weights saved in model/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model/checkpoint-16092 (score: 3.2532026767730713).\n",
            "Saving model checkpoint to model/bert-tiny\n",
            "Configuration saved in model/bert-tiny/config.json\n",
            "Model weights saved in model/bert-tiny/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(bert2bert,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/bert-tiny_1337',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_1337',\n",
        "            prediction_loss_only=False)"
      ],
      "id": "GFcvXglC1l_i"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_1337', bert_tiny)"
      ],
      "metadata": {
        "id": "lEBHhhvnEqr7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lEBHhhvnEqr7"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_results = apply_generate_answers(dataset_test, bert2bert, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "COTKGDzsEqr7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "COTKGDzsEqr7"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score = compute_f1_squad(bert2bert_results)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e689bf-1531-4fa2-b448-95e377a7b0bd",
        "id": "C80sL5xlKcXA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.14828039654819314\n"
          ]
        }
      ],
      "id": "C80sL5xlKcXA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, it can be noticed that training and evaluating the model with different seeds leads to the same poor performances."
      ],
      "metadata": {
        "id": "mrACfIsKiQfq"
      },
      "id": "mrACfIsKiQfq"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=bert2bert_results), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf90a7b-d721-415b-cad4-d77ebbe670a7",
        "id": "UGcVfyB2KcXA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: black and white\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: her mother\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: blue\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: she went to the kitchen\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: her mother\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: a little dog\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: a dog\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "UGcVfyB2KcXA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $f_\\theta(P, Q, H)$ implementation\n"
      ],
      "metadata": {
        "id": "q2GIvfwbL_MA"
      },
      "id": "q2GIvfwbL_MA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 42"
      ],
      "metadata": {
        "id": "b0HzTWLjL_MB"
      },
      "id": "b0HzTWLjL_MB"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 42)\n",
        "bert2bert_h = define_model(bert_tiny, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "BWoZYG6rL_MB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BWoZYG6rL_MB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59ccbdd3-1c89-451c-9687-3070df667d2f",
        "id": "j_ZWNGuDYXLH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 38:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Squad Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.736500</td>\n",
              "      <td>3.616460</td>\n",
              "      <td>0.119107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.170800</td>\n",
              "      <td>3.341781</td>\n",
              "      <td>0.144172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.588700</td>\n",
              "      <td>3.253925</td>\n",
              "      <td>0.151068</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to models/checkpoint-5364\n",
            "Configuration saved in models/checkpoint-5364/config.json\n",
            "Model weights saved in models/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to models/checkpoint-10728\n",
            "Configuration saved in models/checkpoint-10728/config.json\n",
            "Model weights saved in models/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to models/checkpoint-16092\n",
            "Configuration saved in models/checkpoint-16092/config.json\n",
            "Model weights saved in models/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from models/checkpoint-16092 (score: 3.253925085067749).\n",
            "Saving model checkpoint to models/bert-tiny_history_42\n",
            "Configuration saved in models/bert-tiny_history_42/config.json\n",
            "Model weights saved in models/bert-tiny_history_42/pytorch_model.bin\n",
            "Saving model checkpoint to drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_history_42\n",
            "Configuration saved in drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_history_42/config.json\n",
            "Model weights saved in drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_history_42/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(bert2bert_h,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/bert-tiny_history_42',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_history_42',\n",
        "            prediction_loss_only=False)"
      ],
      "id": "j_ZWNGuDYXLH"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_h, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_history_42', bert_tiny)"
      ],
      "metadata": {
        "id": "Wq2pB5EfL_MD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Wq2pB5EfL_MD"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_h_results = apply_generate_answers(dataset_test, bert2bert_h, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "2zwAo2Z4L_ME"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2zwAo2Z4L_ME"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score = compute_f1_squad(bert2bert_h_results)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6397e77b-7ac6-4420-959d-89e5db47b554",
        "id": "npM0mu8fYfeK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.14817783625390293\n"
          ]
        }
      ],
      "id": "npM0mu8fYfeK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the obtained results it can be seen that BERTTiny history version has very poor performances, really close to the baseline ones."
      ],
      "metadata": {
        "id": "bTJhtVbnkPZB"
      },
      "id": "bTJhtVbnkPZB"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=bert2bert_h_results), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWaJHkD13wmU",
        "outputId": "28537e8b-1189-45f9-b200-b69f2cdd7846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: red\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: in the woods\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: her mother\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: black\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: she wanted to see her.\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: her mother\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: she was a dog\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: he's a dog\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "iWaJHkD13wmU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 2022"
      ],
      "metadata": {
        "id": "hUl5KjgNNpdd"
      },
      "id": "hUl5KjgNNpdd"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 2022)\n",
        "bert2bert_h = define_model(bert_tiny, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "uiMmQ09eNpdd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "uiMmQ09eNpdd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f7c3aee6-3bf3-42c3-c3a9-0ad4e63c4483",
        "id": "U-dk5KelIcLO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 27:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Squad Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.776600</td>\n",
              "      <td>3.685289</td>\n",
              "      <td>0.107246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.184300</td>\n",
              "      <td>3.337252</td>\n",
              "      <td>0.143185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.581400</td>\n",
              "      <td>3.253896</td>\n",
              "      <td>0.150533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-5364\n",
            "Configuration saved in model_history/checkpoint-5364/config.json\n",
            "Model weights saved in model_history/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-10728\n",
            "Configuration saved in model_history/checkpoint-10728/config.json\n",
            "Model weights saved in model_history/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-16092\n",
            "Configuration saved in model_history/checkpoint-16092/config.json\n",
            "Model weights saved in model_history/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model_history/checkpoint-16092 (score: 3.253896474838257).\n",
            "Saving model checkpoint to model_history/bert-tiny\n",
            "Configuration saved in model_history/bert-tiny/config.json\n",
            "Model weights saved in model_history/bert-tiny/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(bert2bert_h,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/bert-tiny_history_2022',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_history_2022',\n",
        "            prediction_loss_only = False)"
      ],
      "id": "U-dk5KelIcLO"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_h, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_history_2022', bert_tiny)"
      ],
      "metadata": {
        "id": "18BtUhKDNpde"
      },
      "execution_count": null,
      "outputs": [],
      "id": "18BtUhKDNpde"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_h_results = apply_generate_answers(dataset_test, bert2bert_h, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "rWKEhe02Npde"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rWKEhe02Npde"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score = compute_f1_squad(bert2bert_h_results)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql01X95EFpMv",
        "outputId": "6a730097-f20a-42f8-973f-cfc81907cf73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.14840237583158428\n"
          ]
        }
      ],
      "id": "ql01X95EFpMv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also in this case, the performances do not change when changing the seed."
      ],
      "metadata": {
        "id": "ulQS95EYksXU"
      },
      "id": "ulQS95EYksXU"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=bert2bert_h_results), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc7324c-6bb0-416b-e4fd-3080ae801f48",
        "id": "VcxBuIIyFpMv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: black and white\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: yes\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: his mother\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: black and white\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: she had a little dog\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: his mother\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: a dog\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: a dog\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "VcxBuIIyFpMv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 1337"
      ],
      "metadata": {
        "id": "jhWlr0yUNpsi"
      },
      "id": "jhWlr0yUNpsi"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 1337)\n",
        "bert2bert_h = define_model(bert_tiny, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "l8E89eIHNpsj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "l8E89eIHNpsj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "503ad0cc-b550-4eb6-df77-97c14c34e077",
        "id": "POsqkdb84Ym6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 37:19, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Squad Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.776600</td>\n",
              "      <td>3.685289</td>\n",
              "      <td>0.104166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.184300</td>\n",
              "      <td>3.337252</td>\n",
              "      <td>0.141192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.581400</td>\n",
              "      <td>3.253896</td>\n",
              "      <td>0.150398</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-5364\n",
            "Configuration saved in model_history/checkpoint-5364/config.json\n",
            "Model weights saved in model_history/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-10728\n",
            "Configuration saved in model_history/checkpoint-10728/config.json\n",
            "Model weights saved in model_history/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-16092\n",
            "Configuration saved in model_history/checkpoint-16092/config.json\n",
            "Model weights saved in model_history/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model_history/checkpoint-16092 (score: 3.253896474838257).\n",
            "Saving model checkpoint to model_history/bert-tiny\n",
            "Configuration saved in model_history/bert-tiny/config.json\n",
            "Model weights saved in model_history/bert-tiny/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(bert2bert_h,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/bert-tiny_history_1337',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_history_1337',\n",
        "            prediction_loss_only = False)"
      ],
      "id": "POsqkdb84Ym6"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_h, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_history_1337', bert_tiny)"
      ],
      "metadata": {
        "id": "V8-81bz7Npsk"
      },
      "execution_count": null,
      "outputs": [],
      "id": "V8-81bz7Npsk"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_h_results = apply_generate_answers(dataset_test, bert2bert_h, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "InedRxKvNpsk"
      },
      "execution_count": null,
      "outputs": [],
      "id": "InedRxKvNpsk"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score = compute_f1_squad(bert2bert_h_results)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score)"
      ],
      "metadata": {
        "id": "VVzeLODrKcXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5458d145-ff75-4baf-9613-fa6770f5c3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.14593499057851528\n"
          ]
        }
      ],
      "id": "VVzeLODrKcXD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, a change in the seed does not influence the model performances.\n",
        "\n",
        "Finally, it can be observed that the SQuAD F1-score is almost equivalent across different seeds and different versions of the model."
      ],
      "metadata": {
        "id": "Q0aANLrUk-W5"
      },
      "id": "Q0aANLrUk-W5"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=bert2bert_h_results), 0)"
      ],
      "metadata": {
        "id": "3qB2CXLVKcXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf54b9f-ae3f-4080-9988-990290d4807d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: red\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: yes\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: his mother\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: red\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: he wanted to eat.\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: his mother\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: a dog\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: a dog\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "3qB2CXLVKcXD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upper bound model"
      ],
      "metadata": {
        "id": "KVPiSEhEPhTy"
      },
      "id": "KVPiSEhEPhTy"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 1337)\n",
        "bert2bert_ub = define_model(bert_tiny, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = False, ub = True)"
      ],
      "metadata": {
        "id": "fgaqGoxFPhTz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fgaqGoxFPhTz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b7beb1a8-4caa-483e-b186-2bee5c714125",
        "id": "26yaWrA7XpRU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 38:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Squad Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.441900</td>\n",
              "      <td>3.245790</td>\n",
              "      <td>0.141884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.413300</td>\n",
              "      <td>2.448279</td>\n",
              "      <td>0.313950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.797900</td>\n",
              "      <td>2.211749</td>\n",
              "      <td>0.376353</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to models_ub/checkpoint-5364\n",
            "Configuration saved in models_ub/checkpoint-5364/config.json\n",
            "Model weights saved in models_ub/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to models_ub/checkpoint-10728\n",
            "Configuration saved in models_ub/checkpoint-10728/config.json\n",
            "Model weights saved in models_ub/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to models_ub/checkpoint-16092\n",
            "Configuration saved in models_ub/checkpoint-16092/config.json\n",
            "Model weights saved in models_ub/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from models_ub/checkpoint-16092 (score: 2.2117486000061035).\n",
            "Saving model checkpoint to models_ub/bert-tiny_ub_1337\n",
            "Configuration saved in models_ub/bert-tiny_ub_1337/config.json\n",
            "Model weights saved in models_ub/bert-tiny_ub_1337/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(bert2bert_ub,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/bert-tiny_ub_1337',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_ub_1337',\n",
        "            prediction_loss_only = False)"
      ],
      "id": "26yaWrA7XpRU"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_ub, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/bert-tiny_ub_1337', bert_tiny)"
      ],
      "metadata": {
        "id": "-E0PcJpwPhTz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-E0PcJpwPhTz"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_ub_results = apply_generate_answers(dataset_test, bert2bert_ub, encoder_max_length, decoder_max_length, batch_size, history = False, ub = True)"
      ],
      "metadata": {
        "id": "9Auw4dZ4PhT0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9Auw4dZ4PhT0"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score = compute_f1_squad(bert2bert_ub_results)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bf6b3e-653f-411d-9546-9548ea68c6f1",
        "id": "269_m25AXpRY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.37618301537664406\n"
          ]
        }
      ],
      "id": "269_m25AXpRY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performances are better than previous models, but still no satisfactory, showing that neither the upper bound model can reach really high results.\n"
      ],
      "metadata": {
        "id": "r_TXv1X-mW7t"
      },
      "id": "r_TXv1X-mW7t"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=bert2bert_ub_results), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abf0c38-bc05-4e58-b372-9fb09f57532e",
        "id": "yEYB_5emXpRZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: a white dog\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: a small dog\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: yes\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: her sisters and sisters\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: black black black and black black\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: she wanted to paint\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: a white white house\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: hugged her.\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: in the kitchen\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: her mouth\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: no\n",
            "\n"
          ]
        }
      ],
      "id": "yEYB_5emXpRZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistilRoBERTa models"
      ],
      "metadata": {
        "id": "Zyw9ky-qqEAO"
      },
      "id": "Zyw9ky-qqEAO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, DistilRoBERTa-based model is implemented. As before, both its baseline and its history version are fine-tuned and evaluated using  3  different seeds ($42$, $2022$, $1337$)."
      ],
      "metadata": {
        "id": "2rFNFYUCZY-g"
      },
      "id": "2rFNFYUCZY-g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "6FpNby15vLRc"
      },
      "id": "6FpNby15vLRc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the DistilRoBERTa tokenizer from HuggingFace as done for BERTTiny model."
      ],
      "metadata": {
        "id": "TQ4mli_lZKCt"
      },
      "id": "TQ4mli_lZKCt"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_name = \"distilroberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(distilroberta_name)\n",
        "passage_max_length, answer_max_length = check_passage_answer_max_length(tokenizer, df_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a023f38-12db-4a7b-f5bf-2023d1b3356e",
        "id": "xf2-vRyYwR1x"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passage max length: 460.0\n",
            "Answer max length: 9.0\n"
          ]
        }
      ],
      "id": "xf2-vRyYwR1x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the `check_passage_answer_max_length` function, this time using the DistilRoBERTa tokenizer, results appear to be the same as before. Then, the `encoder_max_length` can safely be set to the maximum number of tokens accepting in input by RoBERTa encoders, which is $512$, and 9 can be used as `decoder_max_length`."
      ],
      "metadata": {
        "id": "gz8h9xjVZwWI"
      },
      "id": "gz8h9xjVZwWI"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_max_length = 512\n",
        "decoder_max_length = int(answer_max_length)\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "QZMF32h1vF-r"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QZMF32h1vF-r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `batch_size` is set again equal to $16$."
      ],
      "metadata": {
        "id": "5P0QFiR_aSgz"
      },
      "id": "5P0QFiR_aSgz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $f_\\theta(P, Q)$ implementation"
      ],
      "metadata": {
        "id": "9OKP5RbRQGQk"
      },
      "id": "9OKP5RbRQGQk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 42"
      ],
      "metadata": {
        "id": "eUTk_S79QGQl"
      },
      "id": "eUTk_S79QGQl"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 42)\n",
        "distilroberta = define_model(distilroberta_name, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "k9RSQ4iAQGQl"
      },
      "execution_count": null,
      "outputs": [],
      "id": "k9RSQ4iAQGQl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f39fba4-e91e-413f-c4a2-65ea3885eafc",
        "id": "FlWrI68kKnlQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 2:10:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.859300</td>\n",
              "      <td>3.000268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.073400</td>\n",
              "      <td>2.141485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.898500</td>\n",
              "      <td>1.750379</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-5364\n",
            "Configuration saved in model/checkpoint-5364/config.json\n",
            "Model weights saved in model/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-10728\n",
            "Configuration saved in model/checkpoint-10728/config.json\n",
            "Model weights saved in model/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-16092\n",
            "Configuration saved in model/checkpoint-16092/config.json\n",
            "Model weights saved in model/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model/checkpoint-16092 (score: 1.7503787279129028).\n",
            "Saving model checkpoint to model/distilroberta\n",
            "Configuration saved in model/distilroberta/config.json\n",
            "Model weights saved in model/distilroberta/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(distilroberta,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/distilroberta_42',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_42',\n",
        "            prediction_loss_only = True)"
      ],
      "id": "FlWrI68kKnlQ"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_42', distilroberta_name)"
      ],
      "metadata": {
        "id": "7CWrRBcDQGQm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7CWrRBcDQGQm"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_results_val = apply_generate_answers(dataset_val, distilroberta, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "zxx3FuVWe1nv"
      },
      "id": "zxx3FuVWe1nv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_val = compute_f1_squad(distilroberta_results_val)\n",
        "print(\"The SQuAD F1-score on the validation set is\", squad_score_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71076fa-0af0-4936-d100-b1642cac3fef",
        "id": "xawwQVHugXMW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the validation set is 0.5066541098857399\n"
          ]
        }
      ],
      "id": "xawwQVHugXMW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation on the validation set is done only at the end of the training for computational reasons."
      ],
      "metadata": {
        "id": "XqbbykPXtFZE"
      },
      "id": "XqbbykPXtFZE"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_results_test = apply_generate_answers(dataset_test, distilroberta, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "X_1bpbQVQGQm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "X_1bpbQVQGQm"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_test = compute_f1_squad(distilroberta_results_test)\n",
        "print(\"The SQuAD F1-scoreon the test set is\", squad_score_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SklKPhTFm-Dp",
        "outputId": "6ebc6deb-2215-43a5-9980-84958b4e597d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.5186477342626427\n"
          ]
        }
      ],
      "id": "SklKPhTFm-Dp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the obtained results on the test set it can be affirmed that the performances are much better than BERTTiny's, as there is a huge increment in the SQuAD F1-score."
      ],
      "metadata": {
        "id": "RGggLxFVuJmI"
      },
      "id": "RGggLxFVuJmI"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=distilroberta_results_test), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "speVIoGSnD62",
        "outputId": "c9195470-c831-4f02-ee0a-2b7cacbce065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: white\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: in a farm house\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: Cotton\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: orange\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: yes\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: She painted her face\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: Cotton\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: smiled her face\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: a bucket of water\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: dropped into the hay bed\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "speVIoGSnD62"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 2022"
      ],
      "metadata": {
        "id": "hn6ocq8gQuwc"
      },
      "id": "hn6ocq8gQuwc"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 2022)\n",
        "distilroberta = define_model(distilroberta_name, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "tJsgwnKHQuwc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tJsgwnKHQuwc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32cdc2ed-72de-48d4-8635-29a919d7b5c7",
        "id": "_bmlGBZWqmgW"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5268' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5268/16092 38:04 < 1:18:16, 2.30 it/s, Epoch 0.98/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 2:05:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.785800</td>\n",
              "      <td>2.952650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.178600</td>\n",
              "      <td>2.111102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.924100</td>\n",
              "      <td>1.748275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-5364\n",
            "Configuration saved in model/checkpoint-5364/config.json\n",
            "Model weights saved in model/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-10728\n",
            "Configuration saved in model/checkpoint-10728/config.json\n",
            "Model weights saved in model/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-16092\n",
            "Configuration saved in model/checkpoint-16092/config.json\n",
            "Model weights saved in model/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model/checkpoint-16092 (score: 1.7482746839523315).\n",
            "Saving model checkpoint to model/distilroberta\n",
            "Configuration saved in model/distilroberta/config.json\n",
            "Model weights saved in model/distilroberta/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(distilroberta,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/distilroberta_2022',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_2022',\n",
        "            prediction_loss_only = True)"
      ],
      "id": "_bmlGBZWqmgW"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_2022', distilroberta_name)"
      ],
      "metadata": {
        "id": "OHcy8KaeQuwd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OHcy8KaeQuwd"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_results_val = apply_generate_answers(dataset_val, distilroberta, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "5BjRHilxfTFJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5BjRHilxfTFJ"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_val = compute_f1_squad(distilroberta_results_val)\n",
        "print(\"The SQuAD F1-score on the validation set is\", squad_score_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6VN0mALdvsm",
        "outputId": "655e4765-36f7-4d5d-dc35-55c08a42ded1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the validation set is 0.5045086347069125\n"
          ]
        }
      ],
      "id": "-6VN0mALdvsm"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_results_test = apply_generate_answers(dataset_test, distilroberta, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "cyi0EBjvQuwd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cyi0EBjvQuwd"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_test = compute_f1_squad(distilroberta_results_test)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c40fe33-c72a-4236-e942-9318320611a9",
        "id": "krg_9vjkw9zE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.5153232856305855\n"
          ]
        }
      ],
      "id": "krg_9vjkw9zE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As for BERTTiny, a change in seed does not highly influence the model's results.  \n"
      ],
      "metadata": {
        "id": "Hpjun0rNvEaa"
      },
      "id": "Hpjun0rNvEaa"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=distilroberta_results_test), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bccb7ba0-d9ba-4a20-c245-e47de78f6768",
        "id": "Jg42MI9xxBL9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: white\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: in a barn\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: Cotton kitten\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: orange\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: yes\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: tried to paint her hair\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: Cotton\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: She laughed.\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: a big bucket of water\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: She picked up her mom\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "Jg42MI9xxBL9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 1337"
      ],
      "metadata": {
        "id": "5s_CrbQzQu9R"
      },
      "id": "5s_CrbQzQu9R"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 1337)\n",
        "distilroberta = define_model(distilroberta_name, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "nT9LInq_Qu9S"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nT9LInq_Qu9S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5abc8ed8-a9e2-49fd-8a31-afd0c4173b38",
        "id": "Ojzh7jaP09qj"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10166' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10166/16092 1:17:52 < 45:24, 2.18 it/s, Epoch 1.90/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.919700</td>\n",
              "      <td>3.047958</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-5364\n",
            "Configuration saved in model/checkpoint-5364/config.json\n",
            "Model weights saved in model/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 2:07:48, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.919700</td>\n",
              "      <td>3.047958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.046100</td>\n",
              "      <td>2.137752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.930800</td>\n",
              "      <td>1.773598</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-10728\n",
            "Configuration saved in model/checkpoint-10728/config.json\n",
            "Model weights saved in model/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model/checkpoint-16092\n",
            "Configuration saved in model/checkpoint-16092/config.json\n",
            "Model weights saved in model/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model/checkpoint-16092 (score: 1.7735984325408936).\n",
            "Saving model checkpoint to model/distilroberta\n",
            "Configuration saved in model/distilroberta/config.json\n",
            "Model weights saved in model/distilroberta/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(distilroberta,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/distilroberta_1337',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_1337',\n",
        "            prediction_loss_only=True)"
      ],
      "id": "Ojzh7jaP09qj"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_1337', distilroberta_name)"
      ],
      "metadata": {
        "id": "2PdjVrydQu9S"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2PdjVrydQu9S"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_results_val = apply_generate_answers(dataset_val, distilroberta, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "FOwzdZVCgIo8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FOwzdZVCgIo8"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_val = compute_f1_squad(distilroberta_results_val)\n",
        "print(\"The SQuAD F1-score on the validation set is\", squad_score_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w___HB0wgJSw",
        "outputId": "e3febc0c-910a-469d-a8ed-7b4c5b1e05e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the validation set is 0.5046266199690509\n"
          ]
        }
      ],
      "id": "w___HB0wgJSw"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_results_test = apply_generate_answers(dataset_test, distilroberta, encoder_max_length, decoder_max_length, batch_size, history = False, ub = False)"
      ],
      "metadata": {
        "id": "lcJgw0w0Qu9T"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lcJgw0w0Qu9T"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_test = compute_f1_squad(distilroberta_results_test)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9eb109-c40a-41ca-f29b-8b0aad933775",
        "id": "8X4und3O8Why"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.5180209851322327\n"
          ]
        }
      ],
      "id": "8X4und3O8Why"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, it can be noticed that training and evaluating the model with different seeds leads to the same SQuAD F1-score on the test set."
      ],
      "metadata": {
        "id": "reWS_A1DvYm3"
      },
      "id": "reWS_A1DvYm3"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=distilroberta_results_test), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718d3b45-6231-4f02-b871-a67f1aedf3ab",
        "id": "k9NDkov88cr2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: white\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: in a barn\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: Cotton\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: orange\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: yes\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: tried to paint her hair\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: Cotton\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: She laughed\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: in a big bucket\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: She threw it in the bucket\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "k9NDkov88cr2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $f_\\theta(P, Q, H)$ implementation"
      ],
      "metadata": {
        "id": "7jZutYVkQGQp"
      },
      "id": "7jZutYVkQGQp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 42"
      ],
      "metadata": {
        "id": "kdSbTQanQGQp"
      },
      "id": "kdSbTQanQGQp"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 42)\n",
        "distilroberta_h = define_model(distilroberta_name, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "bXZ3c38lQGQq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bXZ3c38lQGQq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62609373-3317-4e64-a790-ed73c33af9eb",
        "id": "4PzLKt-C3DQC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 2:09:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.871600</td>\n",
              "      <td>3.076537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.115000</td>\n",
              "      <td>1.886151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.877000</td>\n",
              "      <td>1.461600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-5364\n",
            "Configuration saved in model_history/checkpoint-5364/config.json\n",
            "Model weights saved in model_history/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-10728\n",
            "Configuration saved in model_history/checkpoint-10728/config.json\n",
            "Model weights saved in model_history/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-16092\n",
            "Configuration saved in model_history/checkpoint-16092/config.json\n",
            "Model weights saved in model_history/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model_history/checkpoint-16092 (score: 1.4615997076034546).\n",
            "Saving model checkpoint to model_history/distilroberta\n",
            "Configuration saved in model_history/distilroberta/config.json\n",
            "Model weights saved in model_history/distilroberta/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(distilroberta_h,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/distilroberta_history_42',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_history_42',\n",
        "            prediction_loss_only=True)"
      ],
      "id": "4PzLKt-C3DQC"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_h, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_history_42', distilroberta_name)"
      ],
      "metadata": {
        "id": "Y42Vc_RaQGQq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Y42Vc_RaQGQq"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_h_results_val = apply_generate_answers(dataset_val, distilroberta_h, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "k8RyM5x7gJzH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "k8RyM5x7gJzH"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_val = compute_f1_squad(distilroberta_h_results_val)\n",
        "print(\"The SQuAD F1-score on the validation set is\", squad_score_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eff0424-521b-4a99-d697-fb131dd5b546",
        "id": "IbtQBaTj1rSi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the validation set is 0.584847622060626\n"
          ]
        }
      ],
      "id": "IbtQBaTj1rSi"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_h_results_test = apply_generate_answers(dataset_test, distilroberta_h, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "0iAAsUxRQGQq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0iAAsUxRQGQq"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_test = compute_f1_squad(distilroberta_h_results_test)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045deefa-6296-4042-f6ac-d3f004f2ec8b",
        "id": "q6QBpD-Zgl15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.5977882571470061\n"
          ]
        }
      ],
      "id": "q6QBpD-Zgl15"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results on the test set show that the history version of the model performs better than the baseline, proving that feeding DistilRoBERTa with also conversation history information can have a positive impact."
      ],
      "metadata": {
        "id": "hXudS1kDw-jR"
      },
      "id": "hXudS1kDw-jR"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=distilroberta_h_results_test), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4030e548-0ced-4abf-9b3d-6dcd44342857",
        "id": "p-lEzbfbYs4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: white\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: a farm near the farm\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: her mom and five other sisters\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: orange and white\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: yes\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: painted her own paint\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: Cotton\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: tried to laugh\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: into a bucket of water\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: dropped her fur\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: no\n",
            "\n"
          ]
        }
      ],
      "id": "p-lEzbfbYs4d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 2022"
      ],
      "metadata": {
        "id": "KVOfcErPRWvo"
      },
      "id": "KVOfcErPRWvo"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 2022)\n",
        "distilroberta_h = define_model(distilroberta_name, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "XuYFlfyiRWvp"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XuYFlfyiRWvp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16e67616-7e65-4690-e1e6-3cda7dddd23a",
        "id": "93cBaOwlqmgY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 2:13:15, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.861400</td>\n",
              "      <td>2.983907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.126500</td>\n",
              "      <td>1.883478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.916800</td>\n",
              "      <td>1.478960</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-5364\n",
            "Configuration saved in model_history/checkpoint-5364/config.json\n",
            "Model weights saved in model_history/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-10728\n",
            "Configuration saved in model_history/checkpoint-10728/config.json\n",
            "Model weights saved in model_history/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-16092\n",
            "Configuration saved in model_history/checkpoint-16092/config.json\n",
            "Model weights saved in model_history/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model_history/checkpoint-16092 (score: 1.4789601564407349).\n",
            "Saving model checkpoint to model_history/distilroberta\n",
            "Configuration saved in model_history/distilroberta/config.json\n",
            "Model weights saved in model_history/distilroberta/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(distilroberta_h,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/distilroberta_history_2022',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_history_2022',\n",
        "            prediction_loss_only=True)"
      ],
      "id": "93cBaOwlqmgY"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_h, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_history_2022', distilroberta_name)"
      ],
      "metadata": {
        "id": "GYsAsEZeRWvq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GYsAsEZeRWvq"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_h_results_val = apply_generate_answers(dataset_val, distilroberta_h, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "Dqsi_893gYgd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Dqsi_893gYgd"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_val = compute_f1_squad(distilroberta_h_results_val)\n",
        "print(\"The SQuAD F1-score on the validation set is\", squad_score_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFn2y17qrrN8",
        "outputId": "e2a82d4c-032f-4937-8ca2-c46c058a6501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the validation set is 0.5820381792702394\n"
          ]
        }
      ],
      "id": "uFn2y17qrrN8"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_h_results_test = apply_generate_answers(dataset_test, distilroberta_h, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "5zriJbApRWvq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5zriJbApRWvq"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_test = compute_f1_squad(distilroberta_h_results_test)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1daf874e-9792-4f54-d09c-09c314b2d172",
        "id": "lH7DNly-mxRe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.5882844262239737\n"
          ]
        }
      ],
      "id": "lH7DNly-mxRe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The seed seems again to not influence performances."
      ],
      "metadata": {
        "id": "s0Ni-KxSyLN6"
      },
      "id": "s0Ni-KxSyLN6"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=distilroberta_h_results_test), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b3b4a7-aa32-448e-98ad-2d1f9e1afe85",
        "id": "PWQYnIYGm9QF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: orange\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: a farm near a farm\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: her mom\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: orange\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: tried to paint\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: Cotton\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: smiled\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: into a bucket of buckets\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: dropped her mom\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: no\n",
            "\n"
          ]
        }
      ],
      "id": "PWQYnIYGm9QF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed 1337"
      ],
      "metadata": {
        "id": "-B4rZj6yRW3Z"
      },
      "id": "-B4rZj6yRW3Z"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 1337)\n",
        "distilroberta_h = define_model(distilroberta_name, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "rOpwgna6RW3Z"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rOpwgna6RW3Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6fe3afb1-1c0a-49d5-bbac-ffaea35e52cd",
        "id": "jY5xBN7expMW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 2:08:12, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.832400</td>\n",
              "      <td>2.917188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.987400</td>\n",
              "      <td>1.882701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.830700</td>\n",
              "      <td>1.479111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-5364\n",
            "Configuration saved in model_history/checkpoint-5364/config.json\n",
            "Model weights saved in model_history/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-10728\n",
            "Configuration saved in model_history/checkpoint-10728/config.json\n",
            "Model weights saved in model_history/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_history/checkpoint-16092\n",
            "Configuration saved in model_history/checkpoint-16092/config.json\n",
            "Model weights saved in model_history/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model_history/checkpoint-16092 (score: 1.4791107177734375).\n",
            "Saving model checkpoint to model_history/distilroberta\n",
            "Configuration saved in model_history/distilroberta/config.json\n",
            "Model weights saved in model_history/distilroberta/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "train_model(distilroberta_h,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/distilroberta_history_1337',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_history_1337',\n",
        "            prediction_loss_only=True)"
      ],
      "id": "jY5xBN7expMW"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_h, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_history_1337', distilroberta_name)"
      ],
      "metadata": {
        "id": "ipCqH3FWRW3a"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ipCqH3FWRW3a"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_h_results_val = apply_generate_answers(dataset_val, distilroberta_h, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "Fl8eo6qJgjIX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Fl8eo6qJgjIX"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_val = compute_f1_squad(distilroberta_h_results_val)\n",
        "print(\"The SQuAD F1-score on the validation set is\", squad_score_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8986b0ee-59dd-49fe-dbb7-61c641bcb7a1",
        "id": "6k3_iy7Eu2o7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the validation set is 0.5831664601742995\n"
          ]
        }
      ],
      "id": "6k3_iy7Eu2o7"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_h_results_test = apply_generate_answers(dataset_test, distilroberta_h, encoder_max_length, decoder_max_length, batch_size, history = True, ub = False)"
      ],
      "metadata": {
        "id": "LXn59qGGRW3a"
      },
      "execution_count": null,
      "outputs": [],
      "id": "LXn59qGGRW3a"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score_test = compute_f1_squad(distilroberta_h_results_test)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ce30b8-7ead-4cd7-ab62-c4a23cad4cb1",
        "id": "h78S0XtD9G-k"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.5944254256911093\n"
          ]
        }
      ],
      "id": "h78S0XtD9G-k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As for the DistilRoBERTa baseline, performances are similar across different seeds."
      ],
      "metadata": {
        "id": "b-sOWbq2yOx2"
      },
      "id": "b-sOWbq2yOx2"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=distilroberta_h_results_test), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d762a5-19b1-427f-b410-bffcbf998cd0",
        "id": "NQLAqt9m9G-l"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
            "\n",
            "\"What are you doing, Cotton?!\" \n",
            "\n",
            "\"I only wanted to be more like you\". \n",
            "\n",
            "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
            "\n",
            "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
            "\n",
            "Then Cotton thought, \"I change my mind. I like being special\".\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: white\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: a barn near a farm house\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: her mom\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: orange\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: yes\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: painted it\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: Cotton\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: smiled her face\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: a big bucket of water\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: dropped it off\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: no\n",
            "\n"
          ]
        }
      ],
      "id": "NQLAqt9m9G-l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upper bound model"
      ],
      "metadata": {
        "id": "THzdUvRCQGQt"
      },
      "id": "THzdUvRCQGQt"
    },
    {
      "cell_type": "code",
      "source": [
        "set_reproducibility(seed = 42)\n",
        "distilroberta_ub = define_model(distilroberta_name, tokenizer)\n",
        "train_tokenized, val_tokenized = apply_tokenize_function(dataset_train, dataset_val, encoder_max_length, decoder_max_length, batch_size, history = False, ub = True)"
      ],
      "metadata": {
        "id": "JckYI3yyQGQv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JckYI3yyQGQv"
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(distilroberta_ub,\n",
        "            train_tokenized,\n",
        "            val_tokenized,\n",
        "            batch_size,\n",
        "            'models/distilroberta_ub_42',\n",
        "            'drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_ub_42',\n",
        "            prediction_loss_only = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d01cccf4-388b-4174-b906-66a0e1584d0f",
        "id": "uhPEvOsWqmgh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 85824\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 16092\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16092' max='16092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16092/16092 2:02:43, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.955400</td>\n",
              "      <td>1.806960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.329900</td>\n",
              "      <td>1.130685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.914500</td>\n",
              "      <td>0.875855</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_ub/checkpoint-5364\n",
            "Configuration saved in model_ub/checkpoint-5364/config.json\n",
            "Model weights saved in model_ub/checkpoint-5364/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_ub/checkpoint-10728\n",
            "Configuration saved in model_ub/checkpoint-10728/config.json\n",
            "Model weights saved in model_ub/checkpoint-10728/pytorch_model.bin\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: id. If id are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 21452\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to model_ub/checkpoint-16092\n",
            "Configuration saved in model_ub/checkpoint-16092/config.json\n",
            "Model weights saved in model_ub/checkpoint-16092/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from model_ub/checkpoint-16092 (score: 0.8758554458618164).\n",
            "Saving model checkpoint to model_ub/distilroberta\n",
            "Configuration saved in model_ub/distilroberta/config.json\n",
            "Model weights saved in model_ub/distilroberta/pytorch_model.bin\n"
          ]
        }
      ],
      "id": "uhPEvOsWqmgh"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_ub, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/NLP/models/distilroberta_ub_42', distilroberta_name)"
      ],
      "metadata": {
        "id": "8Qq3WpEeQGQv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8Qq3WpEeQGQv"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_ub_results = apply_generate_answers(dataset_test, distilroberta_ub, encoder_max_length, decoder_max_length, batch_size, history = False, ub = True)"
      ],
      "metadata": {
        "id": "9YoAHmNyQGQw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9YoAHmNyQGQw"
    },
    {
      "cell_type": "code",
      "source": [
        "squad_score = compute_f1_squad(distilroberta_ub_results)\n",
        "print(\"The SQuAD F1-score on the test set is\", squad_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJE0uAZZJya1",
        "outputId": "253e2a2d-67c9-44ea-ca1d-6e93215985fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The SQuAD F1-score on the test set is 0.7433871277191999\n"
          ]
        }
      ],
      "id": "aJE0uAZZJya1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performances of the model are quite good. Also, they are significantly higher with respect to BERTTiny upper bound ones."
      ],
      "metadata": {
        "id": "-2R-OXwAy7Be"
      },
      "id": "-2R-OXwAy7Be"
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(pd.DataFrame(data=distilroberta_ub_results), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gixQQHXXKTQ9",
        "outputId": "8e2755b8-b590-42a8-b44e-80456434a43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "\n",
            "CONVERSATION STRUCTURE:\n",
            "\n",
            "Turn 1\n",
            "Q: What color was Cotton?\n",
            "A: white\n",
            "PA: white\n",
            "\n",
            "Turn 2\n",
            "Q: Where did she live?\n",
            "A: in a barn\n",
            "PA: in a barn near a farm\n",
            "\n",
            "Turn 3\n",
            "Q: Did she live alone?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 4\n",
            "Q: Who did she live with?\n",
            "A: with her mommy and 5 sisters\n",
            "PA: her mommy and 5 other sisters\n",
            "\n",
            "Turn 5\n",
            "Q: What color were her sisters?\n",
            "A: orange and white\n",
            "PA: orange\n",
            "\n",
            "Turn 6\n",
            "Q: Was Cotton happy that she looked different than the rest of her family?\n",
            "A: no\n",
            "PA: no\n",
            "\n",
            "Turn 7\n",
            "Q: What did she do to try to make herself the same color as her sisters?\n",
            "A: she painted herself\n",
            "PA: she used to paint herself like them\n",
            "\n",
            "Turn 8\n",
            "Q: Whose paint was it?\n",
            "A: the farmer\n",
            "PA: the old farmer's orange\n",
            "\n",
            "Turn 9\n",
            "Q: What did Cotton's mother and siblings do when they saw her painted orange?\n",
            "A: they started laughing\n",
            "PA: They started laughing\n",
            "\n",
            "Turn 10\n",
            "Q: Where did Cotton's mother put her to clean the paint off?\n",
            "A: a bucket of water\n",
            "PA: a bucket of water\n",
            "\n",
            "Turn 11\n",
            "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
            "A: licked her face\n",
            "PA: licked her face\n",
            "\n",
            "Turn 12\n",
            "Q: Did they want Cotton to change the color of her fur?\n",
            "A: no\n",
            "PA: no\n",
            "\n"
          ]
        }
      ],
      "id": "gixQQHXXKTQ9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Analysis"
      ],
      "metadata": {
        "id": "TLI7HQCqcT05"
      },
      "id": "TLI7HQCqcT05"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better reason on errors causes, the two best baseline models per type are chosen and the analysis focuses on the $5$ hardest conversations per source, in which selected models mostly fail to answer questions."
      ],
      "metadata": {
        "id": "P7WgxsEqQHto"
      },
      "id": "P7WgxsEqQHto"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTTiny model"
      ],
      "metadata": {
        "id": "-fklSymEsbin"
      },
      "id": "-fklSymEsbin"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best performance of BERTTiny model on the test set has been obtained with the seed $2022$. Thus, its results are analyzed.\n",
        "\n",
        "For convenience reasons, `bert2bert_results` dataset is first converted into a `Dataframe`."
      ],
      "metadata": {
        "id": "9uboC5H4QZci"
      },
      "id": "9uboC5H4QZci"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_results = pd.DataFrame(bert2bert_results)"
      ],
      "metadata": {
        "id": "BQkz5Dj574dw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BQkz5Dj574dw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's compute the SQuAD F1-score on each Q&A pair and store the results."
      ],
      "metadata": {
        "id": "esBS4jW0QutS"
      },
      "id": "esBS4jW0QutS"
    },
    {
      "cell_type": "code",
      "source": [
        "bert2bert_results['f1_squad'] = bert2bert_results.apply(lambda x: squad.compute_f1(x.pred_answer, x.answer), axis=1)\n",
        "bert2bert_results.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "d93ef0ee-dbbe-4117-9b57-fca98f68fae4",
        "id": "ECS9VaXc74dx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               id  source  \\\n",
              "0  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "1  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "2  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "3  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "4  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "\n",
              "                                             passage  \\\n",
              "0  Once upon a time, in a barn near a farm house,...   \n",
              "1  Once upon a time, in a barn near a farm house,...   \n",
              "2  Once upon a time, in a barn near a farm house,...   \n",
              "3  Once upon a time, in a barn near a farm house,...   \n",
              "4  Once upon a time, in a barn near a farm house,...   \n",
              "\n",
              "                       question                        answer  \\\n",
              "0        What color was Cotton?                         white   \n",
              "1           Where did she live?                     in a barn   \n",
              "2           Did she live alone?                            no   \n",
              "3        Who did she live with?  with her mommy and 5 sisters   \n",
              "4  What color were her sisters?              orange and white   \n",
              "\n",
              "                                             history  \\\n",
              "0                                                      \n",
              "1                     What color was Cotton? white.    \n",
              "2  Where did she live? in a barn. What color was ...   \n",
              "3  Did she live alone? no. Where did she live? in...   \n",
              "4  Who did she live with? with her mommy and 5 si...   \n",
              "\n",
              "                                     passage_history  \\\n",
              "0  Once upon a time, in a barn near a farm house,...   \n",
              "1  Once upon a time, in a barn near a farm house,...   \n",
              "2  Once upon a time, in a barn near a farm house,...   \n",
              "3  Once upon a time, in a barn near a farm house,...   \n",
              "4  Once upon a time, in a barn near a farm house,...   \n",
              "\n",
              "                                           rationale  rationale_end  \\\n",
              "0                 a little white kitten named Cotton             93   \n",
              "1  in a barn near a farm house, there lived a lit...             80   \n",
              "2                                Cotton wasn't alone            215   \n",
              "3                 with her mommy and 5 other sisters            315   \n",
              "4  her sisters were all orange with beautiful whi...            490   \n",
              "\n",
              "   __index_level_0__      pred_answer  f1_squad  \n",
              "0                  0  black and white  0.500000  \n",
              "1                  1   in the kitchen  0.500000  \n",
              "2                  2               no  1.000000  \n",
              "3                  3       her mother  0.250000  \n",
              "4                  4  black and white  0.666667  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a8de670b-90d9-4a0d-89e6-b3d406f6b384\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>history</th>\n",
              "      <th>passage_history</th>\n",
              "      <th>rationale</th>\n",
              "      <th>rationale_end</th>\n",
              "      <th>__index_level_0__</th>\n",
              "      <th>pred_answer</th>\n",
              "      <th>f1_squad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>What color was Cotton?</td>\n",
              "      <td>white</td>\n",
              "      <td></td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>a little white kitten named Cotton</td>\n",
              "      <td>93</td>\n",
              "      <td>0</td>\n",
              "      <td>black and white</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Where did she live?</td>\n",
              "      <td>in a barn</td>\n",
              "      <td>What color was Cotton? white.</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>in a barn near a farm house, there lived a lit...</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>in the kitchen</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Did she live alone?</td>\n",
              "      <td>no</td>\n",
              "      <td>Where did she live? in a barn. What color was ...</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Cotton wasn't alone</td>\n",
              "      <td>215</td>\n",
              "      <td>2</td>\n",
              "      <td>no</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Who did she live with?</td>\n",
              "      <td>with her mommy and 5 sisters</td>\n",
              "      <td>Did she live alone? no. Where did she live? in...</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>with her mommy and 5 other sisters</td>\n",
              "      <td>315</td>\n",
              "      <td>3</td>\n",
              "      <td>her mother</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>What color were her sisters?</td>\n",
              "      <td>orange and white</td>\n",
              "      <td>Who did she live with? with her mommy and 5 si...</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>her sisters were all orange with beautiful whi...</td>\n",
              "      <td>490</td>\n",
              "      <td>4</td>\n",
              "      <td>black and white</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8de670b-90d9-4a0d-89e6-b3d406f6b384')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a8de670b-90d9-4a0d-89e6-b3d406f6b384 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a8de670b-90d9-4a0d-89e6-b3d406f6b384');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "id": "ECS9VaXc74dx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After grouping all passages by `source`, they are sorted by SQuAD F1-score, computed as the average of all SQuAD F1-scores of related Q&A pairs. Then, the $5$ hardest conversations per source, i.e. those with the lower score, are extracted."
      ],
      "metadata": {
        "id": "4U9-eMbDRXVY"
      },
      "id": "4U9-eMbDRXVY"
    },
    {
      "cell_type": "code",
      "source": [
        "worst_5_passages = bert2bert_results.groupby(['source', 'id'],\n",
        "                                             as_index=False).mean().drop(['rationale_end', '__index_level_0__'],\n",
        "                                                                         axis = 1).sort_values(['source','f1_squad']).groupby(['source']).head(5)\n",
        "worst_5_passages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "c20fb1f6-2a08-4225-f697-726b7be9b0a8",
        "id": "dV-25wzI74dx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        source                              id  f1_squad\n",
              "42         cnn  3gu1kf0o4i11dq9wdl6yo829jv9pbz       0.0\n",
              "52         cnn  3jwh6j9i9sd1a5xjx6t6kjxekv8bnk       0.0\n",
              "59         cnn  3lya37p8iqn02zcg0t1qsrgaqutkb2       0.0\n",
              "68         cnn  3p59jyt76lk5h527b9m7sp02f2n2t6       0.0\n",
              "88         cnn  3y54sxro1lle1hb9utwdy8vuyvituv       0.0\n",
              "107  gutenberg  34z02eimisdylvztwmit917niam0tf       0.0\n",
              "128  gutenberg  3cp1to84pt13w3rhad49p9uoz2c25w       0.0\n",
              "131  gutenberg  3efvcay5l39mph8rfwh40aqw343j88       0.0\n",
              "137  gutenberg  3h0w84iwbk2kw61v04cdub89nc4ere       0.0\n",
              "139  gutenberg  3jw0ylfxrtgjl248kygp3gnqn7oww4       0.0\n",
              "210     mctest  34x6j5flptysvl8n1qy4m1bww3kjqz       0.0\n",
              "217     mctest  382m9cohehfccytc4y7izmvtuh2euo       0.0\n",
              "218     mctest  39u1bhvtdlru2nyqf90cbz5ukfdt3v       0.0\n",
              "225     mctest  3bdcf01ogxu7zdn9vlrbf2rqz14ylh       0.0\n",
              "251     mctest  3m0nz3jdp1yt2eutzkdnck4vj6kz5z       0.0\n",
              "309       race  34pgfrqonobxfi49dzxaeqtil33jw4       0.0\n",
              "310       race  34s9dkfk73pxndqu7y7qsuvf5vcnyf       0.0\n",
              "324       race  3b3wtrp3db2mxqttd3hq1pzqlrc92x       0.0\n",
              "342       race  3ikz72a5b4grnm9z28f239ozzl9fnz       0.0\n",
              "347       race  3iuzpwiu1o7sq2arvkxmf5tvzm4wk4       0.0\n",
              "412  wikipedia  352ythgrovdpfaqzfto67lucoosh4f       0.0\n",
              "416  wikipedia  36dsne9qz5ypa9v7md60xwgwi91oja       0.0\n",
              "429  wikipedia  3a0ex8zrn8ovm41x482h1zvlo3vby5       0.0\n",
              "435  wikipedia  3dip6yhapcsee1mz1v6d3ud4yuve82       0.0\n",
              "478  wikipedia  3u84xhcdicdb6vqtlfud7syhj444z2       0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac8a4a3a-60ce-4463-8978-6e4414a16d90\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>id</th>\n",
              "      <th>f1_squad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3gu1kf0o4i11dq9wdl6yo829jv9pbz</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3jwh6j9i9sd1a5xjx6t6kjxekv8bnk</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3lya37p8iqn02zcg0t1qsrgaqutkb2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3p59jyt76lk5h527b9m7sp02f2n2t6</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3y54sxro1lle1hb9utwdy8vuyvituv</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>34z02eimisdylvztwmit917niam0tf</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3cp1to84pt13w3rhad49p9uoz2c25w</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3efvcay5l39mph8rfwh40aqw343j88</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3h0w84iwbk2kw61v04cdub89nc4ere</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3jw0ylfxrtgjl248kygp3gnqn7oww4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>mctest</td>\n",
              "      <td>34x6j5flptysvl8n1qy4m1bww3kjqz</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>mctest</td>\n",
              "      <td>382m9cohehfccytc4y7izmvtuh2euo</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>mctest</td>\n",
              "      <td>39u1bhvtdlru2nyqf90cbz5ukfdt3v</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3bdcf01ogxu7zdn9vlrbf2rqz14ylh</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3m0nz3jdp1yt2eutzkdnck4vj6kz5z</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>race</td>\n",
              "      <td>34pgfrqonobxfi49dzxaeqtil33jw4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>race</td>\n",
              "      <td>34s9dkfk73pxndqu7y7qsuvf5vcnyf</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>race</td>\n",
              "      <td>3b3wtrp3db2mxqttd3hq1pzqlrc92x</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>race</td>\n",
              "      <td>3ikz72a5b4grnm9z28f239ozzl9fnz</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>race</td>\n",
              "      <td>3iuzpwiu1o7sq2arvkxmf5tvzm4wk4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>352ythgrovdpfaqzfto67lucoosh4f</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>36dsne9qz5ypa9v7md60xwgwi91oja</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3a0ex8zrn8ovm41x482h1zvlo3vby5</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3dip6yhapcsee1mz1v6d3ud4yuve82</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3u84xhcdicdb6vqtlfud7syhj444z2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac8a4a3a-60ce-4463-8978-6e4414a16d90')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac8a4a3a-60ce-4463-8978-6e4414a16d90 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac8a4a3a-60ce-4463-8978-6e4414a16d90');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "id": "dV-25wzI74dx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to poor results of BERTTiny, the $5$ hardest conversation per source all show a $0$ SQuAD F1-score.\n",
        "\n",
        "Let's print all the $25$ conversations extracted, in order to better visualize the model behavior."
      ],
      "metadata": {
        "id": "v3KtEXMfTTsv"
      },
      "id": "v3KtEXMfTTsv"
    },
    {
      "cell_type": "code",
      "source": [
        "sources = list(worst_5_passages['source'].unique())\n",
        "sources"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjsAjrp_xi4V",
        "outputId": "4614c9e5-f792-4a0a-a165-854566ebb478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cnn', 'gutenberg', 'mctest', 'race', 'wikipedia']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "id": "XjsAjrp_xi4V"
    },
    {
      "cell_type": "code",
      "source": [
        "def show_worst_5_passages_per_source(source, results, history, results_h = None):\n",
        "\n",
        "  \"\"\"\n",
        "  Prints the 5 worst `source` passages, by visualizing for each Q&A pair the predicted answer contained in `results`.\n",
        "  If `history = True`, the function also shows the predicted answer contained in `results_h`.\n",
        "  \"\"\"\n",
        "\n",
        "  worst_5_passages_source = worst_5_passages[worst_5_passages['source'] == source]\n",
        "  id_passage = 1\n",
        "\n",
        "  print(f\"#####################  {source.upper()}  #####################\\n\")\n",
        "  for _, r in worst_5_passages_source.iterrows():\n",
        "\n",
        "    source = r['source']\n",
        "    id = r['id']\n",
        "\n",
        "    if id_passage > 1:\n",
        "      print(\"------\"*50)\n",
        "\n",
        "    print(f\"\\nPASSAGE {id_passage}:\\n\")\n",
        "\n",
        "    id_passage+=1\n",
        "    print(list(results[results['id'] == id]['passage'])[0])\n",
        "\n",
        "    print(\"\\n\\nCONVERSATION:\\n\")\n",
        "\n",
        "    for idx, row in results.iterrows():\n",
        "      if(id == row['id']):\n",
        "        question = row['question']\n",
        "        answer = row['answer']\n",
        "        pred_answer = row['pred_answer']\n",
        "        rationale = row['rationale']\n",
        "\n",
        "        print(f\"Q: {question}\")\n",
        "        print(f\"A: {answer}\")\n",
        "        print(f\"R: {rationale}\")\n",
        "        print(f\"PA: {pred_answer}\")\n",
        "        if history:\n",
        "          pred_answers_h = results_h[results_h['id'] == id]\n",
        "          for idx_h, row_h in pred_answers_h.iterrows():\n",
        "            if row_h['question'] == question and row_h['answer'] == answer:\n",
        "              print(f\"PA_H: {row_h['pred_answer']}\")\n",
        "\n",
        "\n",
        "        print()"
      ],
      "metadata": {
        "id": "2r92o9S5YKSi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2r92o9S5YKSi"
    },
    {
      "cell_type": "code",
      "source": [
        "show_worst_5_passages_per_source('cnn', bert2bert_results, history = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6482a6-48b2-4031-c922-05a5ab98bf38",
        "id": "2Y646U9274dx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################  CNN  #####################\n",
            "\n",
            "\n",
            "PASSAGE 1:\n",
            "\n",
            "Dennis Rodman says he's not a diplomat, just a former NBA star fighting addiction and trying to be a better father. \n",
            "\n",
            "But that hasn't kept him from becoming embroiled, even if unintentionally, in U.S.-North Korea relations. \n",
            "\n",
            "His apparent chumminess with North Korea's leader has been controversial, as have previous proclamations that Kim Jong Un is a \"nice guy.\" \n",
            "\n",
            "Rodman said he simply does not know enough about the allegations of abuse and authoritarianism that Kim is reviled for. \n",
            "\n",
            "\"I keep telling people, I'm not there to be an ambassador to try to figure out why (is Kim) doing all these things? That's not my job,\" Rodman told CNN's Chris Cuomo on \"New Day\" Friday. \n",
            "\n",
            "It was his first interview since entering an alcohol rehabilitation facility two weeks ago. \n",
            "\n",
            "The former basketball player is probably the American with the most access to the North Korean leader, and many have been critical that he has not lobbied for the release of American prisoner Kenneth Bae from a labor camp. \n",
            "\n",
            "\"I'm not an ambassador, and I tried to strive and tell people, just because I know the marshal (Kim), that doesn't mean I know the marshal like that,\" he said. \n",
            "\n",
            "Rodman says he didn't mean to insinuate during an earlier interview this month that he knew why Bae, a Korean-American, was being held in North Korea. \n",
            "\n",
            "\"To this day I still don't know what he did,\" Rodman said, even offering to take Bae's place if he could. \n",
            "\n",
            "\"I feel for (Bae's family). I feel for them deeply. ... I would do anything literally. This is Dennis Rodman talking. If they (North Korea) said, 'We'll take Dennis Rodman and we'll let Kenenth Bae go,' I'll say, 'You know what? I'll do that. ... Take me.'\" \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What sport was Rodman in?\n",
            "A: basketball\n",
            "R:  a former NBA star\n",
            "PA: soccer\n",
            "\n",
            "Q: What league did he play in?\n",
            "A: NBA\n",
            "R: ust a former NBA star fighting addiction \n",
            "PA: the united states\n",
            "\n",
            "Q: Who is he friends with?\n",
            "A: Kin Jong Un\n",
            "R: His apparent chumminess with North Korea's leader has been controversial, as have previous proclamations that Kim Jong Un is a \"nice guy.\" \n",
            "PA: cnn and cnn.\n",
            "\n",
            "Q: Where is he from?\n",
            "A: North Korea\n",
            "R: North Korea's leader \n",
            "PA: the united states\n",
            "\n",
            "Q: What is Jong Un's position there?\n",
            "A: leader\n",
            "R: North Korean leader\n",
            "PA: the u. s..\n",
            "\n",
            "Q: Who talked to Rodman?\n",
            "A: Chris Cuomo\n",
            "R: Rodman told CNN's Chris Cuomo on \"New Day\" Friday\n",
            "PA: cnn\n",
            "\n",
            "Q: Who does he work for?\n",
            "A: CNN\n",
            "R: Rodman told CNN's Chris Cuomo \n",
            "PA: a former former president\n",
            "\n",
            "Q: When was the interview?\n",
            "A: Friday\n",
            "R: Rodman told CNN's Chris Cuomo on \"New Day\" Friday\n",
            "PA: in 2008\n",
            "\n",
            "Q: On what program was it?\n",
            "A: New Day\n",
            "R: Rodman told CNN's Chris Cuomo on \"New Day\" Friday.\n",
            "PA: cnn and cnn\n",
            "\n",
            "Q: Who was a prisoner?\n",
            "A: Kenneth Bae\n",
            "R:  American prisoner Kenneth Bae\n",
            "PA: a former former president\n",
            "\n",
            "Q: Where was he from?\n",
            "A: Korean-American\n",
            "R: Bae, a Korean-American\n",
            "PA: the united states\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 2:\n",
            "\n",
            "Kabul, Afghanistan (CNN) -- The German news outlet Der Spiegel has published photographs of what appear to be two U.S. soldiers in Afghanistan posing over the bodies of dead Afghans -- images which threaten to further complicate the American military effort there. \n",
            "\n",
            "Two images show the soldiers kneeling by a bloody body sprawled over a patch of sand and grass. A third shows what appears to be two bodies propped up, back to back, against a post in front of a military vehicle. \n",
            "\n",
            "Der Spiegel identifies the soldiers as Spc. Jeremy Morlock and Pfc. Andrew Holmes, who are both facing charges relating to the wrongful deaths of Afghan civilians. \n",
            "\n",
            "Specifically, Holmes is charged with the premeditated deaths of three civilians, possessing a dismembered human finger, wrongfully possessing photographs of human casualties, and smoking hashish. \n",
            "\n",
            "He is also accused of conspiring with Morlock to shoot at a civilian and then toss a grenade so it would look like the soldiers were under attack. \n",
            "\n",
            "Morlock is charged with three counts of murder. He is accused of killing one Afghan civilian in January 2010 with a grenade and rifle; killing another in May 2010 in a similar manner; and shooting a third to death in February 2010. \n",
            "\n",
            "U.S. military rules also prohibit \"taking or retaining individual souvenirs or trophies,\" which the photographs may be construed as. \n",
            "\n",
            "The trial for the two soldiers is being conducted at Joint Base Lewis-McChord in Washington. Morlock's court martial is slated to begin Wednesday, while the start date for Holmes' court martial has not been publicly announced. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What news agency showed photos of American soldiers?\n",
            "A: Der Spiegel\n",
            "R: The German news outlet Der Spiegel\n",
            "PA: cnn\n",
            "\n",
            "Q: From what country?\n",
            "A: Germany\n",
            "R: The German news outlet Der Spiegel\n",
            "PA: south africa\n",
            "\n",
            "Q: What were the soldiers doing in the photos?\n",
            "A: posing over the bodies of dead Afghans\n",
            "R: photographs of what appear to be two U.S. soldiers in Afghanistan posing over the bodies of dead Afghans\n",
            "PA: he was shot in a shot\n",
            "\n",
            "Q: What was the condition of the body?\n",
            "A: bloody\n",
            "R: Two images show the soldiers kneeling by a bloody body sprawled over a patch of sand and grass\n",
            "PA: it's an earthquake of the\n",
            "\n",
            "Q: What does another photo show?\n",
            "A: propped up, back to back\n",
            "R: A third shows what appears to be two bodies propped up, back to bac\n",
            "PA: he was shot on a shot\n",
            "\n",
            "Q: Near what?\n",
            "A: military vehicle.\n",
            "R:  A third shows what appears to be two bodies propped up, back to back, against a post in front of a military vehicle. \n",
            "\n",
            "PA: the u. s.\n",
            "\n",
            "Q: What could the photos be construed as?\n",
            "A: taking or retaining individual souvenirs or trophies\n",
            "R: U.S. military rules also prohibit \"taking or retaining individual souvenirs or trophies,\" which the photographs may be construed as. \n",
            "PA: he was killed in the battle of\n",
            "\n",
            "Q: What is the name of one of the soldiers?\n",
            "A: Jeremy Morlock\n",
            "R: Der Spiegel identifies the soldiers as Spc. Jeremy Morlock \n",
            "PA: the british army\n",
            "\n",
            "Q: The other?\n",
            "A: Pfc. Andrew Holmes\n",
            "R: Pfc. Andrew Holmes\n",
            "PA: a terrorist attack\n",
            "\n",
            "Q: What is Holmes being charged with?\n",
            "A: Holmes is charged with the premeditated deaths of three civilians\n",
            "R: Holmes is charged with the premeditated deaths of three civilians\n",
            "PA: he was shot in a shot\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 3:\n",
            "\n",
            "Los Angeles (CNN) -- Dr. Conrad Murray, who was Michael Jackson's personal physician, is refusing to testify in the wrongful death lawsuit that the singer's mother filed against concert promoter AEG Live. \n",
            "\n",
            "If called, he will plead the fifth so as not to incriminate himself, the doctor said in a statement sent to the Jackson family. \n",
            "\n",
            "Murray has never been questioned under oath about Jackson's death, which occurred on his watch. \n",
            "\n",
            "He did not testify at his trial, where he was found guilty of causing Jackson's death on June 25, 2009, by administering a deadly overdose of sedatives and the surgical anesthetic propofol in what he told police was an attempt to cure the singer's insomnia. \n",
            "\n",
            "He is serving a four-year sentence but could be out in two. \n",
            "\n",
            "On Monday, he was supposed to be deposed in the wrongful death suit. \n",
            "\n",
            "But lawyers for Jackson's mother, Katherine, and her three children called off their jailhouse visit because Murray swore \"he would not answer any questions at the deposition or the trial,\" Jackson lawyer Kevin Boyle told CNN. \n",
            "\n",
            "He said the Jackson case would not suffer without his testimony. \n",
            "\n",
            "AEG's lawyer suggests the Jacksons canceled Murray's deposition because his testimony would \"destroy\" their case. \n",
            "\n",
            "\"They are not interested in the truth,\" said the lawyer, Marvin Putnam. \n",
            "\n",
            "Asserting his 'Fifth Amendment privilege' \n",
            "\n",
            "The cancellation came after Murray's attorney Valerie Wass sent the Jacksons a sworn statement signed by Murray making it clear he would not answer any questions while his involuntary manslaughter conviction in Michael Jackson's death is being appealed. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Who was the singer's doctor?\n",
            "A: Conrad Murray\n",
            "R: Dr. Conrad Murray, who was Michael Jackson's personal physician,\n",
            "PA: his wife\n",
            "\n",
            "Q: Is he involved in the lawsuit?\n",
            "A: he refused to testify in it\n",
            "R: who was Michael Jackson's personal physician, is refusing to testify in the wrongful death lawsuit \n",
            "PA: yes\n",
            "\n",
            "Q: Who filed the lawsuit?\n",
            "A: the singer's mother\n",
            "R: lawsuit that the singer's mother filed\n",
            "PA: his wife\n",
            "\n",
            "Q: Against who?\n",
            "A: AEG Live.\n",
            "R: lawsuit that the singer's mother filed against concert promoter AEG Live. \n",
            "\n",
            "PA: his wife.\n",
            "\n",
            "Q: What was the lawsuit about?\n",
            "A: wrongful death\n",
            "R: the wrongful death lawsuit\n",
            "PA: he was arrested\n",
            "\n",
            "Q: Did he testify when he was the subject of a suit?\n",
            "A: No\n",
            "R: He did not testify at his trial\n",
            "PA: yes\n",
            "\n",
            "Q: When did Jackson die?\n",
            "A: June 25\n",
            "R: Jackson's death on June 25\n",
            "PA: in october\n",
            "\n",
            "Q: What year?\n",
            "A: 2009\n",
            "R: Jackson's death on June 25, 2009\n",
            "PA: 2011\n",
            "\n",
            "Q: He died of what?\n",
            "A: an overdose\n",
            "R: Jackson's death on June 25, 2009, by administering a deadly overdose\n",
            "PA: his wife\n",
            "\n",
            "Q: Of what?\n",
            "A: sedatives\n",
            "R: deadly overdose of sedatives\n",
            "PA: he was arrested\n",
            "\n",
            "Q: And what else?\n",
            "A: Yes\n",
            "R: overdose of sedatives and the surgical anesthetic propofol \n",
            "PA: he was murdered\n",
            "\n",
            "Q: What is the singer's mother's name?\n",
            "A: Katherine\n",
            "R: Jackson's mother, Katherine\n",
            "PA: dr. brown\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 4:\n",
            "\n",
            "(CNN) -- According to the \"Guinness Book of World Records,\" Sherlock Holmes is the most popular role in the movies. \n",
            "\n",
            "So if Robert Downey Jr. hardly seems like the iconic Sherlock (he's too short and muscular, and strains to suggest the intellectual arrogance that comes so easily to the English), he can take his place alongside such oddball castings as Buster Keaton, Charlton Heston and George C. Scott. And he's ideally suited to director Guy Ritchie's purpose, which is not to dust off your grandfather's Conan Doyle, but to juice today's kids with a quirky 19th century super sleuth. \n",
            "\n",
            "What's fun about these movies is that the equation works both ways. \n",
            "\n",
            "Yes, this Holmes is more likely to get physical than his predecessors, he's even a bit of a boor, but he's still a more cerebral action hero than we find in most blockbusters, and the period trappings (a seamless blend of CGI and location work) help this franchise stand out from the crowd of comic book adaptations. \n",
            "\n",
            "Ritchie and husband-and-wife screenwriters Kieran and Michele Mulroney (\"Paper Man\") map out a fast-moving -- if ridiculously digressive -- Victorian adventure that crisscrosses Europe by horseless carriage, train, boat, and, in Sherlock's case, pony. \n",
            "\n",
            "Just down the road from 221 Baker Street they're digging a tunnel for what will become the London Underground. The Industrial Age is beginning to stretch its muscles, while a series of terrorist atrocities are gnawing at the fragile understanding between the great European powers. Holmes detects something other than social unrest behind the bombings: an evil master plan leading inexorably to his nemesis, Professor Moriarty (Jared Harris). \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What is the most popular character in films?\n",
            "A: Sherlock Holmes\n",
            "R: Sherlock Holmes is the most popular role in the movies. \n",
            "PA: the adventures of comics\n",
            "\n",
            "Q: According to whom?\n",
            "A: Guinness book of world records\n",
            "R: According to the \"Guinness Book of World Records,\" \n",
            "PA: the british empire\n",
            "\n",
            "Q: Who is playing Holmes in Guy Ritchie's film?\n",
            "A: Robert Downey Jr.\n",
            "R: Robert Downey Jr. hardly seems like the iconic Sherlock\n",
            "PA: his father.\n",
            "\n",
            "Q: Is he typical for the part?\n",
            "A: no\n",
            "R: Robert Downey Jr. hardly seems like the iconic Sherlock\n",
            "PA: yes\n",
            "\n",
            "Q: Why not?\n",
            "A: he's too short and muscular\n",
            "R: he's too short and muscular, and strains to suggest the intellectual arrogance that comes so easily to the English\n",
            "PA: he was an actor in an actor\n",
            "\n",
            "Q: Who are some other unusual actors cast as the character?\n",
            "A: Buster Keaton\n",
            "R:  oddball castings as Buster Keaton, Charlton Heston and George C. Scott.\n",
            "PA: his wife.\n",
            "\n",
            "Q: Any others?\n",
            "A: Charlton Heston\n",
            "R: Charlton Heston\n",
            "PA: yes\n",
            "\n",
            "Q: Who else?\n",
            "A: George C. Scott\n",
            "R: George C. Scott\n",
            "PA: his wife.\n",
            "\n",
            "Q: Is this version of the character less physical than usual?\n",
            "A: no\n",
            "R: Yes, this Holmes is more likely to get physical than his predecessors\n",
            "PA: yes\n",
            "\n",
            "Q: Is he well-mannered?\n",
            "A: no\n",
            "R: he's even a bit of a boor,\n",
            "PA: yes\n",
            "\n",
            "Q: Is he dumber than most film heroes?\n",
            "A: no\n",
            "R: but he's still a more cerebral action hero\n",
            "PA: yes\n",
            "\n",
            "Q: Who wrote the film's screenplay?\n",
            "A: Kieran and Michele Mulroney\n",
            "R: screenwriters Kieran and Michele Mulroney\n",
            "PA: his wife.\n",
            "\n",
            "Q: What other film are they known for?\n",
            "A: Paper Man\n",
            "R: screenwriters Kieran and Michele Mulroney (\"Paper Man\")\n",
            "PA: the adventures of comics\n",
            "\n",
            "Q: What continent is the film set on?\n",
            "A: Europe\n",
            "R:  Victorian adventure that crisscrosses Europe \n",
            "PA: new york\n",
            "\n",
            "Q: Is it during the Edwardian era?\n",
            "A: no\n",
            "R: Victorian adventure that crisscrosses Europe\n",
            "PA: yes\n",
            "\n",
            "Q: How do the film characters travel?\n",
            "A: horseless carriage\n",
            "R: crisscrosses Europe by horseless carriage\n",
            "PA: it's a book.\n",
            "\n",
            "Q: How else?\n",
            "A: train\n",
            "R: train\n",
            "PA: he was an actor.\n",
            "\n",
            "Q: Any other means?\n",
            "A: boat\n",
            "R: boat\n",
            "PA: yes\n",
            "\n",
            "Q: How does the main character travel?\n",
            "A: pony\n",
            "R: Sherlock's case, pony. \n",
            "PA: it's a lot of a\n",
            "\n",
            "Q: What is the name of the main character's enemy?\n",
            "A: Professor Moriarty\n",
            "R: his nemesis, Professor Moriarty \n",
            "PA: the adventures of comics\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 5:\n",
            "\n",
            "(CNN) -- Smart, successful, funny and handsome. Robin Williams seemed to have it all. And yet, today he is dead. Apparently, by his own choice. \n",
            "\n",
            "But why? What went wrong? \n",
            "\n",
            "The joy, spontaneity and humor of Robin Williams likely masked the daily torment he endured in his on-again-off-again struggle with depression. He made us laugh out here, but he was in pain in there. Unfortunately, I know that pain. \n",
            "\n",
            "In his death, we have lost one of the most talented and creative spirits on the planet. Still, his death by suicide should be a wake-up call for us all. It is to remind us that many of us are walking a fine line -- smiling on the outside while slowly dying on the inside. \n",
            "\n",
            "I didn't know Robin personally, so I am unable to speak with any certainty as to why he chose to end his life. But I have experienced that kind of torment and pain. For years, I, too, struggled with depression. And many days, I still do. \n",
            "\n",
            "Complete coverage on Robin Williams \n",
            "\n",
            "Those who are suffering will do just about anything not to feel the pain anymore. And in those moments, their brains become their worst enemies. It often takes an outside force to provide light, to make sure those dark thoughts aren't, as in the case of Robin, our last thoughts. \n",
            "\n",
            "According to the Centers for Disease Center, suicide rates increased from 2000 to 2011 from 10.4 deaths per 100,000 to 12.3 deaths per 100,000. \n",
            "\n",
            "The rate of suicide is higher and rising among men. In 2011, 78.5% of suicides were by men, at a rate of 20.2 deaths per 100,000. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Is Robin Williams still living?\n",
            "A: no\n",
            "R: And yet, today he is dead\n",
            "PA: yes\n",
            "\n",
            "Q: What happened to him?\n",
            "A: suicide\n",
            "R: Apparently, by his own choice. \n",
            "PA: he was killed in the battle of\n",
            "\n",
            "Q: What made him do that?\n",
            "A: depression\n",
            "R:  on-again-off-again struggle with depression. \n",
            "PA: he wanted to be able to be\n",
            "\n",
            "Q: Did that effect him all the time?\n",
            "A: no\n",
            "R: his on-again-off-again struggle with depression. \n",
            "PA: yes\n",
            "\n",
            "Q: What hid his struggle from other?\n",
            "A: his humor\n",
            "R: The joy, spontaneity and humor of Robin Williams\n",
            "PA: he was afraid that he's\n",
            "\n",
            "Q: What did it look like he had?\n",
            "A: Heseemed to have it all.\n",
            "R: Robin Williams seemed to have it all. \n",
            "PA: it's a heart.\n",
            "\n",
            "Q: Does suicide occur most often in males or females?\n",
            "A: males\n",
            "R:  78.5% of suicides were by men, at a rate of 20.2 deaths per 100,000.\n",
            "PA: yes\n",
            "\n",
            "Q: What percentage were they responsible for in 2011?\n",
            "A: 78.5\n",
            "R: In 2011, 78.5% of suicides were by men, at a rate of 20.2 deaths per 100,000.\n",
            "PA: 1. 5 %\n",
            "\n",
            "Q: How many is that for every 100,000?\n",
            "A: 20.2\n",
            "R: ate of 20.2 deaths per 100,000. \n",
            "PA: three\n",
            "\n",
            "Q: What happened with suicide between 2000 and 2011?\n",
            "A: the rate increased\n",
            "R: uicide rates increased from 2000 to 2011 from 10.4 deaths per 100,000 to 12.3 deaths per 100,000. \n",
            "PA: he died in a heart attack\n",
            "\n",
            "Q: What was it in 2000?\n",
            "A: 10.4 deaths per 100,000\n",
            "R: 0.4 deaths per 100,000 \n",
            "PA: it's a heart attack\n",
            "\n",
            "Q: And in 2011?\n",
            "A: 12.3 deaths per 100,000.\n",
            "R: 12.3 deaths per 100,000. \n",
            "PA: cnn\n",
            "\n",
            "Q: Was Robin Williams successfu?\n",
            "A: yes\n",
            "R: Smart, successful, funny and handso\n",
            "PA: no\n",
            "\n",
            "Q: What does the author say about his spirit?\n",
            "A: it was talented and creative\n",
            "R: we have lost one of the most talented and creative spirits o\n",
            "PA: he's not not to be\n",
            "\n",
            "Q: What will someone suffering do to get rid of the pain?\n",
            "A: just about anythinh\n",
            "R: Those who are suffering will do just about anything not to feel the pain a\n",
            "PA: he's a heart attack\n",
            "\n",
            "Q: What can their brains be?\n",
            "A: their worst enemies\n",
            "R: heir brains become their worst enemies. I\n",
            "PA: it's too much to be\n",
            "\n",
            "Q: How can their thoughts turn?\n",
            "A: dark\n",
            "R: make sure those dark thoughts aren't, \n",
            "PA: it is very very good.\n",
            "\n",
            "Q: What do they need to help?\n",
            "A: an outside force\n",
            "R: t often takes an outside force to provide light,\n",
            "PA: he's going to go to\n",
            "\n"
          ]
        }
      ],
      "id": "2Y646U9274dx"
    },
    {
      "cell_type": "code",
      "source": [
        "show_worst_5_passages_per_source('gutenberg', bert2bert_results, history = False)"
      ],
      "metadata": {
        "id": "K6ZYr7Qt74dy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdcc209-7fe7-4656-84dc-9df512faade7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################  GUTENBERG  #####################\n",
            "\n",
            "\n",
            "PASSAGE 1:\n",
            "\n",
            "CHAPTER XXVI \n",
            "\n",
            "THE JUDGMENT OF DOMITIAN \n",
            "\n",
            "Two hours had gone by and Caleb, with fury in his heart, sat brooding in the office attached to the warehouse that he had hired. At that moment he had but one desire--to kill his successful rival, Marcus. Marcus had escaped and returned to Rome; of that there could be no doubt. He, one of the wealthiest of its patricians, had furnished the vast sum which enabled old Nehushta to buy the coveted Pearl-Maiden in the slave-ring. Then his newly acquired property had been taken to this house, where he awaited her. This then was the end of their long rivalry; for this he, Caleb, had fought, toiled, schemed and suffered. Oh! rather than such a thing should be, in that dark hour of his soul, he would have seen her cast to the foul Domitian, for Domitian, at least, she would have hated, whereas Marcus, he knew, she loved. \n",
            "\n",
            "Now there remained nothing but revenge. Revenged he must be, but how? He might dog Marcus and murder him, only then his own life would be hazarded, since he knew well the fate that awaited the foreigner, and most of all the Jew, who dared to lift his hand against a Roman noble, and if he hired others to do the work they might bear evidence against him. Now Caleb did not wish to die; life seemed the only good that he had left. Also, while he lived he might still win Miriam--after his rival had ceased to live. Doubtless, then she would be sold with his other slaves, and he could buy her at the rate such tarnished goods command. No, he would do nothing to run himself into danger. He would wait, wait and watch his opportunity. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What was now left?\n",
            "A: revenge\n",
            "R: Now there remained nothing but revenge\n",
            "PA: he was murdered in his father\n",
            "\n",
            "Q: Against who?\n",
            "A: Marcus\n",
            "R: He might dog Marcus\n",
            "PA: his father\n",
            "\n",
            "Q: What was in the angered man's heart?\n",
            "A: fury\n",
            "R: with fury in his heart\n",
            "PA: he was murdered by his father\n",
            "\n",
            "Q: What was one option to get back at the man?\n",
            "A: dog and murder him\n",
            "R: He might dog Marcus and murder him\n",
            "PA: he was a lawyer\n",
            "\n",
            "Q: What was a drawback to that plan?\n",
            "A: his own life would be hazarded\n",
            "R: only then his own life would be hazarded\n",
            "PA: he was going to kill him\n",
            "\n",
            "Q: What was the subject of their rivalry?\n",
            "A: the coveted Pearl-Maiden\n",
            "R: the coveted Pearl-Maiden\n",
            "PA: the king of the king\n",
            "\n",
            "Q: What was the maiden's occupation?\n",
            "A: a slave\n",
            "R: the coveted Pearl-Maiden in the slave-ring\n",
            "PA: he was murdered\n",
            "\n",
            "Q: And her name?\n",
            "A: Miriam\n",
            "R: he might still win Miriam\n",
            "PA: his father\n",
            "\n",
            "Q: What might he be able to do if his rival were dead?\n",
            "A: win Miriam\n",
            "R: he might still win Miriam--after his rival had ceased to live\n",
            "PA: he had been murdered.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 2:\n",
            "\n",
            "CHAPTER XXV. THE VELVET COACH \n",
            "\n",
            "\n",
            "\n",
            "No, my good Lord, Diana-- ALL'S WELL THAT ENDS WELL \n",
            "\n",
            "A late autumn journey from the west coast to Paris was a more serious undertaking in the sixteenth century than the good seaman Master Hobbs was aware of, or he would have used stronger dissuasive measures against such an undertaking by the two youths, when the elder was in so frail a state of health; but there had been a certain deceptive strength and vigour about young Ribaumont while under strong excitement and determination, and the whole party fancied him far fitter to meet the hardships than was really the case. Philip Thistlewood always recollected that journey as the most distressing period of his life. \n",
            "\n",
            "They were out of the ordinary highways, and therefore found the hiring of horses often extremely difficult. They had intended to purchase, but found no animals that, as Philip said, they would have accepted as a gift, though at every wretched inn where they had to wait while the country was scoured for the miserable jades, their proposed requirements fell lower and lower. Dens of smoke, dirt, and boorishness were the great proportion of those inns, where they were compelled to take refuge by the breaking down of one or other of the beasts, or by stress of weather. Snow, rain, thaw and frost alternated, each variety rendering the roads impassable; and at the best, the beasts could seldom be urged beyond a walk, fetlock-deep in mire or water. Worse than all, Berenger, far from recovered, and under the heavy oppression of a heartrending grief, could hardly fail to lose the ground that he had gained under the influence of hope. The cold seemed to fix itself on the wound on his cheek, terrible pain and swelling set in, depriving him entirely of sleep, permitting him to take no nourishment but fragments of soft crumbs soaked in wine or broth--when the inns afforded any such fare--and rendering speech excessively painful, and at last unintelligible. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What time of year was it?\n",
            "A: autumn\n",
            "R: late autumn journey\n",
            "PA: two years ago\n",
            "\n",
            "Q: who was described as having a deceptive strenght and vigor?\n",
            "A: Ribaumont\n",
            "R:  strength and vigour about young Ribaumont \n",
            "PA: his father\n",
            "\n",
            "Q: What was it that was diffuclt for them to hire?\n",
            "A: horses\n",
            "R:  found the hiring of horses often extremely difficult\n",
            "PA: it's a great place of\n",
            "\n",
            "Q: Where was it they were traveling from?\n",
            "A: the west coast\n",
            "R: journey from the west coast \n",
            "PA: in the middle - room\n",
            "\n",
            "Q: to where?\n",
            "A: to Paris\n",
            "R:  from the west coast to Paris\n",
            "PA: in the house of the house\n",
            "\n",
            "Q: Was was rendering the roads they were traveling impassable?\n",
            "A: Snow, rain, thaw and frost\n",
            "R:  Snow, rain, thaw and frost alternated\n",
            "PA: no\n",
            "\n",
            "Q: Why were they have troubles hiring horses?\n",
            "A: They were out of the ordinary highways\n",
            "R: They were out of the ordinary highways, and therefore found the hiring of horses often extremely difficult\n",
            "PA: because he had been gone to be\n",
            "\n",
            "Q: How did they decribe a great proportion of the inns they were staying at?\n",
            "A: Dens of smoke, dirt, and boorishness were the great proportion\n",
            "R: Dens of smoke, dirt, and boorishness were the great proportion \n",
            "PA: he had been a long - time\n",
            "\n",
            "Q: And why is it that they were compelled to stay in those inn?\n",
            "A: breaking down  of the beasts\n",
            "R: breaking down of one or other of the beasts, or by stress of weather.\n",
            "PA: because he had been gone to be\n",
            "\n",
            "Q: Who was it that was far from recovered, had a wound on his cheek, and was deprived of sleep?\n",
            "A: Berenger\n",
            "R: Worse than all, Berenger, far from recovered, \n",
            "PA: his father\n",
            "\n",
            "Q: what was the only thing he was able to eat?\n",
            "A: soft crumbs\n",
            "R: nourishment but fragments of soft crumbs\n",
            "PA: it's a lot of the\n",
            "\n",
            "Q: how was his speech?\n",
            "A: unintelligible.\n",
            "R: endering speech excessively painful, and at last unintelligible. \n",
            "PA: that he was a great great -\n",
            "\n",
            "Q: Who recollected the Jurney from the west coast to Paris was the most distressing period of their life?\n",
            "A: Philip Thistlewood\n",
            "R: Philip Thistlewood always recollected that journey as the most distressing\n",
            "PA: his wife.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 3:\n",
            "\n",
            "CHAPTER XVI. THE SUPREME SACRIFICE \n",
            "\n",
            "THROUGH the balance of the day and all during the long night Billy Byrne swung along his lonely way, retracing the familiar steps of the journey that had brought Barbara Harding and himself to the little island in the turbulent river. \n",
            "\n",
            "Just before dawn he came to the edge of the clearing behind the dwelling of the late Oda Yorimoto. Somewhere within the silent village he was sure that the two prisoners lay. \n",
            "\n",
            "During the long march he had thrashed over again and again all that the success of his rash venture would mean to him. Of all those who might conceivably stand between him and the woman he loved--the woman who had just acknowledged that she loved him--these two men were the most to be feared. \n",
            "\n",
            "Billy Byrne did not for a moment believe that Anthony Harding would look with favor upon the Grand Avenue mucker as a prospective son-in-law. And then there was Mallory! He was sure that Barbara had loved this man, and now should he be restored to her as from the grave there seemed little doubt but that the old love would be aroused in the girl's breast. The truth of the matter was that Billy Byrne could not conceive the truth of the testimony of his own ears--even now he scarce dared believe that the wonderful Miss Harding loved him--him, the despised mucker! \n",
            "\n",
            "But the depth of the man's love for the girl, and the genuineness of his new-found character were proven beyond question by the relentless severity with which he put away every thought of himself and the consequences to him in the matter he had undertaken. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: how long did the main character travel?\n",
            "A: THROUGH the balance of the day and all during the long night\n",
            "R: THROUGH the balance of the day and all during the long night Billy Byrne swung along his lonely way,\n",
            "PA: two years ago\n",
            "\n",
            "Q: what is the main character's first name?\n",
            "A: Billy\n",
            "R: HROUGH the balance of the day and all during the long night Billy Byrne s\n",
            "PA: mr. smith\n",
            "\n",
            "Q: what is his last name?\n",
            "A: Byrne\n",
            "R: \n",
            "THROUGH the balance of the day and all during the long night Billy Byrne \n",
            "PA: his father\n",
            "\n",
            "Q: what was his profession?\n",
            "A: Grand Avenue mucker\n",
            "R: he Grand Avenue mucker\n",
            "PA: he was a lawyer\n",
            "\n",
            "Q: would his prospective father and law care for that profession?\n",
            "A: no\n",
            "R: Billy Byrne did not for a moment believe that Anthony Harding would look with favor upon the Grand Avenue mucker as a prospective son-in-law. \n",
            "PA: yes\n",
            "\n",
            "Q: who was his prospective father in law?\n",
            "A: Anthony Harding\n",
            "R: Billy Byrne did not for a moment believe that Anthony Harding would look with favor upon the Grand Avenue mucker as a prospective son-in-law.\n",
            "PA: his father\n",
            "\n",
            "Q: and what is his daughter's name?\n",
            "A: Miss Harding\n",
            "R: Miss Harding\n",
            "PA: his father\n",
            "\n",
            "Q: what is her first name?\n",
            "A: Barbara\n",
            "R: He was sure that Barbara had loved this man, a\n",
            "PA: mr. smith\n",
            "\n",
            "Q: who was the other guy he feared?\n",
            "A: Mallory\n",
            "R: And then there was Mallory!\n",
            "PA: his father\n",
            "\n",
            "Q: why is that?\n",
            "A: Barbara had loved this man\n",
            "R: He was sure that Barbara had loved this man\n",
            "PA: because he was afraid of her.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 4:\n",
            "\n",
            "CHAPTER VIII. STEAD IN POSSESSION. \n",
            "\n",
            "\"At night returning, every labour sped, He sits him down, the monarch of a shed.\" GOLDSMITH. \n",
            "\n",
            "Another day made it certain that the garrison of Bristol had surrendered to the besiegers. A few shots were heard, but they were only fired in rejoicing by the Royalists, and while Steadfast was studying his barley field, already silvered over by its long beards, and wondering how soon it would be ripe, and how he should get it cut and stacked, his name was shouted out, and he saw Tom Oates and all the rest of the boys scampering down the lane. \n",
            "\n",
            "\"Come along, Stead Kenton, come on and see, the Parliament soldiers come out and go by.\" \n",
            "\n",
            "Poor Steadfast had not much heart for watching soldiers, but it struck him that he might see or hear something of Jephthah, so he came with the other boys to the bank, where from behind a hedge they could look down at the ranks of soldiers as they marched along, five abreast, the road was not wide enough to hold more. They had been allowed to keep their weapons, so the officers had their swords, and the men carried their musquets. Most of them looked dull and dispirited, and the officers had very gloomy, displeased faces. In fact, they were very angry with their commander, Colonel Fiennes, for having surrendered so easily, and he was afterwards brought to a court-martial for having done so. \n",
            "\n",
            "Stead did not understand this, he thought only of looking under each steel cap or tall, slouching hat for Jephthah. Several times a youthful, slender figure raised his hopes, and disappointed him, and he began to wonder whether Jeph could have after all stayed behind in the town, or if he could have been hurt and was ill there. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Who surrendered?\n",
            "A: the garrison of Bristol\n",
            "R: garrison of Bristol had surrendered\n",
            "PA: his father\n",
            "\n",
            "Q: Why were there gunshots?\n",
            "A: they were rejoicing\n",
            "R: they were only fired in rejoicing by the Royalists\n",
            "PA: because he was going to see the\n",
            "\n",
            "Q: Who was?\n",
            "A: the Royalists\n",
            "R: rejoicing by the Royalists\n",
            "PA: his father\n",
            "\n",
            "Q: What was Steadfast doing?\n",
            "A: studying his barley field\n",
            "R: Steadfast was studying his barley field\n",
            "PA: he was going to go to the\n",
            "\n",
            "Q: What did he not want to watch?\n",
            "A: soldiers\n",
            "R: Poor Steadfast had not much heart for watching soldiers\n",
            "PA: he was going to go to the\n",
            "\n",
            "Q: Who was he hoping to learn something about?\n",
            "A: Jephthah\n",
            "R: hear something of Jephthah\n",
            "PA: his father\n",
            "\n",
            "Q: Where did they watch the soldiers?\n",
            "A: behind a hedge\n",
            "R: where from behind a hedge \n",
            "PA: in the woods.\n",
            "\n",
            "Q: What weapons did the officers have?\n",
            "A: swords\n",
            "R: so the officers had their swords\n",
            "PA: a gun - gun\n",
            "\n",
            "Q: And the men?\n",
            "A: musquets\n",
            "R: the men carried their musquets\n",
            "PA: no\n",
            "\n",
            "Q: Did they look happy?\n",
            "A: no\n",
            "R: Most of them looked dull and dispirited\n",
            "PA: yes\n",
            "\n",
            "Q: Who were they mad at?\n",
            "A: their commander\n",
            "R:  they were very angry with their commander\n",
            "PA: his father\n",
            "\n",
            "Q: Wht?\n",
            "A: for surrendering\n",
            "R:  Colonel Fiennes, for having surrendered so easily\n",
            "PA: no\n",
            "\n",
            "Q: What was the consequence for him?\n",
            "A: a court-martial\n",
            "R:  he was afterwards brought to a court-martial for having done so. \n",
            "PA: he's a little man.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 5:\n",
            "\n",
            "CHAPTER XXIII \n",
            "\n",
            "A SLIDE DOWN THE MOUNTAIN SIDE \n",
            "\n",
            "\"My gracious, Dick! It sure is snowing some now!\" \n",
            "\n",
            "\"Yes, and it is getting colder every minute.\" \n",
            "\n",
            "\"If we don't get out of the mountains putty quick we'll be snowed in,\" came from Jack Wumble. \n",
            "\n",
            "\"Did you calkerlate to git back to Dawson afore winter sot in?\" inquired Ike Furner, curiously. \n",
            "\n",
            "\"Why of course!\" cried Sam. \n",
            "\n",
            "\"I don't see how you are goin' to make it.\" \n",
            "\n",
            "\"Oh, we must get back,\" said Dick. \"If only we could find Tom,\" he added, with a sigh. \n",
            "\n",
            "It was fully an hour after they had left the campfire at the entrance to the cave of the mountain. They had walked to the chasm where they thought Tom might have had a tumble and crossed and recrossed it several times. But they had found no traces of the missing Rover boy. \n",
            "\n",
            "\"If only we knew whether he went down in that opening!\" said Sam, for at least the tenth time. \"Dick, do you suppose we can climb down into it?\" \n",
            "\n",
            "\"Not without a rope, Sam. The sides are too steep and slippery.\" \n",
            "\n",
            "Time and again they called down. But no answer came back. If Tom was down there he was either unconscious or dead. \n",
            "\n",
            "And now it had begun to snow harder than ever. The air was so full of the white flakes that they could not see ten feet in any direction. It was a typical Alaskan snowstorm. There was a sweep to the wind that found the very marrow of their bones. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Who says it's getting cold?\n",
            "A: Dick\n",
            "R: My gracious, Dick\n",
            "PA: his brother\n",
            "\n",
            "Q: Is it snowing?\n",
            "A: yes\n",
            "R: mountains putty quick we'll be snowed in\n",
            "PA: no\n",
            "\n",
            "Q: What will happen if they stay?\n",
            "A: they will be snowed in\n",
            "R: et out of the mountains putty quick we'll be snowed in\n",
            "PA: he was going to go to the\n",
            "\n",
            "Q: Who says this?\n",
            "A: Wumble\n",
            "R: came from Jack Wumble\n",
            "PA: his brother\n",
            "\n",
            "Q: What is his first name?\n",
            "A: Jack\n",
            "R: came from Jack Wumble\n",
            "PA: tom\n",
            "\n",
            "Q: How long did it take them to get to their destination?\n",
            "A: an hour\n",
            "R: It was fully an hour\n",
            "PA: a few minutes\n",
            "\n",
            "Q: Where had they come from?\n",
            "A: entrance to the cave of the mountain\n",
            "R: they had left the campfire at the entrance to the cave of the mountain\n",
            "PA: in the woods\n",
            "\n",
            "Q: What was there?\n",
            "A: campfire\n",
            "R: the campfire at the entrance to the cave of the mountain\n",
            "PA: the snow and snow\n",
            "\n",
            "Q: Did they find what they were looking for?\n",
            "A: no\n",
            "R: they had found no traces of the missing Rover boy\n",
            "PA: yes\n",
            "\n",
            "Q: What was it?\n",
            "A: Tom\n",
            "R: Tom might have had a tumble \n",
            "PA: the snow.\n",
            "\n",
            "Q: What would have been helpful to know?\n",
            "A: whether he went down in the opening\n",
            "R: we knew whether he went down in that opening\n",
            "PA: he's going to go to\n",
            "\n",
            "Q: Who says this?\n",
            "A: Sam\n",
            "R: said Sam\n",
            "PA: his brother\n",
            "\n",
            "Q: Did he say it once?\n",
            "A: no\n",
            "R: said Sam, for at least the tenth time\n",
            "PA: yes\n",
            "\n",
            "Q: After how many times?\n",
            "A: at least ten\n",
            "R: at least the tenth time\n",
            "PA: two\n",
            "\n",
            "Q: What does he suggest?\n",
            "A: climb into the opening\n",
            "R: suppose we can climb down into it\n",
            "PA: he's going to see the\n",
            "\n",
            "Q: Can they?\n",
            "A: no\n",
            "R: Not without a rope, Sam\n",
            "PA: yes\n",
            "\n",
            "Q: Says who?\n",
            "A: Dick\n",
            "R: Dick, do you suppose\n",
            "PA: his brother\n",
            "\n",
            "Q: Why not?\n",
            "A: they don't have a rope\n",
            "R: Not without a rope, Sam\n",
            "PA: he didn't want to see\n",
            "\n",
            "Q: What do they do then?\n",
            "A: they called down\n",
            "R: Time and again they called down\n",
            "PA: he was going to go to the\n",
            "\n",
            "Q: Does anyone respond?\n",
            "A: no\n",
            "R: But no answer came back\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "K6ZYr7Qt74dy"
    },
    {
      "cell_type": "code",
      "source": [
        "show_worst_5_passages_per_source('mctest', bert2bert_results, history = False)"
      ],
      "metadata": {
        "id": "9Dy7vtfb74dy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf7ab08-779c-488e-c035-e2e6d4c2727d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################  MCTEST  #####################\n",
            "\n",
            "\n",
            "PASSAGE 1:\n",
            "\n",
            "Once upon a time, there was a squirrel named Joey. Joey loved to go outside and play with his cousin Jimmy. Joey and Jimmy played silly games together, and were always laughing. One day, Joey and Jimmy went swimming together at their Aunt Julie's pond. Joey woke up early in the morning to eat some food before they left. He couldn't find anything to eat except for pie! Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast. After he ate, he and Jimmy went to the pond. On their way there they saw their friend Jack Rabbit. They dove into the water and swam for several hours. The sun was out, but the breeze was cold. Joey and Jimmy got out of the water and started walking home. Their fur was wet, and the breeze chilled them. When they got home, they dried off, and Jimmy put on his favorite purple shirt. Joey put on a blue shirt with red and green dots. The two squirrels ate some food that Joey's mom, Jasmine, made and went off to bed.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: what kind of animal was joey\n",
            "A: a squirrel\n",
            "R: a squirrel\n",
            "PA: a dog\n",
            "\n",
            "Q: what was his cousin's name\n",
            "A: Jimmy\n",
            "R: Jimmy\n",
            "PA: a dog\n",
            "\n",
            "Q: Were they happy?\n",
            "A: yes\n",
            "R: always laughing\n",
            "PA: no\n",
            "\n",
            "Q: Where did they go?\n",
            "A: outside\n",
            "R: outside \n",
            "PA: in the woods\n",
            "\n",
            "Q: Then where?\n",
            "A: at their Aunt Julie's pond\n",
            "R: at their Aunt Julie's pond\n",
            "PA: in the woods\n",
            "\n",
            "Q: what did he eat before he left\n",
            "A: pie\n",
            "R: pie\n",
            "PA: a dog\n",
            "\n",
            "Q: Who was at the pond?\n",
            "A: Jack Rabbit\n",
            "R: Jack Rabbit\n",
            "PA: a dog\n",
            "\n",
            "Q: Who is Joey's mom?\n",
            "A: Jasmine\n",
            "R: Jasmine\n",
            "PA: a dog\n",
            "\n",
            "Q: What did they do at the pond?\n",
            "A: swam\n",
            "R: swam \n",
            "PA: he was going to eat\n",
            "\n",
            "Q: What was the weather?\n",
            "A: Sunny\n",
            "R: The sun was out\n",
            "PA: ice cream\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 2:\n",
            "\n",
            "As Michael put each finger on the white laces of the football like his dad had shown him he thought about his school trip to the zoo tomorrow. He could not wait to get to the zoo and most of all could not wait to see his favorite animal, the lion. Aiming the football at the tire swing that hung in his back yard, he remembered the second thing his dad had taught him about throwing a football which was making sure his shoulder and the football were in a straight line before he threw it. He watched the football sail toward the tire, right as his mom called him in for dinner. His mom had made his favorite food, hotdogs. He sat in the kitchen and watched as ketchup fell on to his plate as he ate his hotdog. His mom told him that in order to get his after dinner treat he would have to eat his corn, carrots, and drink all of his milk too. \n",
            "\n",
            "That night as his mom tucked him in to bed he starred out the window and wondered if the lions at the zoo were looking up at the moon too. Michael, wondered if his best friends Joe, Nick, and Ryan were as excited as he was about going to the zoo the next day. He closed his eyes and went to sleep. \n",
            "\n",
            "The next day he hopped from one foot to the other as his class lined up to get on the bus that would take him to the zoo. On the bus he sat with Ryan. The bus driver started the engine and turned the big steering wheel leading them out on to the road. Finally, at the zoo Michael began to imagine how cool it would be to finally get to see the lion cage. First his class went to see the monkeys and then headed over to see the long necked giraffes. As their teacher announced that they would then be going to see the elephants, we wondered if he would ever get to see the lions. Finally after learning about the elephants it was time to see the lions. The lion stood on a huge rock and swung its long tail from side to side. The lion licked his lips with its long pink tongue and Michael wondered if it was thinking about having a class full of kids for its lunch.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What sport was Michael playing?\n",
            "A: football\n",
            "R: Michael put each finger on the white laces of the football\n",
            "PA: soccer\n",
            "\n",
            "Q: What was he throwing the ball at?\n",
            "A: a tire swing\n",
            "R: Aiming the football at the tire swing\n",
            "PA: a ball\n",
            "\n",
            "Q: What was his favorite food?\n",
            "A: hotdogs\n",
            "R: his favorite food, hotdogs\n",
            "PA: a dog\n",
            "\n",
            "Q: Where was he going the next day?\n",
            "A: go to the zoo\n",
            "R: his school trip to the zoo tomorrow\n",
            "PA: at the park\n",
            "\n",
            "Q: Who did he sit beside on the bus?\n",
            "A: Ryan\n",
            "R: On the bus he sat with Ryan\n",
            "PA: his brother\n",
            "\n",
            "Q: What did the class go see first?\n",
            "A: the monkeys\n",
            "R: First his class went to see the monkeys\n",
            "PA: a ball\n",
            "\n",
            "Q: Then where?\n",
            "A: giraffes\n",
            "R: then headed over to see the long necked giraffes\n",
            "PA: at the park\n",
            "\n",
            "Q: What was he really anxious to see?\n",
            "A: the lion\n",
            "R: could not wait to see his favorite animal, the lion\n",
            "PA: he didn't want to play\n",
            "\n",
            "Q: What was it standing on?\n",
            "A: a rock\n",
            "R: The lion stood on a huge rock\n",
            "PA: a ball\n",
            "\n",
            "Q: Who were Michael's other friends?\n",
            "A: Joe and Nick\n",
            "R: his best friends Joe, Nick, and Ryan\n",
            "PA: his brother\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 3:\n",
            "\n",
            "Lisa has a pet cat named Whiskers. Whiskers is black with a white spot on her chest. Whiskers also has white paws that look like little white mittens. \n",
            "\n",
            "Whiskers likes to sleep in the sun on her favorite chair. Whiskers also likes to drink creamy milk. \n",
            "\n",
            "Lisa is excited because on Saturday, Whiskers turns two years old. \n",
            "\n",
            "After school on Friday, Lisa rushes to the pet store. She wants to buy Whiskers' birthday presents. Last year, she gave Whiskers a play mouse and a blue feather. \n",
            "\n",
            "For this birthday, Lisa is going to give Whiskers a red ball of yarn and a bowl with a picture of a cat on the side. The picture is of a black cat. It looks a lot like Whiskers.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Why is Lisa excited?\n",
            "A: Saturday, Whiskers turns two\n",
            "R: Saturday, Whiskers turns two\n",
            "PA: he's going to go to\n",
            "\n",
            "Q: Who is Whiskers?\n",
            "A: a cat\n",
            "R: a pet cat\n",
            "PA: his mother\n",
            "\n",
            "Q: What does it look like?\n",
            "A: black with a white spot\n",
            "R: black with a white spot\n",
            "PA: a dog\n",
            "\n",
            "Q: Where did she rush on Friday?\n",
            "A: the pet store\n",
            "R: the pet store\n",
            "PA: in the kitchen\n",
            "\n",
            "Q: For what?\n",
            "A: to buy presents\n",
            "R: to buy Whiskers' birthday presents\n",
            "PA: it's a little dog\n",
            "\n",
            "Q: What did she get last year?\n",
            "A: a play mouse\n",
            "R: a play mouse\n",
            "PA: a little dog\n",
            "\n",
            "Q: anything else?\n",
            "A: a blue feather\n",
            "R: a blue feather\n",
            "PA: he's a little dog\n",
            "\n",
            "Q: What about this birthday?\n",
            "A: a ball of yarn\n",
            "R: a red ball of yarn\n",
            "PA: it's a little\n",
            "\n",
            "Q: What else?\n",
            "A: a bowl\n",
            "R: a bowl with a picture of a cat on the side.\n",
            "PA: it's a dog\n",
            "\n",
            "Q: whats on the bowl?\n",
            "A: a picture\n",
            "R: a picture of a cat\n",
            "PA: a bowl\n",
            "\n",
            "Q: of what?\n",
            "A: a black cat\n",
            "R: a black cat\n",
            "PA: a dog\n",
            "\n",
            "Q: Does it match Whiskers?\n",
            "A: definitely\n",
            "R: It looks a lot like Whiskers\n",
            "PA: yes\n",
            "\n",
            "Q: Where does this cat sleep?\n",
            "A: on her favorite chair\n",
            "R: on her favorite chair\n",
            "PA: in the kitchen\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 4:\n",
            "\n",
            "Hey! That isn't fair! Knights can't fly! Only wizards like me can! And maybe also witches. Bobby yelled. \n",
            "\n",
            "\"Yes! They can, too!\" Frank said, \"They're the most powerful and you can't stop me from flying!\" \n",
            "\n",
            "\"Well, if you fly, then I can fight with a sword, too. It's only fair,\" Bobby said back. \n",
            "\n",
            "\"No! They can't! They aren't even strong enough to pick up a sword. They can't even pick up a knife, they're so weak! I don't even know what you're talking about. You're crazy, aren't you?\" Frank pointed his finger at Bobby and ran at him with the sword made out of cardboard. \n",
            "\n",
            "Bobby jumped out of the way as quickly as he could! Bobby then pointed a finger at Frank. \"FREEZE!\" \n",
            "\n",
            "Frank stopped in place. \"Hey, you can't use the freeze spell. How can I fight you if I can't move? I'm going to tell mom.\" \n",
            "\n",
            "\"I can use any spell I want! I can use Freeze, Trap, and Fly! I can do any of them! I'm also telling mom that you think you can fly and you can't. You're a stupid knight with no brains. I have all the brains here.\" Bobby crossed his arms over his chest and stomped a foot on the ground. \n",
            "\n",
            "\"You take that back! I'm the smartest knight there is and I'll get you any day!\" Suddenly, Frank pulled Bobby to the ground. \"Take it back right now!\" \n",
            "\n",
            "\"BOYS! What's all the noise?\" Mom asked. \n",
            "\n",
            "\"Bobby called me stupid!\" Frank yelled. \n",
            "\n",
            "\"Frank is trying to fly!\" Bobby cried. \n",
            "\n",
            "\"Oh boy.\" Mom laughed.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What character is Frank playing?\n",
            "A: Knight\n",
            "R: Knight\n",
            "PA: a dog\n",
            "\n",
            "Q: Who thinks knights can fly?\n",
            "A: Frank\n",
            "R: Frank \n",
            "PA: his brother\n",
            "\n",
            "Q: Who thinks they can't?\n",
            "A: bobby\n",
            "R: Knights can't fly! \n",
            "PA: his brother\n",
            "\n",
            "Q: Who first casts a freeze spell?\n",
            "A: Bobby\n",
            "R: Bobby jumped out of the way as quickly as he could! Bobby then pointed a finger at Frank. \"FREEZE!\" \n",
            "\n",
            "PA: tom\n",
            "\n",
            "Q: Who is it used upon?\n",
            "A: Frank\n",
            "R: Bobby then pointed a finger at Frank. \"FREEZE!\"\n",
            "PA: his brother\n",
            "\n",
            "Q: What object to Frank use to attack?\n",
            "A: sword\n",
            "R: Frank pointed his finger at Bobby and ran at him with the sword made out of cardboard. \n",
            "PA: it was a knife\n",
            "\n",
            "Q: What material is it made of?\n",
            "A: cardboard.\n",
            "R:  the sword made out of cardboard. \n",
            "PA: a dog\n",
            "\n",
            "Q: Who stomps?\n",
            "A: Bobby\n",
            "R: Bobby crossed his arms over his chest and stomped a foot on the ground. \n",
            "PA: his brother\n",
            "\n",
            "Q: Who tattles first?\n",
            "A: Frank\n",
            "R: Bobby called me stupid!\" Frank yelled. \n",
            "PA: his brother\n",
            "\n",
            "Q: Then who does?\n",
            "A: Bobby\n",
            "R: Frank is trying to fly!\" Bobby cried. \n",
            "PA: his brother\n",
            "\n",
            "Q: To whom\n",
            "A: Mom\n",
            "R: Oh boy.\" Mom laughed\n",
            "PA: his brother\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 5:\n",
            "\n",
            "There once was a guy named Kevin and girl named Erin. They really liked spending time together. One day, Kevin traveled from his home in the United States of America to Erin's house in England. They then thought to take a sudden trip around the world. They first traveled by plane around Europe, where they saw many different people and sights. They then took a boat to Africa and Asia, where they went on a trip through the mountains. Later in the month, they traveled to China by train and were allowed to see how different life was over there. Next they took another plane to Australia, where they had a lot of fun seeing kangaroos and a different type of English speaking people. After spending a week in Australia, Kevin and Erin took a really long plane ride to North America, where they drove across the land. They saw everything from the mountains to forests. They even got to visit the beach! Because they had so much fun, Kevin returned home with Erin to England where they hung out and spent the next few days and months talking about all of the neat things they saw and did on their trip.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What was kevin's friend's name?\n",
            "A: Erin\n",
            "R: girl named Erin\n",
            "PA: the park\n",
            "\n",
            "Q: Where'd she live?\n",
            "A: in England\n",
            "R: Erin's house in England\n",
            "PA: new york city\n",
            "\n",
            "Q: Where'd they go first?\n",
            "A: around Europe\n",
            "R: hey first traveled by plane around Europe\n",
            "PA: new york city\n",
            "\n",
            "Q: how'd they get there?\n",
            "A: by plane\n",
            "R: traveled by plane around Europe\n",
            "PA: it's a lot of the\n",
            "\n",
            "Q: where'd they go next?\n",
            "A: Africa\n",
            "R: They then took a boat to Africa and Asia\n",
            "PA: new york city\n",
            "\n",
            "Q: how'd they get there?\n",
            "A: by boat\n",
            "R: They then took a boat to Africa\n",
            "PA: it's a lot of the\n",
            "\n",
            "Q: Where'd they go next?\n",
            "A: China\n",
            "R:  Later in the month, they traveled to China\n",
            "PA: new york city\n",
            "\n",
            "Q: how'd they get there?\n",
            "A: by train\n",
            "R: to China by train\n",
            "PA: it's a lot of the\n",
            "\n",
            "Q: then where?\n",
            "A: Australia\n",
            "R: Next they took another plane to Australia\n",
            "PA: at the park\n",
            "\n",
            "Q: and how?\n",
            "A: by plane\n",
            "R: took another plane to Australia\n",
            "PA: he was going to go to the\n",
            "\n",
            "Q: what'd they see in the states?\n",
            "A: mountains\n",
            "R: They saw everything from the mountains to forests\n",
            "PA: new york city\n",
            "\n"
          ]
        }
      ],
      "id": "9Dy7vtfb74dy"
    },
    {
      "cell_type": "code",
      "source": [
        "show_worst_5_passages_per_source('race', bert2bert_results, history = False)"
      ],
      "metadata": {
        "id": "0kJJREmT74dy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6ee937-bec1-4478-8aaa-6c2ce7823a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################  RACE  #####################\n",
            "\n",
            "\n",
            "PASSAGE 1:\n",
            "\n",
            "Tom Brennan was working in a Philadelphia office building when he noticed a black bag. The bag contained a book. This chance discovery ended a 12-day search by the Library Company of Philadelphia for a historical treasure - a 120-page diary kept 190 years ago by Deborah Logan, \"a woman who knew everybody in her day,\" James Green, the librarian told the magazine American Libraries. Most of the diary is a record of big events in Philadelphia. It also includes a description of British soldiers burning Washington, D.C. in the war of 1812. She describes President James Madison on horseback as \"perfectly shaking with fear\" during the troubled days. George Washington, she writes, mistook her for the wife of a French man, and praised her excellent English. The adventure of the lost book began September 4 when Cory Luxmoore arrived from England to deliver the diary of his ancestor to the Library Company, which he and his wife considered to be the best home for the diary. Green told American Libraries he had the diary in his possession \"about five minutes\" when Luxmoore took it back because he had promised to show it to one other person. On returning to his hotel after showing the precious book to Green, Luxmoore was shocked to realize that he had left it in the taxi. Without any delay, Green began calling every taxi company in the city, with no luck. \"I've felt sick since then,\" Luxmoore told reporters. According to Green, no one has yet learned how the diary came to the office building. Tom Brennan received a reward of $ 1,000, Philadelphia gained another treasure for its history, and Luxmoore told reporters, \"It's wonderful news. I'm on high.\"\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What was in the bag?\n",
            "A: A book.\n",
            "R: The bag contained a book.\n",
            "PA: new york city\n",
            "\n",
            "Q: What type of book?\n",
            "A: A diary.\n",
            "R: a 120-page diary kept 190 years ago by Deborah Logan\n",
            "PA: new york times\n",
            "\n",
            "Q: Whose diary was it?\n",
            "A: Deborah Logan's\n",
            "R:  a 120-page diary kept 190 years ago by Deborah Logan\n",
            "PA: mrs. smith\n",
            "\n",
            "Q: How long did it take the library to find it?\n",
            "A: 12 days.\n",
            "R: 12-day search by the Library Company of Philadelphia\n",
            "PA: two years\n",
            "\n",
            "Q: How old was the diary?\n",
            "A: 190 years old\n",
            "R: kept 190 years ago\n",
            "PA: two\n",
            "\n",
            "Q: What was recorded in the diary?\n",
            "A: Mostly big events in Philadelphia.\n",
            "R: Most of the diary is a record of big events in Philadelphia. It also includes a description of British soldiers burning Washington, D.C. in the war of 1812\n",
            "PA: new york times\n",
            "\n",
            "Q: What else?\n",
            "A: A description of British soldiers burning Washington, D.C.\n",
            "R: It also includes a description of British soldiers burning Washington, D.C.\n",
            "PA: a new york city\n",
            "\n",
            "Q: What did George Washington think of Deborah's command of the English language?\n",
            "A: Excellent English.\n",
            "R: George Washington, she writes, mistook her for the wife of a French man, and praised her excellent English\n",
            "PA: a new york city.\n",
            "\n",
            "Q: Why did he praise her for her excellence?\n",
            "A: Mistook her for the wife of a French man\n",
            "R: mistook her for the wife of a French man\n",
            "PA: he was a new york times\n",
            "\n",
            "Q: Who made the trip from England to the US?\n",
            "A: Cory Luxmoore\n",
            "R: Cory Luxmoore\n",
            "PA: the new york\n",
            "\n",
            "Q: Why did he make the trip?\n",
            "A: To deliver the diary.\n",
            "R: Luxmoore arrived from England to deliver the diary of his ancestor to the Library Company, which he and his wife considered to be the best home for the diary.\n",
            "PA: he was a new york times\n",
            "\n",
            "Q: What did he tell the reporters?\n",
            "A: \"I've felt sick since then.\"\n",
            "R: \"I've felt sick since then,\" Luxmoore told reporters\n",
            "PA: he was a new york times\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 2:\n",
            "\n",
            "Christopher Thomas, 27, was a writer by night and a teacher by day when he noticed he was always tired and was losing weight fast. Diagnosed with diabetes , Thomas would need to inject himself with insulin three times a day for the rest of his life or risk nerve damage, blindness, and even death. And if that weren't bad enough, he had no health insurance. After a month of feeling upset, Thomas decided he'd better find a way to fight back. He left Canton, Michigan for New York, got a job waiting tables, nicknamed himself the Diabetic Rockstar , and created diabeticrockstar.com, a free online community for diabetics and their loved ones--a place where over 1,100 people share personal stories, information, and resources. \n",
            "\n",
            "Jason Swencki\"s son, Kody, was diagnosed with type diabetes at six. Father and son visit the online children's forums together most evenings. \"Kody gets so excited, writing to kids from all over,\" says Swencki, one of the site's volunteers. \"They know what he's going through, so he doesn't feel alone.\" Kody is anything but alone: Diabetes is now the seventh leading cause of death in the United States, with 24 million diagnosed cases. And more people are being diagnosed at younger ages. \n",
            "\n",
            "These days, Thomas's main focus is his charity, Fight It, which provides medicines and supplies to people--225 to date--who can't afford a diabetic's huge expenses. Fight-it.org has raised about $23,000--in products and in cash. In May, Thomas will hold the first annual Diabetic Rockstar Festival in the Caribbean. \n",
            "\n",
            "Even with a staff of 22 volunteers, Thomas often devotes up to 50 hours a week to his cause, while still doing his full-time job waiting tables. \"Of the diabetes charities out there, most are putting money into finding a cure,\" says Bentley Gubar one of Rockstar's original members. \"But Christopher is the only person I know saying people need help now.\"\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What was Christopher's day job?\n",
            "A: teaching\n",
            "R: Christopher Thomas, 27, was a writer by night and a teacher by day\n",
            "PA: he was a lawyer\n",
            "\n",
            "Q: What disease was he diagnosed with?\n",
            "A: diabetes\n",
            "R: he noticed he was always tired and was losing weight fast. Diagnosed with diabetes\n",
            "PA: alzheimer disease disease\n",
            "\n",
            "Q: How often did he have to inject himself with insulin?\n",
            "A: three\n",
            "R: Thomas would need to inject himself with insulin three times a day \n",
            "PA: two years\n",
            "\n",
            "Q: How old was Kody when he was diagnosed with diabetes?\n",
            "A: six\n",
            "R: Kody, was diagnosed with type diabetes at six\n",
            "PA: 15\n",
            "\n",
            "Q: How many diagnosed cases of diabetes are there?\n",
            "A: 24 million\n",
            "R: Diabetes is now the seventh leading cause of death in the United States, with 24 million diagnosed cases\n",
            "PA: three\n",
            "\n",
            "Q: What website did Christopher start?\n",
            "A: diabeticrockstar.com\n",
            "R: He left Canton, Michigan for New York, got a job waiting tables, nicknamed himself the Diabetic Rockstar , and created diabeticrockstar.com, a free online community for diabetic\n",
            "PA: cnn\n",
            "\n",
            "Q: How many people use the site?\n",
            "A: over 1,100\n",
            "R: diabeticrockstar.com, a free online community for diabetics and their loved ones--a place where over 1,100 people\n",
            "PA: three\n",
            "\n",
            "Q: What is the name of his charity?\n",
            "A: Fight It,\n",
            "R: s charity, Fight It,\n",
            "PA: dr. smith\n",
            "\n",
            "Q: How many people has the charity helped?\n",
            "A: 225\n",
            "R: Fight It, which provides medicines and supplies to people--225\n",
            "PA: three\n",
            "\n",
            "Q: How much money has the charity raised?\n",
            "A: about $23,000--in products and in cash\n",
            "R: Fight-it.org has raised about $23,000--in products and in cash\n",
            "PA: $ 100, 000\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 3:\n",
            "\n",
            "A great loss--Shirley Temple dies at 85 \n",
            "\n",
            "February 12,2014 \n",
            "\n",
            "BYDERRIKJ.LANG ,Associated Press \n",
            "\n",
            "Shirley Temple Black, who died on February 10that age 85, wasn't just a child star. She was THE child star--the sweet little girl whose shining smile helped _ some of the darkest days the US has known during the Great Depression. \n",
            "\n",
            "It's hard today to imagine the super star Shirley was once \"America's Little Darling\". She sang and danced her way to the top of the box office in such films as Bright Eyes, Curly Top and Heidi. By 1940, she had appeared in 43 films. Temple teamed with Bill Robison in four movies, and their dance on the stairs in The Little Colonel is still a legendary film moment. \n",
            "\n",
            "In the 1930s, her name on a movie introduction assured a packed house. She inspired dolls, dresses, dishes--even a drink (alcohol-free, of course). \n",
            "\n",
            "US President Franklin D. Roosevelt once famously said that \"as long as our country has Shirley Temple, we will be all right.'' \n",
            "\n",
            "Unlike so many of today's child stars, Temple didn't end up with her name appearing across the headlines for bad behaviors. Instead of getting her photos on front pages or struggling with drugs and alcohol, Temple went on to a second career in diplomacy , including presidential appointments as ambassador to Ghana. \n",
            "\n",
            "She surprised a lot of people who doubted her with her grace, knowledge and eagerness to serve. In fact, her career in public service (20 years) was longer than her career in movies (19). The role she valued most, however, was as wife, mother, grandmother and great-grandmother. \n",
            "\n",
            "The world has lost a treasured Hollywood legend. But her movies will allow that little dynamic figure to continue charming audiences for a very long time.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Who is this news story about?\n",
            "A: Shirley Temple Black\n",
            "R: Shirley Temple Black\n",
            "PA: a young woman\n",
            "\n",
            "Q: In what industry did she become famous?\n",
            "A: Entertainment.\n",
            "R: She was THE child star\n",
            "PA: new york city.\n",
            "\n",
            "Q: How many movies did she appear in?\n",
            "A: 43 films.\n",
            "R: she had appeared in 43 films.\n",
            "PA: three\n",
            "\n",
            "Q: Any examples?\n",
            "A: Bright Eyes, Curly Top\n",
            "R: Bright Eyes, Curly Top\n",
            "PA: yes\n",
            "\n",
            "Q: Did she continue to act her whole life?\n",
            "A: No.\n",
            "R: She was THE child star\n",
            "PA: yes\n",
            "\n",
            "Q: What else did she do?\n",
            "A: Second career in diplomacy.\n",
            "R: Temple went on to a second career in diplomacy\n",
            "PA: she was a young woman.\n",
            "\n",
            "Q: Would you say her family or carrer was more important to her?\n",
            "A: Family.\n",
            "R:  The role she valued most, however, was as wife, mother, grandmother and great-grandmother. \n",
            "PA: she's a young woman\n",
            "\n",
            "Q: How old was she when she started acting?\n",
            "A: Four.\n",
            "R: By 1940, she had appeared in 43 films.\n",
            "PA: 14\n",
            "\n",
            "Q: And during what period of US history was that?\n",
            "A: The Great Depression.\n",
            "R: darkest days the US has known during the Great Depression. \n",
            "PA: the late 19th century\n",
            "\n",
            "Q: What nick-name did she pickup?\n",
            "A: \"America's Little Darling\n",
            "R: \"America's Little Darling\n",
            "PA: a young man\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 4:\n",
            "\n",
            "Ron Meyer is the president and COO of Universal Studios.As one of the most successful business leaders in Hollywood, he heads up Universal Pictures and Universal Parks & Resorts.He's the guy who oversees the production of Multimillion-dollar extravaganzas likeKing KongandCinderella Man. \n",
            "\n",
            "Meyer's story sounds like the plot of one of his motion pictures.He grew up in a modest home where there was little income.It was a big deal to go to a restaurant.At 15, he quit high school and spent his time hanging out with the neighborhood toughs .He was then a kid quick with his fists who seemed to get into fights somewhat regularly.At one point, he was separated from others with an infectious disease, having no TV and nothing to do.His mother sent him two books.One wasThe Amboy Dukes, a novel about kids in street gangs.The other wasThe Flesh Peddlers, about a guy in the talent agency who lived a successful life.\"I realized,\" he says, \"that I was no longer that silly kid I had been, and I wanted to change my life.\" \n",
            "\n",
            "Meyer took any job he could get.He worked as a busboy and short-order cook.He cleaned offices and sold shoes.That attitude made an impression on people.One day he received a call from Paul Kohner, a successful agent who represented stars like John Huston, Charles Bronson and Lana Turner.Their messenger and driver had quit, and they knew Ron was willing to take whatever job they offered. He started the job the next day. \n",
            "\n",
            "Meyer was lucky to work with a good boss--and _ .In nearly six years of driving for Kohner, Meyer became his right-hand man and learned a lot.By the 1970s, Meyer had built many relationships in the business.In 1975, the fate presented an opportunity and he started his own operation Creative Artists Agency, which became a huge success, representing Hollywood legends like Barbra Streisand, Tom Hanks and Tom Cruise. \n",
            "\n",
            "Twenty years later, Meyer was appointed to run Universal Studios, a position far beyond his youthful dream.But once he saw success was possible, he was driven to achieve it.Today, colleagues regularly owe his success--and theirs--to his humility and perseverance.It's a level of success that takes determination, personality and intelligence, whether it comes from a college education or from the street.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Meyer grew up where ?\n",
            "A: Universal Parks & Resorts.\n",
            "R: Universal Parks & Resorts.\n",
            "PA: new york city\n",
            "\n",
            "Q: where is he the president ?\n",
            "A: Universal Studios\n",
            "R: Universal Studios\n",
            "PA: new york city\n",
            "\n",
            "Q: hom many multi million dollar productions he did ?\n",
            "A: Two\n",
            "R: King KongandCinderella Man\n",
            "PA: 10, 000\n",
            "\n",
            "Q: did he finish school ?\n",
            "A: no\n",
            "R: he quit high schoo\n",
            "PA: yes\n",
            "\n",
            "Q: how old was he ?\n",
            "A: 15\n",
            "R: 15\n",
            "PA: 14\n",
            "\n",
            "Q: how did he spend time ?\n",
            "A: hanging out with the neighborhood toughs\n",
            "R: hanging out with the neighborhood toughs\n",
            "PA: he was born in his family\n",
            "\n",
            "Q: Paul Kohner is who ?\n",
            "A: successful agen\n",
            "R:  successful agen\n",
            "PA: his father\n",
            "\n",
            "Q: how many stars sis he repressent ?\n",
            "A: Three\n",
            "R: hn Huston, Charles Bronson and Lana Turner\n",
            "PA: two\n",
            "\n",
            "Q: name 2\n",
            "A: John Huston, Charles Bronson\n",
            "R: John Huston, Charles Bronson\n",
            "PA: a new york city\n",
            "\n",
            "Q: how many years did he drive for someone ?\n",
            "A: six years\n",
            "R: six years\n",
            "PA: two\n",
            "\n",
            "Q: for who ?\n",
            "A: Kohner\n",
            "R: Kohner\n",
            "PA: his wife.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 5:\n",
            "\n",
            "I'm Leo. There is a great artist in my family. She is Lisa. Lisa likes drawing a lot. Here are her three drawings. There is an animal in the first drawing. It has two big eyes, a big mouth and two small ears. It has long arms and long legs. It is black and white. There are some apples in its hands. It looks very happy. What is it? I don't know, but Lisa says it is a panda. There is a woman in the second drawing. She is thin. She has straight blonde hair, a small mouth, a big nose and two big eyes. She looks angry. Who is she? Lisa says she is our mom. But Mom has curly blonde hair, small eyes and a small nose. There is an animal in the third drawing, too. Its head is an apple. Its hair and tail are leaves. It has a long mouth, and _ is a banana. Its two legs are carrots. What is it? Lisa says it is a horse, but it doesn't look like a horse. Lisa is really a great artist, isn't she? .\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What is in the second drawing?\n",
            "A: a woman\n",
            "R: . There is a woman in the second drawing.\n",
            "PA: a dog\n",
            "\n",
            "Q: Who is the artist in the family?\n",
            "A: Lisa\n",
            "R: There is a great artist in my family. She is Lisa.\n",
            "PA: her mother\n",
            "\n",
            "Q: What is in the third drawing?\n",
            "A: an animal\n",
            "R: There is an animal in the third drawing,\n",
            "PA: a dog\n",
            "\n",
            "Q: What is it's head made out of?\n",
            "A: apple\n",
            "R:  Its head is an apple\n",
            "PA: a dog\n",
            "\n",
            "Q: And what about it's hair and tail?\n",
            "A: leaves\n",
            "R:  Its hair and tail are leaves.\n",
            "PA: black and white\n",
            "\n",
            "Q: What are its legs made out of?\n",
            "A: carrots\n",
            "R: Its two legs are carrots.\n",
            "PA: blue and blue\n",
            "\n",
            "Q: Did Lisa say it was a zebra?\n",
            "A: no\n",
            "R:  Lisa says it is a horse\n",
            "PA: yes\n",
            "\n",
            "Q: What is in the first drawing?\n",
            "A: an animal\n",
            "R: There is an animal in the first drawing.\n",
            "PA: a dog\n",
            "\n",
            "Q: What's in it's hands?\n",
            "A: apples\n",
            "R: There are some apples in its hands.\n",
            "PA: a dog\n",
            "\n",
            "Q: Does the woman in the second picture have brown hair?\n",
            "A: no\n",
            "R: She has straight blonde hair\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "0kJJREmT74dy"
    },
    {
      "cell_type": "code",
      "source": [
        "show_worst_5_passages_per_source('wikipedia', bert2bert_results, history = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16888e1-5ba8-4dbd-d0b6-be5d35378001",
        "id": "2WaLx9YI74dy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################  WIKIPEDIA  #####################\n",
            "\n",
            "\n",
            "PASSAGE 1:\n",
            "\n",
            "Providence is the capital of and most populous city in the U.S. state of Rhode Island, founded in 1636 and one of the oldest cities in the United States. It was founded by Roger Williams, a religious exile from the Massachusetts Bay Colony. He named the area in honor of \"God's merciful Providence\" which he believed was responsible for revealing such a haven for him and his followers to settle. The city is situated at the mouth of the Providence River at the head of Narragansett Bay. \n",
            "\n",
            "Providence was one of the first cities in the country to industrialize and became noted for its textile manufacturing and subsequent machine tool, jewelry, and silverware industries. Today, the city of Providence is home to eight hospitals and seven institutions of higher learning which have shifted the city's economy into service industries, though it still retains some manufacturing activity. The city was once nicknamed the \"Beehive of Industry\"; it began rebranding itself as the \"Creative Capital\" in 2009 to emphasize its educational resources and arts community. \n",
            "\n",
            "The city is located in Providence County and is the third most populous city in New England, after Boston and Worcester. Providence has a city population of 179,154; it is also part of the Providence metropolitan area which extends into southern Massachusetts. The Providence metropolitan area has an estimated population of 1,604,291, which exceeds that of Rhode Island as a whole by about 60%. This can be considered, in turn, to be part of the Greater Boston commuting area, which contains 7.6 million people.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What was Providence once nicknamed?\n",
            "A: \"Beehive of Industry\"\n",
            "R: The city was once nicknamed the \"Beehive of Industry\n",
            "PA: the new york city\n",
            "\n",
            "Q: When did it rebrand itself as the \"Creative Captial\"?\n",
            "A: 2009\n",
            "R:  it began rebranding itself as the \"Creative Capital\" in 2009\n",
            "PA: in 1880\n",
            "\n",
            "Q: Which state is it the capital of?\n",
            "A: Rhode Island\n",
            "R: Providence is the capital of and most populous city in the U.S. state of Rhode Island\n",
            "PA: new york\n",
            "\n",
            "Q: What year was it founded?\n",
            "A: 1636\n",
            "R: founded in 1636 \n",
            "PA: 1871\n",
            "\n",
            "Q: By who?\n",
            "A: Roger Williams\n",
            "R: It was founded by Roger Williams\n",
            "PA: the united states.\n",
            "\n",
            "Q: What did he name the area for?\n",
            "A: in honor of \"God's merciful Providence\"\n",
            "R: He named the area in honor of \"God's merciful Providence\"\n",
            "PA: the new york city\n",
            "\n",
            "Q: How many hospitals does it have?\n",
            "A: eight\n",
            "R: eight hospitals\n",
            "PA: three\n",
            "\n",
            "Q: And how many institutions of higher education?\n",
            "A: seven\n",
            "R: seven institutions of higher learning\n",
            "PA: more than 100\n",
            "\n",
            "Q: What county is it in?\n",
            "A: Providence County\n",
            "R: The city is located in Providence County\n",
            "PA: new york city\n",
            "\n",
            "Q: What is the population of the city?\n",
            "A: 179,154\n",
            "R: Providence has a city population of 179,154\n",
            "PA: 1. 5 million\n",
            "\n",
            "Q: What about the population of the metropolitan area?\n",
            "A: 1,604,291\n",
            "R: The Providence metropolitan area has an estimated population of 1,604,291\n",
            "PA: 1. 5 million\n",
            "\n",
            "Q: And how many are in the Greater Boston commuting area?\n",
            "A: 7.6 million\n",
            "R: Greater Boston commuting area, which contains 7.6 million people.\n",
            "PA: three\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 2:\n",
            "\n",
            "Weimar Republic was an unofficial, historical designation for the German state between 1919 and 1933. The name derives from the city of Weimar, where its constitutional assembly first took place. The official name of the state was \"Deutsches Reich\"; it had remained unchanged since 1871. In English the country was usually known simply as Germany. A national assembly was convened in Weimar, where a new constitution for the \"Deutsches Reich\" was written, and adopted on 11 August 1919. In its fourteen years, the Weimar Republic faced numerous problems, including hyperinflation, political extremism (with paramilitaries – both left- and right-wing), as well as contentious relationships with the victors of the First World War. The people of Germany blamed the Weimar Republic rather than their wartime leaders for the country's defeat and for the humiliating terms of the Treaty of Versailles. Weimar Germany fulfilled most of the requirements of the Treaty of Versailles although it never completely met its disarmament requirements, and eventually paid only a small portion of the war reparations (by twice restructuring its debt through the Dawes Plan and the Young Plan). Under the Locarno Treaties, Germany accepted the western borders of the republic, but continued to dispute the Eastern border.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What was an unofficial designation\n",
            "A: the Weimar Republic\n",
            "R: Weimar Republic was an unofficial, historical designation\n",
            "PA: the soviet union.\n",
            "\n",
            "Q: For what?\n",
            "A: the German state\n",
            "R: Weimar Republic was an unofficial, historical designation for the German state\n",
            "PA: the napoleonic wars\n",
            "\n",
            "Q: During what year?\n",
            "A: between 1919 and 1933\n",
            "R: historical designation for the German state between 1919 and 1933\n",
            "PA: 1871\n",
            "\n",
            "Q: Was Weimar a city?\n",
            "A: Yes\n",
            "R: he name derives from the city of Weimar\n",
            "PA: no\n",
            "\n",
            "Q: What took place there?\n",
            "A: its constitutional assembly\n",
            "R:  Weimar, where its constitutional assembly\n",
            "PA: the 19th century\n",
            "\n",
            "Q: What was the official name?\n",
            "A: Deutsches Reich\n",
            "R: The official name of the state was \"Deutsches Reich\n",
            "PA: the napoleonic wars\n",
            "\n",
            "Q: When did that begin?\n",
            "A: 1871\n",
            "R: \"Deutsches Reich\"; it had remained unchanged since 1871\n",
            "PA: the 19th century\n",
            "\n",
            "Q: What was adopted in 1919\n",
            "A: a new constitution\n",
            "R: new constitution for the \"Deutsches Reich\" was written, and adopted on 11 August 1919.\n",
            "PA: the napoleonic wars\n",
            "\n",
            "Q: What month?\n",
            "A: August\n",
            "R: adopted on 11 August\n",
            "PA: january\n",
            "\n",
            "Q: What day?\n",
            "A: the 11th\n",
            "R: adopted on 11 Augus\n",
            "PA: october\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 3:\n",
            "\n",
            "Since the Protestant Reformation, the most prominent Christian denomination in Thuringia has been Lutheranism. During the GDR period, church membership was discouraged and has continued shrinking since the reunification in 1990. Today over two thirds of the population is non-religious. The Protestant Evangelical Church in Germany has had the largest number of members in the state, adhered to by 24.0% of the population in 2009. Members of the Catholic Church formed 7.8% of the population, while 68.2% of Thuringians were non-religious or adhere to other faiths. The highest Protestant concentrations are in the small villages of southern and western Thuringia, whereas the bigger cities are even more non-religious (up to 88% in Gera). Catholic regions are the Eichsfeld in the northwest and parts of the Rhön Mountains around Geisa in the southwest. Protestant church membership is shrinking rapidly, whereas the Catholic Church is somewhat more stable because of Catholic migration from Poland, Southern Europe and West Germany. Other religions play no significant role in Thuringia. There are only a few thousand Muslims (largely migrants) and about 750 Jews (mostly migrants from Russia) living in Thuringia. Furthermore, there are some Orthodox communities of Eastern European migrants and some traditional Protestant Free churches in Thuringia without any societal influence.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: what is the biggest church in Germany?\n",
            "A: The Protestant Evangelical Church\n",
            "R: The Protestant Evangelical Church in Germany has had the largest number of members in the state\n",
            "PA: the united states\n",
            "\n",
            "Q: what percent of the local people are in that church?\n",
            "A: 24.0% of the population in 2009\n",
            "R: adhered to by 24.0% of the population in 2009\n",
            "PA: 1. 1 million\n",
            "\n",
            "Q: what other group has a large percentage of people in it?\n",
            "A: the Catholic Church\n",
            "R: Members of the Catholic Church formed 7.8% of the population\n",
            "PA: 1, 000\n",
            "\n",
            "Q: are most people a part of one of these groups?\n",
            "A: no\n",
            "R: while 68.2% of Thuringians were non-religious\n",
            "PA: yes\n",
            "\n",
            "Q: what other ethnic groups are there in this area?\n",
            "A: Muslims, Jews\n",
            "R: There are only a few thousand Muslims (largely migrants) and about 750 Jews\n",
            "PA: the united states\n",
            "\n",
            "Q: where are they originally from?\n",
            "A: migrants from Russia\n",
            "R: Jews (mostly migrants from Russia)\n",
            "PA: south africa\n",
            "\n",
            "Q: how many are there?\n",
            "A: about 750 Jews\n",
            "R:  about 750 Jews\n",
            "PA: three\n",
            "\n",
            "Q: when did church-going really start decreasing in the area?\n",
            "A: During the GDR period\n",
            "R: During the GDR period, church membership was discouraged\n",
            "PA: in the late 19th century\n",
            "\n",
            "Q: why did it decrease?\n",
            "A: church membership was discouraged\n",
            "R: church membership was discouraged\n",
            "PA: it is the most important part of\n",
            "\n",
            "Q: is catholicism decreasing as quickly?\n",
            "A: no\n",
            "R: Catholic Church is somewhat more stable\n",
            "PA: yes\n",
            "\n",
            "Q: why not?\n",
            "A: Catholic migration from Poland\n",
            "R: Catholic migration from Poland\n",
            "PA: it is the most important part of\n",
            "\n",
            "Q: and from what other areas?\n",
            "A: Southern Europe and West Germany\n",
            "R: Southern Europe and West Germany\n",
            "PA: the united states.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 4:\n",
            "\n",
            "Schleswig-Holstein is the northernmost of the 16 states of Germany, comprising most of the historical duchy of Holstein and the southern part of the former Duchy of Schleswig. Its capital city is Kiel; other notable cities are Lübeck and Flensburg. \n",
            "\n",
            "Also known in more dated English as Sleswick-Holsatia, the Danish name is \"Slesvig-Holsten,\" the Low German name is \"Sleswig-Holsteen,\" and the North Frisian name is \"Slaswik-Holstiinj.\" Historically, the name can also refer to a larger region, containing both present-day Schleswig-Holstein and the former South Jutland County (Northern Schleswig) in Denmark. \n",
            "\n",
            "The term \"Holstein\" derives from Old Saxon \"Holseta Land,\" (\"Holz\" and \"Holt\" mean wood in modern Standardised German and in literary English, respectively). Originally, it referred to the central of the three Saxon tribes north of the River Elbe: \"Tedmarsgoi\" (Dithmarschen), Holstein and \"Sturmarii\" (Stormarn). The area of the tribe of the Holsts was between the Stör River and Hamburg, and after Christianization, their main church was in Schenefeld. Saxon Holstein became a part of the Holy Roman Empire after Charlemagne's Saxon campaigns in the late eighth century. Since 811, the northern frontier of Holstein (and thus the Empire) was marked by the River Eider. \n",
            "\n",
            "The term Schleswig comes from the city of Schleswig. The name derives from the Schlei inlet in the east and \"vik\" meaning inlet in Old Norse or settlement in Old Saxon, and linguistically identical (cognate) with the \"-wick\" or \"-wich\" element in place-names in Britain.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What is the capital of Schleswig-Holstein?\n",
            "A: Kiel\n",
            "R:  Kiel\n",
            "PA: the alps\n",
            "\n",
            "Q: Name another city in it?\n",
            "A: Lübeck\n",
            "R: Lübec\n",
            "PA: the alps\n",
            "\n",
            "Q: Name one of the duchys it's comprised of?\n",
            "A: Holstein\n",
            "R: Holstein\n",
            "PA: the french\n",
            "\n",
            "Q: And the other?\n",
            "A: Duchy of Schleswig\n",
            "R: Duchy of Schleswig\n",
            "PA: yes\n",
            "\n",
            "Q: What's a Danish name for it?\n",
            "A: Slesvig-Holsten\n",
            "R: Slesvig-Holsten\n",
            "PA: the french\n",
            "\n",
            "Q: And the Low German name for it?\n",
            "A: Sleswig-Holsteen\n",
            "R: Sleswig-Holsteen\n",
            "PA: the alps\n",
            "\n",
            "Q: And the North Frisian name?\n",
            "A: Slaswik-Holstiinj\n",
            "R: Slaswik-Holstiinj\n",
            "PA: the alps\n",
            "\n",
            "Q: Is the modern German word for wood mentioned in this article?\n",
            "A: NO\n",
            "R: erma\n",
            "PA: yes\n",
            "\n",
            "Q: Name one of the three Saxon tribes north of the Elbe?\n",
            "A: Tedmarsgoi\n",
            "R: Tedmarsgoi\n",
            "PA: the french\n",
            "\n",
            "Q: And another?\n",
            "A: Holstein\n",
            "R: Holstein\n",
            "PA: yes\n",
            "\n",
            "Q: And yet another?\n",
            "A: Sturmarii\n",
            "R: Sturmarii\n",
            "PA: no\n",
            "\n",
            "Q: Was Holstein part of the Holy Roman Empire?\n",
            "A: YES\n",
            "R: Saxon Holstein became a part of the Holy Roman Emp\n",
            "PA: no\n",
            "\n",
            "Q: Who carried on Saxon campaigns?\n",
            "A: Charlemagne\n",
            "R: Charlemagne\n",
            "PA: the british empire\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 5:\n",
            "\n",
            "A psychological identity relates to self-image (one's mental model of oneself), self-esteem, and individuality. Consequently, Weinreich gives the definition \"A person's identity is defined as the totality of one's self-construal, in which how one construes oneself in the present expresses the continuity between how one construes oneself as one was in the past and how one construes oneself as one aspires to be in the future\"; this allows for definitions of aspects of identity, such as: \"One's ethnic identity is defined as that part of the totality of one's self-construal made up of those dimensions that express the continuity between one's construal of past ancestry and one's future aspirations in relation to ethnicity\" (Weinreich, 1986a). \n",
            "\n",
            "The description or representation of individual and group identity is a central task for psychologists, sociologists and anthropologists and those of other disciplines where \"identity\" needs to be mapped and defined. How should one describe the identity of another, in ways which encompass both their idiosyncratic qualities and their group memberships or identifications, both of which can shift according to circumstance? Following on from the work of Kelly, Erikson, Tajfel and others Weinreich's Identity Structure Analysis (ISA), is \"a structural representation of the individual's existential experience, in which the relationships between self and other agents are organised in relatively stable structures over time … with the emphasis on the socio-cultural milieu in which self relates to other agents and institutions\" (Weinreich and Saunderson, (eds) 2003, p1). Using constructs drawn from the salient discourses of the individual, the group and cultural norms, the practical operationalisation of ISA provides a methodology that maps how these are used by the individual, applied across time and milieus by the \"situated self\" to appraise self and other agents and institutions (for example, resulting in the individual's evaluation of self and significant others and institutions).[citation needed]\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What is the ISA?\n",
            "A: Identity Structure Analysis (ISA)\n",
            "R:  Identity Structure Analysis (ISA)\n",
            "PA: human rights and human rights, and\n",
            "\n",
            "Q: Who started it?\n",
            "A: Weinreich\n",
            "R: Following on from the work of Kelly, Erikson, Tajfel and others Weinreich's Identity Structure Analysis\n",
            "PA: the president of the united nations\n",
            "\n",
            "Q: What was the date of his quote?\n",
            "A: 1986\n",
            "R: One's ethnic identity is defined as that part of the totality of one's self-construal made up of those dimensions that express the continuity between one's construal of past ancestry and one's future aspirations in relation to ethnicity\" (Weinreich, 1986a).\n",
            "PA: july 2015.\n",
            "\n",
            "Q: Who collaborated with him?\n",
            "A: Saunderson\n",
            "R: Weinreich and Saunderson\n",
            "PA: the president of the president\n",
            "\n",
            "Q: In what year?\n",
            "A: 2003\n",
            "R: (Weinreich and Saunderson, (eds) 2003, p1)\n",
            "PA: 1991\n",
            "\n",
            "Q: What did they emphasize?\n",
            "A: the socio-cultural milieu\n",
            "R:  the socio-cultural milieu in which self relates to other agents and institutions\"\n",
            "PA: it's not not necessarily necessarily\n",
            "\n",
            "Q: In relation to what?\n",
            "A: in which self relates to other agents and institutions\n",
            "R:  in which self relates to other agents and institutions\" (Weinreich and Saunderson, \n",
            "PA: it is the subject of the subject\n",
            "\n",
            "Q: Where did the constraints come from?\n",
            "A: from the salient discourses of the individual, the group and cultural norms,\n",
            "R:  Using constructs drawn from the salient discourses of the individual, the group and cultural norms, \n",
            "PA: in the united states\n",
            "\n",
            "Q: What is mapped?\n",
            "A: \"identity\"\n",
            "R: \"identity\" needs to be mapped and defined.\n",
            "PA: it is the first part of the\n",
            "\n",
            "Q: What example is given?\n",
            "A: esulting in the individual's evaluation of self and significant others and institutions\n",
            "R: esulting in the individual's evaluation of self and significant others and institutions\n",
            "PA: it's not not necessarily necessarily\n",
            "\n",
            "Q: Was there a citation for this?\n",
            "A: citation needed\n",
            "R: itation needed\n",
            "PA: yes\n",
            "\n"
          ]
        }
      ],
      "id": "2WaLx9YI74dy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "By looking at predicted answers, it can be noticed that BERTTiny tends to predict wrong and non-contextualized responses. However, even when missing the correct answer, in most of the cases predict something at least plausible, as it was able to recognize the question type and coherently answer to interrogative adverbs/pronouns like *Where* or *Who* with an actual place or person respectively."
      ],
      "metadata": {
        "id": "aKq_uiM7U0cA"
      },
      "id": "aKq_uiM7U0cA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistilRoBERTa model"
      ],
      "metadata": {
        "id": "WAJftVjk74dy"
      },
      "id": "WAJftVjk74dy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best performance of DistilRoBERTa model on the test set has been obtained with the seed $42$. Thus, its results are analyzed.\n",
        "\n",
        "Again, `distilroberta_results_test` dataset is first converted into a `Dataframe`."
      ],
      "metadata": {
        "id": "W5xafMf5VyWR"
      },
      "id": "W5xafMf5VyWR"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_results = pd.DataFrame(distilroberta_results_test)"
      ],
      "metadata": {
        "id": "CUDEIOMH74dz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CUDEIOMH74dz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's compute the SQuAD F1-score on each Q&A pair and store the results."
      ],
      "metadata": {
        "id": "H80MvBLDVyWS"
      },
      "id": "H80MvBLDVyWS"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_results['f1_squad'] = distilroberta_results.apply(lambda x: squad.compute_f1(x.pred_answer, x.answer), axis=1)\n",
        "distilroberta_results.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "483f013d-8f5b-43ca-81fa-5aded38c0ede",
        "id": "dbDTkmDG74dz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               id  source  \\\n",
              "0  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "1  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "2  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "3  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "4  3dr23u6we5exclen4th8uq9rb42tel  mctest   \n",
              "\n",
              "                                             passage  \\\n",
              "0  Once upon a time, in a barn near a farm house,...   \n",
              "1  Once upon a time, in a barn near a farm house,...   \n",
              "2  Once upon a time, in a barn near a farm house,...   \n",
              "3  Once upon a time, in a barn near a farm house,...   \n",
              "4  Once upon a time, in a barn near a farm house,...   \n",
              "\n",
              "                       question                        answer  \\\n",
              "0        What color was Cotton?                         white   \n",
              "1           Where did she live?                     in a barn   \n",
              "2           Did she live alone?                            no   \n",
              "3        Who did she live with?  with her mommy and 5 sisters   \n",
              "4  What color were her sisters?              orange and white   \n",
              "\n",
              "                                             history  \\\n",
              "0                                                      \n",
              "1                     What color was Cotton? white.    \n",
              "2  Where did she live? in a barn. What color was ...   \n",
              "3  Did she live alone? no. Where did she live? in...   \n",
              "4  Who did she live with? with her mommy and 5 si...   \n",
              "\n",
              "                                     passage_history  \\\n",
              "0  Once upon a time, in a barn near a farm house,...   \n",
              "1  Once upon a time, in a barn near a farm house,...   \n",
              "2  Once upon a time, in a barn near a farm house,...   \n",
              "3  Once upon a time, in a barn near a farm house,...   \n",
              "4  Once upon a time, in a barn near a farm house,...   \n",
              "\n",
              "                                           rationale  rationale_end  \\\n",
              "0                 a little white kitten named Cotton             93   \n",
              "1  in a barn near a farm house, there lived a lit...             80   \n",
              "2                                Cotton wasn't alone            215   \n",
              "3                 with her mommy and 5 other sisters            315   \n",
              "4  her sisters were all orange with beautiful whi...            490   \n",
              "\n",
              "   __index_level_0__ pred_answer  f1_squad  \n",
              "0                  0       white       1.0  \n",
              "1                  1   in a barn       1.0  \n",
              "2                  2          no       1.0  \n",
              "3                  3      Cotton       0.0  \n",
              "4                  4      orange       0.5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e11f3099-d00e-4848-8c5e-5b4531e9be6c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>history</th>\n",
              "      <th>passage_history</th>\n",
              "      <th>rationale</th>\n",
              "      <th>rationale_end</th>\n",
              "      <th>__index_level_0__</th>\n",
              "      <th>pred_answer</th>\n",
              "      <th>f1_squad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>What color was Cotton?</td>\n",
              "      <td>white</td>\n",
              "      <td></td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>a little white kitten named Cotton</td>\n",
              "      <td>93</td>\n",
              "      <td>0</td>\n",
              "      <td>white</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Where did she live?</td>\n",
              "      <td>in a barn</td>\n",
              "      <td>What color was Cotton? white.</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>in a barn near a farm house, there lived a lit...</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>in a barn</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Did she live alone?</td>\n",
              "      <td>no</td>\n",
              "      <td>Where did she live? in a barn. What color was ...</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Cotton wasn't alone</td>\n",
              "      <td>215</td>\n",
              "      <td>2</td>\n",
              "      <td>no</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>Who did she live with?</td>\n",
              "      <td>with her mommy and 5 sisters</td>\n",
              "      <td>Did she live alone? no. Where did she live? in...</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>with her mommy and 5 other sisters</td>\n",
              "      <td>315</td>\n",
              "      <td>3</td>\n",
              "      <td>Cotton</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3dr23u6we5exclen4th8uq9rb42tel</td>\n",
              "      <td>mctest</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>What color were her sisters?</td>\n",
              "      <td>orange and white</td>\n",
              "      <td>Who did she live with? with her mommy and 5 si...</td>\n",
              "      <td>Once upon a time, in a barn near a farm house,...</td>\n",
              "      <td>her sisters were all orange with beautiful whi...</td>\n",
              "      <td>490</td>\n",
              "      <td>4</td>\n",
              "      <td>orange</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e11f3099-d00e-4848-8c5e-5b4531e9be6c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e11f3099-d00e-4848-8c5e-5b4531e9be6c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e11f3099-d00e-4848-8c5e-5b4531e9be6c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "id": "dbDTkmDG74dz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, the $5$ hardest conversations per source are extracted."
      ],
      "metadata": {
        "id": "s7LenSkmWDU_"
      },
      "id": "s7LenSkmWDU_"
    },
    {
      "cell_type": "code",
      "source": [
        "worst_5_passages = distilroberta_results.groupby(['source', 'id'],\n",
        "                                                 as_index=False).mean().drop(['rationale_end', '__index_level_0__'],\n",
        "                                                                             axis = 1).sort_values(['source','f1_squad']).groupby(['source']).head(5)\n",
        "worst_5_passages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "2f46db63-437d-4adc-a7ce-5e715e8322d1",
        "id": "D7d8fYAq74dz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        source                              id  f1_squad\n",
              "94         cnn  3z2r0dq0jhe3smkalexct301crj2ex  0.066667\n",
              "88         cnn  3y54sxro1lle1hb9utwdy8vuyvituv  0.250000\n",
              "30         cnn  3cfjtt4sxtqmusj2n94ya9f1f0f7iu  0.284330\n",
              "62         cnn  3nc5l260mom9579b3nffiyo4px4ofh  0.288810\n",
              "16         cnn  35bldd71i6xa08985bv0giyuwn3zv9  0.295000\n",
              "107  gutenberg  34z02eimisdylvztwmit917niam0tf  0.092593\n",
              "124  gutenberg  3b4yi393v9wx746qgown92hkbf4ssa  0.224091\n",
              "123  gutenberg  3azhrg4cu4ktme1zh7c2ro3po9a30y  0.255615\n",
              "172  gutenberg  3strjbfxowr0yl6x0fsbslmwvuztks  0.298674\n",
              "138  gutenberg  3ihr8nyam71hsrony6wbguw3agap42  0.299232\n",
              "290     mctest  3xuhv3nrvky7btuzty7gcd0qng1h5p  0.126618\n",
              "291     mctest  3y5140z9dxgb0yn2jvyfav6metnpii  0.227619\n",
              "295     mctest  3z4airp3c6d591tvxfnqc9b3zpwx1v  0.227778\n",
              "287     mctest  3wrfbplxraow7at6ide020z2vvc3nl  0.240000\n",
              "215     mctest  3634bbtx0ouz9ly85s2ay1sico1ifb  0.268182\n",
              "355       race  3mrnmeiqw56412sizp4x2hhpidqldp  0.083333\n",
              "317       race  37m28k1j0qd08516cu1iw1wrtkcajw  0.155952\n",
              "360       race  3ochawuvgok7f2fh5pt8ho7294rxkc  0.256905\n",
              "343       race  3ioen3p9s7jsqm9zwse0cwyj3kq612  0.268737\n",
              "363       race  3p59jyt76lk5h527b9m7sp02fzkt2o  0.315000\n",
              "460  wikipedia  3nd9uoo81k23a8s9gk9nu56app6lw0  0.025974\n",
              "413  wikipedia  354p56de9k3bo6myslycebloosw7sp  0.217372\n",
              "451  wikipedia  3jv9lgbjwtefj756e7lx0jogqf0goa  0.227083\n",
              "492  wikipedia  3yw4xosqkqldsxz0sac3s2cz64l1u8  0.284848\n",
              "441  wikipedia  3ftop5warfo47s3oks4p7vkek7uj0q  0.294048"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-04f92764-31e4-44dc-a6e5-14820653489e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>id</th>\n",
              "      <th>f1_squad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3z2r0dq0jhe3smkalexct301crj2ex</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3y54sxro1lle1hb9utwdy8vuyvituv</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3cfjtt4sxtqmusj2n94ya9f1f0f7iu</td>\n",
              "      <td>0.284330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3nc5l260mom9579b3nffiyo4px4ofh</td>\n",
              "      <td>0.288810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>cnn</td>\n",
              "      <td>35bldd71i6xa08985bv0giyuwn3zv9</td>\n",
              "      <td>0.295000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>34z02eimisdylvztwmit917niam0tf</td>\n",
              "      <td>0.092593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3b4yi393v9wx746qgown92hkbf4ssa</td>\n",
              "      <td>0.224091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3azhrg4cu4ktme1zh7c2ro3po9a30y</td>\n",
              "      <td>0.255615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3strjbfxowr0yl6x0fsbslmwvuztks</td>\n",
              "      <td>0.298674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3ihr8nyam71hsrony6wbguw3agap42</td>\n",
              "      <td>0.299232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3xuhv3nrvky7btuzty7gcd0qng1h5p</td>\n",
              "      <td>0.126618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3y5140z9dxgb0yn2jvyfav6metnpii</td>\n",
              "      <td>0.227619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3z4airp3c6d591tvxfnqc9b3zpwx1v</td>\n",
              "      <td>0.227778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3wrfbplxraow7at6ide020z2vvc3nl</td>\n",
              "      <td>0.240000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3634bbtx0ouz9ly85s2ay1sico1ifb</td>\n",
              "      <td>0.268182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>race</td>\n",
              "      <td>3mrnmeiqw56412sizp4x2hhpidqldp</td>\n",
              "      <td>0.083333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>race</td>\n",
              "      <td>37m28k1j0qd08516cu1iw1wrtkcajw</td>\n",
              "      <td>0.155952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>race</td>\n",
              "      <td>3ochawuvgok7f2fh5pt8ho7294rxkc</td>\n",
              "      <td>0.256905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>race</td>\n",
              "      <td>3ioen3p9s7jsqm9zwse0cwyj3kq612</td>\n",
              "      <td>0.268737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>race</td>\n",
              "      <td>3p59jyt76lk5h527b9m7sp02fzkt2o</td>\n",
              "      <td>0.315000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3nd9uoo81k23a8s9gk9nu56app6lw0</td>\n",
              "      <td>0.025974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>354p56de9k3bo6myslycebloosw7sp</td>\n",
              "      <td>0.217372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3jv9lgbjwtefj756e7lx0jogqf0goa</td>\n",
              "      <td>0.227083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3yw4xosqkqldsxz0sac3s2cz64l1u8</td>\n",
              "      <td>0.284848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3ftop5warfo47s3oks4p7vkek7uj0q</td>\n",
              "      <td>0.294048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04f92764-31e4-44dc-a6e5-14820653489e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-04f92764-31e4-44dc-a6e5-14820653489e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-04f92764-31e4-44dc-a6e5-14820653489e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "id": "D7d8fYAq74dz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, SQuAD F1-score of the hardest conversations are all within a range of $[0.02$, $0.3]$ and they are very similar across different sources.\n",
        "\n",
        "Since the baseline and the history version of DistilRoBERTa obtained different SQuAD F1-score on the test set, it might be meaningful to compare their performances on the $5$ worst passages per source extracted for the baseline model."
      ],
      "metadata": {
        "id": "dRlNoiHyWfQn"
      },
      "id": "dRlNoiHyWfQn"
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_h_results = pd.DataFrame(distilroberta_h_results_test)\n",
        "distilroberta_h_results['f1_squad_h'] = distilroberta_h_results.apply(lambda x: squad.compute_f1(x.pred_answer, x.answer), axis=1)\n",
        "\n",
        "worst_5_passages_h = distilroberta_h_results.groupby(['id'], as_index=False).mean().drop(['rationale_end', '__index_level_0__'], axis = 1)\n",
        "df_merge = pd.merge(worst_5_passages, worst_5_passages_h, how = 'left', on = 'id')\n",
        "df_merge"
      ],
      "metadata": {
        "id": "JAE2eOE3MynC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "4a644d1b-5be8-4d4f-b5b8-4182d3ba5520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       source                              id  f1_squad  f1_squad_h\n",
              "0         cnn  3z2r0dq0jhe3smkalexct301crj2ex  0.066667    0.122222\n",
              "1         cnn  3y54sxro1lle1hb9utwdy8vuyvituv  0.250000    0.374339\n",
              "2         cnn  3cfjtt4sxtqmusj2n94ya9f1f0f7iu  0.284330    0.528034\n",
              "3         cnn  3nc5l260mom9579b3nffiyo4px4ofh  0.288810    0.511667\n",
              "4         cnn  35bldd71i6xa08985bv0giyuwn3zv9  0.295000    0.473333\n",
              "5   gutenberg  34z02eimisdylvztwmit917niam0tf  0.092593    0.259019\n",
              "6   gutenberg  3b4yi393v9wx746qgown92hkbf4ssa  0.224091    0.448571\n",
              "7   gutenberg  3azhrg4cu4ktme1zh7c2ro3po9a30y  0.255615    0.562255\n",
              "8   gutenberg  3strjbfxowr0yl6x0fsbslmwvuztks  0.298674    0.300298\n",
              "9   gutenberg  3ihr8nyam71hsrony6wbguw3agap42  0.299232    0.372371\n",
              "10     mctest  3xuhv3nrvky7btuzty7gcd0qng1h5p  0.126618    0.405820\n",
              "11     mctest  3y5140z9dxgb0yn2jvyfav6metnpii  0.227619    0.410107\n",
              "12     mctest  3z4airp3c6d591tvxfnqc9b3zpwx1v  0.227778    0.503761\n",
              "13     mctest  3wrfbplxraow7at6ide020z2vvc3nl  0.240000    0.223333\n",
              "14     mctest  3634bbtx0ouz9ly85s2ay1sico1ifb  0.268182    0.342626\n",
              "15       race  3mrnmeiqw56412sizp4x2hhpidqldp  0.083333    0.278730\n",
              "16       race  37m28k1j0qd08516cu1iw1wrtkcajw  0.155952    0.205952\n",
              "17       race  3ochawuvgok7f2fh5pt8ho7294rxkc  0.256905    0.486905\n",
              "18       race  3ioen3p9s7jsqm9zwse0cwyj3kq612  0.268737    0.403679\n",
              "19       race  3p59jyt76lk5h527b9m7sp02fzkt2o  0.315000    0.296111\n",
              "20  wikipedia  3nd9uoo81k23a8s9gk9nu56app6lw0  0.025974    0.136364\n",
              "21  wikipedia  354p56de9k3bo6myslycebloosw7sp  0.217372    0.251764\n",
              "22  wikipedia  3jv9lgbjwtefj756e7lx0jogqf0goa  0.227083    0.290923\n",
              "23  wikipedia  3yw4xosqkqldsxz0sac3s2cz64l1u8  0.284848    0.462121\n",
              "24  wikipedia  3ftop5warfo47s3oks4p7vkek7uj0q  0.294048    0.351667"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-abad5a03-9acc-423b-a6b4-fb71a0a7542c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>id</th>\n",
              "      <th>f1_squad</th>\n",
              "      <th>f1_squad_h</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3z2r0dq0jhe3smkalexct301crj2ex</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.122222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3y54sxro1lle1hb9utwdy8vuyvituv</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.374339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3cfjtt4sxtqmusj2n94ya9f1f0f7iu</td>\n",
              "      <td>0.284330</td>\n",
              "      <td>0.528034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3nc5l260mom9579b3nffiyo4px4ofh</td>\n",
              "      <td>0.288810</td>\n",
              "      <td>0.511667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cnn</td>\n",
              "      <td>35bldd71i6xa08985bv0giyuwn3zv9</td>\n",
              "      <td>0.295000</td>\n",
              "      <td>0.473333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>34z02eimisdylvztwmit917niam0tf</td>\n",
              "      <td>0.092593</td>\n",
              "      <td>0.259019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3b4yi393v9wx746qgown92hkbf4ssa</td>\n",
              "      <td>0.224091</td>\n",
              "      <td>0.448571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3azhrg4cu4ktme1zh7c2ro3po9a30y</td>\n",
              "      <td>0.255615</td>\n",
              "      <td>0.562255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3strjbfxowr0yl6x0fsbslmwvuztks</td>\n",
              "      <td>0.298674</td>\n",
              "      <td>0.300298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3ihr8nyam71hsrony6wbguw3agap42</td>\n",
              "      <td>0.299232</td>\n",
              "      <td>0.372371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3xuhv3nrvky7btuzty7gcd0qng1h5p</td>\n",
              "      <td>0.126618</td>\n",
              "      <td>0.405820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3y5140z9dxgb0yn2jvyfav6metnpii</td>\n",
              "      <td>0.227619</td>\n",
              "      <td>0.410107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3z4airp3c6d591tvxfnqc9b3zpwx1v</td>\n",
              "      <td>0.227778</td>\n",
              "      <td>0.503761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3wrfbplxraow7at6ide020z2vvc3nl</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.223333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>mctest</td>\n",
              "      <td>3634bbtx0ouz9ly85s2ay1sico1ifb</td>\n",
              "      <td>0.268182</td>\n",
              "      <td>0.342626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>race</td>\n",
              "      <td>3mrnmeiqw56412sizp4x2hhpidqldp</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.278730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>race</td>\n",
              "      <td>37m28k1j0qd08516cu1iw1wrtkcajw</td>\n",
              "      <td>0.155952</td>\n",
              "      <td>0.205952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>race</td>\n",
              "      <td>3ochawuvgok7f2fh5pt8ho7294rxkc</td>\n",
              "      <td>0.256905</td>\n",
              "      <td>0.486905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>race</td>\n",
              "      <td>3ioen3p9s7jsqm9zwse0cwyj3kq612</td>\n",
              "      <td>0.268737</td>\n",
              "      <td>0.403679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>race</td>\n",
              "      <td>3p59jyt76lk5h527b9m7sp02fzkt2o</td>\n",
              "      <td>0.315000</td>\n",
              "      <td>0.296111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3nd9uoo81k23a8s9gk9nu56app6lw0</td>\n",
              "      <td>0.025974</td>\n",
              "      <td>0.136364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>354p56de9k3bo6myslycebloosw7sp</td>\n",
              "      <td>0.217372</td>\n",
              "      <td>0.251764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3jv9lgbjwtefj756e7lx0jogqf0goa</td>\n",
              "      <td>0.227083</td>\n",
              "      <td>0.290923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3yw4xosqkqldsxz0sac3s2cz64l1u8</td>\n",
              "      <td>0.284848</td>\n",
              "      <td>0.462121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3ftop5warfo47s3oks4p7vkek7uj0q</td>\n",
              "      <td>0.294048</td>\n",
              "      <td>0.351667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abad5a03-9acc-423b-a6b4-fb71a0a7542c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-abad5a03-9acc-423b-a6b4-fb71a0a7542c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-abad5a03-9acc-423b-a6b4-fb71a0a7542c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "id": "JAE2eOE3MynC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be clearly noticed that in all the $25$ passages, the history version of DistilRoBERTa performs better. This is probably due to the fact that half of CoQA questions refers back to the conversation history, thus the model which takes into account the dialogue previous turns is more facilitated."
      ],
      "metadata": {
        "id": "C6iqCcjxY45F"
      },
      "id": "C6iqCcjxY45F"
    },
    {
      "cell_type": "code",
      "source": [
        "sources = list(worst_5_passages['source'].unique())\n",
        "sources"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtZg-seK7Ulr",
        "outputId": "1d279758-e495-42f4-bbb5-f75aa9ac17f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cnn', 'gutenberg', 'mctest', 'race', 'wikipedia']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "id": "ZtZg-seK7Ulr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's again visually inspect the worst passages, including both the answers predicted from the baseline and the ones predicted from the history version."
      ],
      "metadata": {
        "id": "gGNJ_urOZtSP"
      },
      "id": "gGNJ_urOZtSP"
    },
    {
      "cell_type": "code",
      "source": [
        "show_worst_5_passages_per_source('cnn', distilroberta_results, history = True, results_h = distilroberta_h_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cex4ywyRVThp",
        "outputId": "4580975d-adca-433b-c604-7b5b149ce3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################  CNN  #####################\n",
            "\n",
            "\n",
            "PASSAGE 1:\n",
            "\n",
            "(CNN) -- There are plenty of country songs about how to be a man. One way is to face the music, and two country crooners did that on Thursday. \n",
            "\n",
            "Ty Herndon and Billy Gilman came out as being gay. \n",
            "\n",
            "Herdon went first, in interviews with People Magazine and Entertainment Tonight. His revelation inspired Gilman to do go public as well. \n",
            "\n",
            "Gilman posted a message to YouTube, thanking Herndon for breaking the ice. He preferred telling his fans directly, from the comfort of home, to sitting down with the press, he said. \n",
            "\n",
            "But an encounter with a journalist also prodded him to spill the beans before someone else did. The reporter bumped into Gilman in a public place and snapped a photo of him -- with his partner. \n",
            "\n",
            "\"It was in that moment that I knew that I'd rather it be from me than you reading it somewhere else,\" Gilman told fans. He also feared being ripped over his sexuality in an article. \n",
            "\n",
            "Country and LGBTQ \n",
            "\n",
            "In the genre of country, Thursday's tune was a tough one to sing, Gilman said. \"Being a gay, male country artist is not the best thing.\" \n",
            "\n",
            "At age 26, he's had a long career, having rocketed up the charts at age 11 with his then silky, pre-voice-change alto pipes, according to his biography on AllMusic.com. \n",
            "\n",
            "But currently, he's hitting snags. \n",
            "\n",
            "Rumors about his sexual orientation have been going around, he said, and he thinks major music labels may have thumbed their noses at him over it. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Who came out?\n",
            "A: Ty Herndon and Billy Gilman\n",
            "R: Ty Herndon and Billy Gilman came out as being gay. \n",
            "PA: Billy Gilman and Billy Gilman\n",
            "PA_H: Ty Herndman and Billy Her\n",
            "\n",
            "Q: What they do?\n",
            "A: they are country singers\n",
            "R: wo country crooners did that on Thursday. \n",
            "PA: listen to Entertainment Tonight\n",
            "PA_H: being gay and gay\n",
            "\n",
            "Q: Who went first about it?\n",
            "A: Herdo\n",
            "R: Herdon went first,\n",
            "PA: Her husband\n",
            "PA_H: Her parents\n",
            "\n",
            "Q: Where?\n",
            "A: in interviews with People Magazine and Entertainment Tonight\n",
            "R: Herdon went first, in interviews with People Magazine and Entertainment Tonight. \n",
            "PA: all over the world\n",
            "PA_H: in interviews and magazines\n",
            "\n",
            "Q: Where he acknowledged that?\n",
            "A: YouTube\n",
            "R: Gilman posted a message to YouTube\n",
            "PA: YouTube.com\n",
            "PA_H: yes\n",
            "\n",
            "Q: Why?\n",
            "A: thanking Herndon\n",
            "R:  thanking Herndon for breaking the ice\n",
            "PA: because he was gay.\n",
            "PA_H: he broke the ice\n",
            "\n",
            "Q: From where he posted that message?\n",
            "A: home\n",
            "R: He preferred telling his fans directly, from the comfort of home, \n",
            "PA: YouTube\n",
            "PA_H: YouTube\n",
            "\n",
            "Q: Were there rumors about their sexual orientation?\n",
            "A: no\n",
            "R: \"It was in that moment that I knew that I'd rather it be from me than you reading it somewhere else,\" Gilman told fans. He also feared being ripped over his sexuality in an article. \n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: Did Gilman meet journalists later?\n",
            "A: no\n",
            "R: \"It was in that moment that I knew that I'd rather it be from me than you reading it somewhere else,\" Gilman told fans. He also feared being ripped over his sexuality in an article. \n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 2:\n",
            "\n",
            "(CNN) -- Smart, successful, funny and handsome. Robin Williams seemed to have it all. And yet, today he is dead. Apparently, by his own choice. \n",
            "\n",
            "But why? What went wrong? \n",
            "\n",
            "The joy, spontaneity and humor of Robin Williams likely masked the daily torment he endured in his on-again-off-again struggle with depression. He made us laugh out here, but he was in pain in there. Unfortunately, I know that pain. \n",
            "\n",
            "In his death, we have lost one of the most talented and creative spirits on the planet. Still, his death by suicide should be a wake-up call for us all. It is to remind us that many of us are walking a fine line -- smiling on the outside while slowly dying on the inside. \n",
            "\n",
            "I didn't know Robin personally, so I am unable to speak with any certainty as to why he chose to end his life. But I have experienced that kind of torment and pain. For years, I, too, struggled with depression. And many days, I still do. \n",
            "\n",
            "Complete coverage on Robin Williams \n",
            "\n",
            "Those who are suffering will do just about anything not to feel the pain anymore. And in those moments, their brains become their worst enemies. It often takes an outside force to provide light, to make sure those dark thoughts aren't, as in the case of Robin, our last thoughts. \n",
            "\n",
            "According to the Centers for Disease Center, suicide rates increased from 2000 to 2011 from 10.4 deaths per 100,000 to 12.3 deaths per 100,000. \n",
            "\n",
            "The rate of suicide is higher and rising among men. In 2011, 78.5% of suicides were by men, at a rate of 20.2 deaths per 100,000. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Is Robin Williams still living?\n",
            "A: no\n",
            "R: And yet, today he is dead\n",
            "PA: no\n",
            "PA_H: No\n",
            "\n",
            "Q: What happened to him?\n",
            "A: suicide\n",
            "R: Apparently, by his own choice. \n",
            "PA: He died\n",
            "PA_H: he died\n",
            "\n",
            "Q: What made him do that?\n",
            "A: depression\n",
            "R:  on-again-off-again struggle with depression. \n",
            "PA: pain in his heart\n",
            "PA_H: he was depressed\n",
            "\n",
            "Q: Did that effect him all the time?\n",
            "A: no\n",
            "R: his on-again-off-again struggle with depression. \n",
            "PA: yes\n",
            "PA_H: no\n",
            "\n",
            "Q: What hid his struggle from other?\n",
            "A: his humor\n",
            "R: The joy, spontaneity and humor of Robin Williams\n",
            "PA: depression of depression.\n",
            "PA_H: joy, spontaneity and spont\n",
            "\n",
            "Q: What did it look like he had?\n",
            "A: Heseemed to have it all.\n",
            "R: Robin Williams seemed to have it all. \n",
            "PA: painful pain\n",
            "PA_H: pain\n",
            "\n",
            "Q: Does suicide occur most often in males or females?\n",
            "A: males\n",
            "R:  78.5% of suicides were by men, at a rate of 20.2 deaths per 100,000.\n",
            "PA: male\n",
            "PA_H: male\n",
            "\n",
            "Q: What percentage were they responsible for in 2011?\n",
            "A: 78.5\n",
            "R: In 2011, 78.5% of suicides were by men, at a rate of 20.2 deaths per 100,000.\n",
            "PA: 78.5%\n",
            "PA_H: 78.5%\n",
            "\n",
            "Q: How many is that for every 100,000?\n",
            "A: 20.2\n",
            "R: ate of 20.2 deaths per 100,000. \n",
            "PA: 12.3\n",
            "PA_H: 20.2 deaths\n",
            "\n",
            "Q: What happened with suicide between 2000 and 2011?\n",
            "A: the rate increased\n",
            "R: uicide rates increased from 2000 to 2011 from 10.4 deaths per 100,000 to 12.3 deaths per 100,000. \n",
            "PA: up to 10.4%\n",
            "PA_H: higher than 10.4 per cent\n",
            "\n",
            "Q: What was it in 2000?\n",
            "A: 10.4 deaths per 100,000\n",
            "R: 0.4 deaths per 100,000 \n",
            "PA: depression rates\n",
            "PA_H: 10.4 per cent\n",
            "\n",
            "Q: And in 2011?\n",
            "A: 12.3 deaths per 100,000.\n",
            "R: 12.3 deaths per 100,000. \n",
            "PA: 78.5%\n",
            "PA_H: 78.5%\n",
            "\n",
            "Q: Was Robin Williams successfu?\n",
            "A: yes\n",
            "R: Smart, successful, funny and handso\n",
            "PA: yes\n",
            "PA_H: no\n",
            "\n",
            "Q: What does the author say about his spirit?\n",
            "A: it was talented and creative\n",
            "R: we have lost one of the most talented and creative spirits o\n",
            "PA: He is lost in one of the\n",
            "PA_H: he lost his talent and lost his\n",
            "\n",
            "Q: What will someone suffering do to get rid of the pain?\n",
            "A: just about anythinh\n",
            "R: Those who are suffering will do just about anything not to feel the pain a\n",
            "PA: to do anything you want to do\n",
            "PA_H: they do not want to do anything\n",
            "\n",
            "Q: What can their brains be?\n",
            "A: their worst enemies\n",
            "R: heir brains become their worst enemies. I\n",
            "PA: their worst enemies\n",
            "PA_H: their worst enemies\n",
            "\n",
            "Q: How can their thoughts turn?\n",
            "A: dark\n",
            "R: make sure those dark thoughts aren't, \n",
            "PA: They don't want to hurt themselves\n",
            "PA_H: they are dark and light\n",
            "\n",
            "Q: What do they need to help?\n",
            "A: an outside force\n",
            "R: t often takes an outside force to provide light,\n",
            "PA: Anxious force\n",
            "PA_H: an outside force\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 3:\n",
            "\n",
            "(CNN) -- A 16-year-old Pennsylvania boy is facing murder charges, accused of fatally shooting a cab driver who he believed was intentionally taking a longer route to his destination. \n",
            "\n",
            "Aazis Richardson is accused of shooting the cab driver, Vincent Darbenzio, 47, twice in the back of his head while still in the cab, according to Lackawanna County Assistant District Attorney Gene Talerico. \n",
            "\n",
            "Richardson believed Darbenzio had ignored his route suggestions in attempt to increase the fare, according to Talerico. \n",
            "\n",
            "\"I just told him, 'take this way;' he didn't want to listen -- he got his (expletive) shot,\" Richardson told CNN affiliate WNEP. \n",
            "\n",
            "Calls and e-mails to the Lackawanna County public defender's office, which is representing Richardson, were not immediately returned. \n",
            "\n",
            "As Richardson was being taken away, he showed no remorse for the driver or his family telling WNEP, \"my homies died, everybody gotta die.\" \n",
            "\n",
            "Richardson was charged as an adult with multiple counts of murder in the first, second and third degrees, as well as robbery and firearms-related offenses, according Talerico. \n",
            "\n",
            "On Sunday, more than 50 of Darbenzio's friends and family members held a vigil near the spot where Darbenzio was killed, according to the victim's brother, Chris Darbenzio. \n",
            "\n",
            "Richardson is currently being held without bail at the Lackawanna County Prison, according to the district attorney's office. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: HOW OLD IS THE PERSPECTIVE?\n",
            "A: 16\n",
            "R: A 16-year-old Pennsylvania boy is facing murder charges, accused of fatally shooting a cab driver who he believed was intentionally taking a longer route to his destination. \n",
            "\n",
            "PA: 47\n",
            "PA_H: 16\n",
            "\n",
            "Q: WHERE IS HE FROM?\n",
            "A: Pennsylvania\n",
            "R: A 16-year-old Pennsylvania boy \n",
            "PA: Pennsylvania\n",
            "PA_H: Pennsylvania\n",
            "\n",
            "Q: WHAT CHARGES IS HE FACING.\n",
            "A: multiple counts of murder in the first, second and third degrees, as well as robbery and firearms-related offenses\n",
            "R: multiple counts of murder in the first, second and third degrees, as well as robbery and firearms-related offenses\n",
            "PA: murder\n",
            "PA_H: murder\n",
            "\n",
            "Q: WHAT IS THE 16 YEAR OLDS NAME?\n",
            "A: Aazis Richardson\n",
            "R: Aazis Richardson is accused of shooting the cab driver, Vincent Darbenzio, 47, twice in the back of his head while still in the cab, according to Lackawanna County Assistant District Attorney Gene Talerico. \n",
            "PA: Aguisis Perez\n",
            "PA_H: A year-old man\n",
            "\n",
            "Q: WHO DID HE SHOOT?\n",
            "A: Vincent Darbenzio\n",
            "R: Aazis Richardson is accused of shooting the cab driver, Vincent Darbenzio\n",
            "PA: a cab driver\n",
            "PA_H: Vincent Darzanio\n",
            "\n",
            "Q: WHAT DID HE DO FOR A LIVING\n",
            "A: cab driver\n",
            "R: Aazis Richardson is accused of shooting the cab driver\n",
            "PA: He was a taxi driver\n",
            "PA_H: He hit the cab driver\n",
            "\n",
            "Q: HOW OLD WAS HE?\n",
            "A: 47\n",
            "R: Vincent Darbenzio, 47\n",
            "PA: 16\n",
            "PA_H: 47\n",
            "\n",
            "Q: WHERE DID HE SHOOT HIM>'\n",
            "A: back of his head\n",
            "R:  twice in the back of his head \n",
            "PA: in the back of his cab\n",
            "PA_H: in the back of his head\n",
            "\n",
            "Q: AND THE LOCATION?\n",
            "A: in the cab\n",
            "R: wice in the back of his head while still in the cab\n",
            "PA: Lackawanna County\n",
            "PA_H: Lackackack County\n",
            "\n",
            "Q: FOR WHAT REASON?\n",
            "A: He ignored his route suggestions.\n",
            "R: Richardson believed Darbenzio had ignored his route suggestions in attempt to increase the fare, according to Talerico. \n",
            "PA: to increase the number of rides on\n",
            "PA_H: attempting to kill the driver\n",
            "\n",
            "Q: WHAT DID HE BELIEVE WAS THE INTENTION?\n",
            "A: To attempt to increase the fare.\n",
            "R: Richardson believed Darbenzio had ignored his route suggestions in attempt to increase the fare, according to Talerico. \n",
            "PA: He didn't take a longer way\n",
            "PA_H: to increase the revenue to the ride\n",
            "\n",
            "Q: WHAT COUNTY IS HOLDING THE CASE?\n",
            "A: Lackawanna County\n",
            "R: Richardson is currently being held without bail at the Lackawanna County Prison, according to the district attorney's office. \n",
            "\n",
            "PA: Lackawanna County\n",
            "PA_H: Lackawanna County\n",
            "\n",
            "Q: DOES THE BOY SHOW REMORSE?\n",
            "A: No\n",
            "R: As Richardson was being taken away, he showed no remorse for the driver\n",
            "PA: no\n",
            "PA_H: No\n",
            "\n",
            "Q: WHAT MAKES YOU SAY THAT?\n",
            "A: He said \"my homies died, everybody gotta die.\"\n",
            "R: As Richardson was being taken away, he showed no remorse for the driver or his family telling WNEP, \"my homies died, everybody gotta die.\" \n",
            "\n",
            "PA: \"I don't want to die\n",
            "PA_H: \"It's his way to die\n",
            "\n",
            "Q: WHO IS TALERICO?\n",
            "A: Lackawanna County Assistant District Attorney\n",
            "R:  Lackawanna County Assistant District Attorney Gene Talerico.\n",
            "PA: Gene Laoyne\n",
            "PA_H: Gene Talbot\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 4:\n",
            "\n",
            "(CNN) -- Three Pakistani cricketers found guilty of corruption pleaded for leniency on Wednesday ahead of their sentencing in a British court. \n",
            "\n",
            "Former national captain Salman Butt and bowlers Mohammad Asif and Mohammad Amir will discover their fate on Thursday, along with agent Mazhar Majeed. \n",
            "\n",
            "Majeed, like Amir, pleaded guilty after being caught accepting money from an undercover newspaper reporter to \"spot-fix\" outcomes during a Test match against England at Lord's in August 2010. \n",
            "\n",
            "Butt and Asif unsuccessfully tried to overturn charges that they both conspired to cheat and accept corrupt funds from Majeed, having been accused of agreeing to deliver three no-ball deliveries at specified times during the match. \n",
            "\n",
            "Former Pakistan captain blames ICC for corruption in cricket \n",
            "\n",
            "Majeed testified that he gave Asif £65,000 ($103,000) and also paid £10,000 ($16,000) to Butt and £2,500 ($4,000) to Amir after receiving £150,000 ($240,000) from a journalist working for the now defunct News of the World paper. \n",
            "\n",
            "The cheating charge carries a maximum sentence of two years plus an unlimited fine, while they could be jailed for seven years with an unlimited fine for accepting corrupt payments. \n",
            "\n",
            "Amir, a rising star in the sport at the age of 19, outlined his remorse in a statement read by his lawyer. \n",
            "\n",
            "\"First I want to apologize to Pakistan and to everyone that cricket is important to,\" he said. \"I do know how much damage this has done to the game, a game which I love more than anything else in the world. \n",
            "\n",
            "\"I did decide many months ago that I wanted to admit that I deliberately threw two no-balls at the Lord's Test last summer. But I know this was very late and I want to apologize for not saying it before. I didn't find the courage to do it at the beginning, and I know very well that made everything much more difficult.\" \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: who was found guilty?\n",
            "A: cricketers\n",
            "R: (CNN) -- Three Pakistani cricketers found guilty of corruption pleaded for leniency on Wednesday ahead of their sentencing in a British court. \n",
            "PA: Sarif Gul\n",
            "PA_H: Two Pakistani cricketeters\n",
            "\n",
            "Q: what did they beg for?\n",
            "A: leniency\n",
            "R: (CNN) -- Three Pakistani cricketers found guilty of corruption pleaded for leniency on Wednesday ahead of their sentencing in a British court. \n",
            "\n",
            "PA: due to leniency\n",
            "PA_H: to plead guilty\n",
            "\n",
            "Q: what day?\n",
            "A: Wednesday\n",
            "R: (CNN) -- Three Pakistani cricketers found guilty of corruption pleaded for leniency on Wednesday ahead of their sentencing in a British court. \n",
            "PA: Thursday\n",
            "PA_H: Wednesday\n",
            "\n",
            "Q: where were they sentenced?\n",
            "A: in a British court\n",
            "R: (CNN) -- Three Pakistani cricketers found guilty of corruption pleaded for leniency on Wednesday ahead of their sentencing in a British court. \n",
            "PA: in a British court\n",
            "PA_H: a British court\n",
            "\n",
            "Q: who will discover their fate?\n",
            "A: the cricketers\n",
            "R: (CNN) -- Three Pakistani cricketers found guilty of corruption pleaded for leniency on Wednesday ahead of their sentencing in a British court. \n",
            "\n",
            "PA: Mohammad Asif and Mohammad As\n",
            "PA_H: Mammar Masif and Mohammad\n",
            "\n",
            "Q: name one\n",
            "A: Salman Butt\n",
            "R: Former national captain Salman Butt and bowlers Mohammad Asif and Mohammad Amir will discover their fate on Thursday, along with agent Mazhar Majeed. \n",
            "\n",
            "PA: Maharif Khan\n",
            "PA_H: Mohammad Asif and Mohammad Mohammad\n",
            "\n",
            "Q: what is his position\n",
            "A: Former national captain\n",
            "R: Former national captain Salman Butt and bowlers Mohammad Asif and Mohammad Amir will discover their fate on Thursday, along with agent Mazhar Majeed. \n",
            "PA: former captain of Pakistan\n",
            "PA_H: former national captain\n",
            "\n",
            "Q: are there any others that are going to get sentenced?\n",
            "A: yes\n",
            "R: Former national captain Salman Butt and bowlers Mohammad Asif and Mohammad Amir will discover their fate on Thursday, along with agent Mazhar Majeed. \n",
            "PA: Yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: who?\n",
            "A: Mohammad Asif and Mohammad Amir\n",
            "R: Former national captain Salman Butt and bowlers Mohammad Asif and Mohammad Amir will discover their fate on Thursday, along with agent Mazhar Majeed. \n",
            "\n",
            "PA: Mohammad Asif\n",
            "PA_H: Mohammad Asif and Mohammad As\n",
            "\n",
            "Q: what is there positions?\n",
            "A: bowlers\n",
            "R: Former national captain Salman Butt and bowlers Mohammad Asif and Mohammad Amir will discover their fate on Thursday, along with agent Mazhar Majeed. \n",
            "PA: National cricketers\n",
            "PA_H: bowlers\n",
            "\n",
            "Q: any others?\n",
            "A: yes\n",
            "R: Former national captain Salman Butt and bowlers Mohammad Asif and Mohammad Amir will discover their fate on Thursday, along with agent Mazhar Majeed. \n",
            "\n",
            "PA: Muhammad Khan and Mohammad Khan\n",
            "PA_H: yes\n",
            "\n",
            "Q: who?\n",
            "A: Mazhar Majeed.\n",
            "R: Former national captain Salman Butt and bowlers Mohammad Asif and Mohammad Amir will discover their fate on Thursday, along with agent Mazhar Majeed. \n",
            "\n",
            "PA: Mohammad Asif\n",
            "PA_H: Mazhar Sadeed\n",
            "\n",
            "Q: what is his postion?\n",
            "A: agent\n",
            "R: Former national captain Salman Butt and bowlers Mohammad Asif and Mohammad Amir will discover their fate on Thursday, along with agent Mazhar Majeed. \n",
            "\n",
            "PA: a Pakistani cricket captain\n",
            "PA_H: an agent\n",
            "\n",
            "Q: what day are they being sentenced?\n",
            "A: on Thursday\n",
            "R: Former national captain Salman Butt and bowlers Mohammad Asif and Mohammad Amir will discover their fate on Thursday, along with agent Mazhar Majeed. \n",
            "PA: Wednesday\n",
            "PA_H: Wednesday\n",
            "\n",
            "Q: how did they plead?\n",
            "A: guilty\n",
            "R: Majeed, like Amir, pleaded guilty after being caught accepting money from an undercover newspaper reporter to \"spot-fix\" outcomes during a Test match against England at Lord's in August 2010\n",
            "PA: they were cooperating with prosecutors\n",
            "PA_H: they were caught cooperating with an extortion\n",
            "\n",
            "Q: what crime did they commit?\n",
            "A: \"spot-fixing\" outcomes\n",
            "R: Majeed, like Amir, pleaded guilty after being caught accepting money from an undercover newspaper reporter to \"spot-fix\" outcomes during a Test match against England at Lord's in August 2010. \n",
            "PA: corruption\n",
            "PA_H: corruption\n",
            "\n",
            "Q: against who?\n",
            "A: against England\n",
            "R: Majeed, like Amir, pleaded guilty after being caught accepting money from an undercover newspaper reporter to \"spot-fix\" outcomes during a Test match against England at Lord's in August 2010. \n",
            "\n",
            "PA: England\n",
            "PA_H: England\n",
            "\n",
            "Q: when?\n",
            "A: in August 2010.\n",
            "R: during a Test match against England at Lord's in August 2010. \n",
            "\n",
            "PA: August 2010\n",
            "PA_H: August 2010\n",
            "\n",
            "Q: where charges overturned?\n",
            "A: no\n",
            "R: Butt and Asif unsuccessfully tried to overturn charges \n",
            "PA: Yes\n",
            "PA_H: at Lord Asif\n",
            "\n",
            "Q: who tried to overturn charges?\n",
            "A: Butt and Asif\n",
            "R: Butt and Asif unsuccessfully tried to overturn charges\n",
            "PA: Tasif and Asif\n",
            "PA_H: Andas and Asif\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 5:\n",
            "\n",
            "NEW YORK (CNN) -- Natasha Richardson, a film star, Tony-winning stage actress and member of the famed Redgrave acting family, died Wednesday after suffering injuries in a ski accident, according to a family statement. She was 45. \n",
            "\n",
            "Natasha Richardson fell on a beginners' slope in Canada. \n",
            "\n",
            "Richardson, wife of actor Liam Neeson, was injured Monday in a fall on a ski slope at a Quebec resort about 80 miles northwest of Montreal. \n",
            "\n",
            "Richardson's family released a statement saying, \"Liam Neeson, his sons, and the entire family are shocked and devastated by the tragic death of their beloved Natasha. They are profoundly grateful for the support, love and prayers of everyone, and ask for privacy during this very difficult time.\" \n",
            "\n",
            "According to a statement from Mont Tremblant Ski Resort, Richardson fell during a lesson on a beginners' trail. Watch a report on Richardson's life » \n",
            "\n",
            "\"She did not show any visible sign of injury, but the ski patrol followed strict procedures and brought her back to the bottom of the slope and insisted she should see a doctor,\" the statement said. \n",
            "\n",
            "Richardson, accompanied by her instructor, returned to her hotel, but about an hour after the fall was \"not feeling good,\" the statement said. An ambulance was called, and Richardson was taken to a local hospital before being transferred to Hopital du Sacre-Coeur in Montreal. From there she was transferred to Lenox Hill Hospital in New York City. \n",
            "\n",
            "Friends and colleagues were saddened by her death. \n",
            "\n",
            "\"Natasha was brilliant, beautiful, funny, talented beyond measure, as emotionally raw as she was razor sharp,\" said Jodie Foster, who worked with Richardson in \"Nell,\" in a statement. \"Tasha loved fiercely and that love continues in all of us who knew her. May Liam, her beautiful boys and her loving family hold her close as they move through this tragic moment.\" \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: How did Natasha die?\n",
            "A: She  fell on a beginners' slope\n",
            "R: Natasha Richardson fell on a beginners' slope in Canada. \n",
            "PA: in a skiing accident\n",
            "PA_H: she fell in a skiing accident\n",
            "\n",
            "Q: Doing what sport?\n",
            "A: Skiing.\n",
            "R: According to a statement from Mont Tremblant Ski Resort, Richardson fell during a lesson on a beginners' trail. \n",
            "PA: a stage actor\n",
            "PA_H: a ski accident\n",
            "\n",
            "Q: Which country was she visiting?\n",
            "A: Canada.\n",
            "R:  was injured Monday in a fall on a ski slope at a Quebec resort\n",
            "PA: Canada\n",
            "PA_H: Canada.\n",
            "\n",
            "Q: Did the resort follow procedure?\n",
            "A: Yes.\n",
            "R: She did not show any visible sign of injury, but the ski patrol followed strict procedures\n",
            "PA: no\n",
            "PA_H: Yes.\n",
            "\n",
            "Q: Did she follow their instructions?\n",
            "A: She did not.\n",
            "R: Richardson, accompanied by her instructor, returned to her hotel, but about an hour after the fall was \"not feeling good,\"\n",
            "PA: Yes\n",
            "PA_H: Yes.\n",
            "\n",
            "Q: How long did it take to develop symptoms?\n",
            "A: About an hour.\n",
            "R: but about an hour after the fall was \"not feeling good,\n",
            "PA: an hour after\n",
            "PA_H: About an hour.\n",
            "\n",
            "Q: Previous to that, did she appear severely injured?\n",
            "A: She did not show signs.\n",
            "R: \"She did not show any visible sign of injury,\n",
            "PA: no\n",
            "PA_H: No.\n",
            "\n",
            "Q: Which hospital did she visit first?\n",
            "A: A local hospital\n",
            "R: An ambulance was called, and Richardson was taken to a local hospital\n",
            "PA: Lenox Hill Hospital\n",
            "PA_H: Lenoxox Hill Hospital.\n",
            "\n",
            "Q: Then where?\n",
            "A: Hopital du Sacre-Coeur\n",
            "R:  before being transferred to Hopital du Sacre-Coeur \n",
            "PA: in a Quebec resort\n",
            "PA_H: Luxox-Sacramento\n",
            "\n",
            "Q: Where was her final medical center?\n",
            "A: New York City.\n",
            "R: From there she was transferred to Lenox Hill Hospital in New York City. \n",
            "PA: Lenox Hill Hospital\n",
            "PA_H: Munox Hill Hospital\n",
            "\n",
            "Q: How old was she?\n",
            "A: She was 45\n",
            "R: She was 45\n",
            "PA: 45\n",
            "PA_H: 45.\n",
            "\n",
            "Q: What was her profession?\n",
            "A: A film star\n",
            "R: Natasha Richardson, a film star\n",
            "PA: comedian\n",
            "PA_H: A film actress.\n",
            "\n",
            "Q: Did her husband share the same profession?\n",
            "A: Yes.\n",
            "R: Richardson, wife of actor Liam Neeson\n",
            "PA: no\n",
            "PA_H: No\n",
            "\n",
            "Q: What was his name?\n",
            "A: Liam Neeson\n",
            "R: wife of actor Liam Neeson\n",
            "PA: Liam Nelson\n",
            "PA_H: Liam Neeson\n",
            "\n",
            "Q: Did they have children?\n",
            "A: Yes.\n",
            "R: Richardson's family released a statement saying, \"Liam Neeson, his sons, and the entire family\n",
            "PA: yes\n",
            "PA_H: Yes.\n",
            "\n",
            "Q: Boys or girls?\n",
            "A: Sons\n",
            "R: his sons\n",
            "PA: Boys\n",
            "PA_H: Girls.\n",
            "\n",
            "Q: Had she won any acting awards?\n",
            "A: Yes.\n",
            "R: Tony-winning stage actress\n",
            "PA: yes\n",
            "PA_H: Yes.\n",
            "\n",
            "Q: Which?\n",
            "A: Tony\n",
            "R: Tony-winning stage\n",
            "PA: a ski hill\n",
            "PA_H: Tony Awards.\n",
            "\n",
            "Q: Was her family well known?\n",
            "A: Yes.\n",
            "R: Redgrave acting family\n",
            "PA: yes\n",
            "PA_H: Yes.\n",
            "\n",
            "Q: For what?\n",
            "A: Acting.\n",
            "R:  famed Redgrave acting family, \n",
            "PA: a skiing accident\n",
            "PA_H: The Redgrave family.\n",
            "\n"
          ]
        }
      ],
      "id": "Cex4ywyRVThp"
    },
    {
      "cell_type": "code",
      "source": [
        "show_worst_5_passages_per_source('gutenberg', distilroberta_results, history = True, results_h = distilroberta_h_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca0005c-cb52-4f8b-d948-9d9ae81a78f3",
        "id": "enKWyZR674dz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################  GUTENBERG  #####################\n",
            "\n",
            "\n",
            "PASSAGE 1:\n",
            "\n",
            "CHAPTER XXVI \n",
            "\n",
            "THE JUDGMENT OF DOMITIAN \n",
            "\n",
            "Two hours had gone by and Caleb, with fury in his heart, sat brooding in the office attached to the warehouse that he had hired. At that moment he had but one desire--to kill his successful rival, Marcus. Marcus had escaped and returned to Rome; of that there could be no doubt. He, one of the wealthiest of its patricians, had furnished the vast sum which enabled old Nehushta to buy the coveted Pearl-Maiden in the slave-ring. Then his newly acquired property had been taken to this house, where he awaited her. This then was the end of their long rivalry; for this he, Caleb, had fought, toiled, schemed and suffered. Oh! rather than such a thing should be, in that dark hour of his soul, he would have seen her cast to the foul Domitian, for Domitian, at least, she would have hated, whereas Marcus, he knew, she loved. \n",
            "\n",
            "Now there remained nothing but revenge. Revenged he must be, but how? He might dog Marcus and murder him, only then his own life would be hazarded, since he knew well the fate that awaited the foreigner, and most of all the Jew, who dared to lift his hand against a Roman noble, and if he hired others to do the work they might bear evidence against him. Now Caleb did not wish to die; life seemed the only good that he had left. Also, while he lived he might still win Miriam--after his rival had ceased to live. Doubtless, then she would be sold with his other slaves, and he could buy her at the rate such tarnished goods command. No, he would do nothing to run himself into danger. He would wait, wait and watch his opportunity. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What was now left?\n",
            "A: revenge\n",
            "R: Now there remained nothing but revenge\n",
            "PA: his life\n",
            "PA_H: Death\n",
            "\n",
            "Q: Against who?\n",
            "A: Marcus\n",
            "R: He might dog Marcus\n",
            "PA: a Roman noble\n",
            "PA_H: Marcus\n",
            "\n",
            "Q: What was in the angered man's heart?\n",
            "A: fury\n",
            "R: with fury in his heart\n",
            "PA: fear\n",
            "PA_H: frightened\n",
            "\n",
            "Q: What was one option to get back at the man?\n",
            "A: dog and murder him\n",
            "R: He might dog Marcus and murder him\n",
            "PA: he was murdered.\n",
            "PA_H: to kill him\n",
            "\n",
            "Q: What was a drawback to that plan?\n",
            "A: his own life would be hazarded\n",
            "R: only then his own life would be hazarded\n",
            "PA: he would have lost his fortune\n",
            "PA_H: his fate would be sealed\n",
            "\n",
            "Q: What was the subject of their rivalry?\n",
            "A: the coveted Pearl-Maiden\n",
            "R: the coveted Pearl-Maiden\n",
            "PA: what he had done\n",
            "PA_H: when he was born\n",
            "\n",
            "Q: What was the maiden's occupation?\n",
            "A: a slave\n",
            "R: the coveted Pearl-Maiden in the slave-ring\n",
            "PA: she was a slave\n",
            "PA_H: she was a slave\n",
            "\n",
            "Q: And her name?\n",
            "A: Miriam\n",
            "R: he might still win Miriam\n",
            "PA: Miritian\n",
            "PA_H: Domestic\n",
            "\n",
            "Q: What might he be able to do if his rival were dead?\n",
            "A: win Miriam\n",
            "R: he might still win Miriam--after his rival had ceased to live\n",
            "PA: to bear evidence\n",
            "PA_H: to buy her\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 2:\n",
            "\n",
            "CHAPTER NINETEEN. \n",
            "\n",
            "A SHOOTING TRIP TO PARADISE ISLE, AND FURTHER DISPLAY OF THE CAPTAIN'S CONTRIVANCES. \n",
            "\n",
            "While our explorers were thus reduced to a state of forced inaction as regarded the main object of their expedition, they did not by any means waste their time in idleness. On the contrary, each of the party went zealously to work in the way that was most suitable to his inclination. \n",
            "\n",
            "After going over the main island of Poloe as a united party, and ascertaining its size, productions, and general features, the Captain told them they might now do as they pleased. For his part he meant to spend a good deal of his time in taking notes and observations, questioning the chief men as to the lands lying to the northward, repairing and improving the hut, and helping the natives miscellaneously so as to gain their regard. \n",
            "\n",
            "Of course Leo spent much of his time with his rifle, for the natives were not such expert hunters but that occasionally they were badly off for food. Of course, also, Alf shouldered his botanical box and sallied forth hammer in hand, to \"break stones,\" as Butterface put it. Benjy sometimes followed Alf--more frequently Leo, and always carried his father's double-barrelled shot-gun. He preferred that, because his powers with the rifle were not yet developed. Sometimes he went with Toolooha, or Tekkona, or Oblooria, in one of the native oomiaks to fish. At other times he practised paddling in the native kayak, so that he might accompany Chingatok on his excursions to the neighbouring islands after seals and wild-fowl. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Which chapter is this?\n",
            "A: NINETEEN\n",
            "R: NINETEEN\n",
            "PA: CHAPTER NINEENEEN\n",
            "PA_H: CHAPTER NTENEENENEN\n",
            "\n",
            "Q: Who is first mentioned?\n",
            "A: the explorers\n",
            "R: While our explorers were thus reduced\n",
            "PA: Leo\n",
            "PA_H: A SHOT TROTLE\n",
            "\n",
            "Q: Are they able to be active?\n",
            "A: no\n",
            "R: While our explorers were thus reduced to a state of forced inaction \n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: With regard to what?\n",
            "A: the main object of their expedition\n",
            "R: While our explorers were thus reduced to a state of forced inaction as regarded the main object of their expedition\n",
            "PA: his lands and waters\n",
            "PA_H: a forced action\n",
            "\n",
            "Q: Were they lazy?\n",
            "A: no\n",
            "R: On the contrary, each of the party went zealously to work in the way that was most suitable to his inclination. \n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: What was gone over?\n",
            "A: Poloe\n",
            "R: After going over the main island of Poloe\n",
            "PA: the main island of Polo\n",
            "PA_H: the main island of Pelo\n",
            "\n",
            "Q: What is that?\n",
            "A: an island\n",
            "R: After going over the main island of Poloe \n",
            "PA: a botanical box\n",
            "PA_H: a united party\n",
            "\n",
            "Q: An insignificant one?\n",
            "A: the main one\n",
            "R: After going over the main island of Poloe \n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: What is discussed?\n",
            "A: that they could do as they pleased\n",
            "R: the Captain told them they might now do as they pleased.\n",
            "PA: A map of the colonies\n",
            "PA_H: a union\n",
            "\n",
            "Q: What aspects of it are compared?\n",
            "A: its size, productions, and general features\n",
            "R: After going over the main island of Poloe as a united party, and ascertaining its size, productions, and general features, \n",
            "PA: small production, production and production,\n",
            "PA_H: size, production, and production\n",
            "\n",
            "Q: Did they do this together?\n",
            "A: yes\n",
            "R: After going over the main island of Poloe as a united party, \n",
            "PA: no\n",
            "PA_H: yes\n",
            "\n",
            "Q: Who speaks to them?\n",
            "A: the Captain\n",
            "R: After going over the main island of Poloe as a united party, and ascertaining its size, productions, and general features, the Captain told them they might now do as they pleased.\n",
            "PA: Captain the Captain\n",
            "PA_H: the Captain\n",
            "\n",
            "Q: Were the native people good at hunting?\n",
            "A: no\n",
            "R: \n",
            "Of course Leo spent much of his time with his rifle, for the natives were not such expert hunters\n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: Who helps them?\n",
            "A: Leo\n",
            "R: Of course Leo spent much of his time with his rifle, for the natives were not such expert hunters but that occasionally they were badly off for food. \n",
            "PA: the natives\n",
            "PA_H: Leo\n",
            "\n",
            "Q: Using what?\n",
            "A: his rifle\n",
            "R: \n",
            "Of course Leo spent much of his time with his rifle,\n",
            "PA: his musket\n",
            "PA_H: his rifle\n",
            "\n",
            "Q: What does someone else use?\n",
            "A: a double-barrelled shot-gun\n",
            "R:  Benjy sometimes followed Alf--more frequently Leo, and always carried his father's double-barrelled shot-gun.\n",
            "PA: his botanical box\n",
            "PA_H: his botanical box\n",
            "\n",
            "Q: Who's is it?\n",
            "A: his father's\n",
            "R: Benjy sometimes followed Alf--more frequently Leo, and always carried his father's double-barrelled shot-gun. \n",
            "PA: Aristotle\n",
            "PA_H: Benjy\n",
            "\n",
            "Q: Why does he use it?\n",
            "A: because his powers with the rifle were not yet developed\n",
            "R: He preferred that, because his powers with the rifle were not yet developed.\n",
            "PA: to break stones from his stones\n",
            "PA_H: because his powers were limited\n",
            "\n",
            "Q: What is one tribe’s name he goes with?\n",
            "A: Toolooha\n",
            "R: Sometimes he went with Toolooha,\n",
            "PA: Tokoi\n",
            "PA_H: Toolo\n",
            "\n",
            "Q: And another?\n",
            "A: Tekkona\n",
            "R: Sometimes he went with Toolooha, or Tekkona, \n",
            "PA: Okekona\n",
            "PA_H: Tekkona\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 3:\n",
            "\n",
            "CHAPTER XXVII \n",
            "\n",
            "WHAT ELSA SAW IN THE MOONLIGHT \n",
            "\n",
            "It will be remembered that some weeks before Elsa's forced marriage in the Red Mill, Foy, on their escape from the Gevangenhuis, had been carried upon the naked back of Martin to the shelter of Mother Martha's lair in the Haarlemer Meer. Here he lay sick many days, for the sword cut in his thigh festered so badly that at one time his life was threatened by gangrene, but, in the end, his own strength and healthy constitution, helped with Martha's simples, cured him. So soon as he was strong again, accompanied by Martin, he travelled into Leyden, which now it was safe enough for him to visit, since the Spaniards were driven from the town. \n",
            "\n",
            "How his young heart swelled as, still limping a little and somewhat pale from recent illness, he approached the well-known house in the Bree Straat, the home that sheltered his mother and his love. Presently he would see them again, for the news had been brought to him that Lysbeth was out of danger and Elsa must still be nursing her. \n",
            "\n",
            "Lysbeth he found indeed, turned into an old woman by grief and sore sickness, but Elsa he did not find. She had vanished. On the previous night she had gone out to take the air, and returned no more. What had become of her none could say. All the town talked of it, and his mother was half-crazed with anxiety and fear, fear of the worst. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: who was carried?\n",
            "A: Foy\n",
            "R: Foy, on their escape from the Gevangenhuis, had been carried\n",
            "PA: Foy\n",
            "PA_H: Fredoy\n",
            "\n",
            "Q: on what?\n",
            "A: the naked back of Martin\n",
            "R:  carried upon the naked back of Martin\n",
            "PA: their escape from the house\n",
            "PA_H: their escape\n",
            "\n",
            "Q: where to?\n",
            "A: the shelter of Mother Martha's lair\n",
            "R: back of Martin to the shelter of Mother Martha's lair \n",
            "PA: Learlemer\n",
            "PA_H: the shelter of Mother Martha's Mar\n",
            "\n",
            "Q: what event was this prior to?\n",
            "A: Elsa's forced marriage\n",
            "R: some weeks before Elsa's forced marriage\n",
            "PA: the death of his wife\n",
            "PA_H: Elsa's marriage marriage\n",
            "\n",
            "Q: where?\n",
            "A: the Red Mill\n",
            "R:  forced marriage in the Red Mill,\n",
            "PA: in the barn\n",
            "PA_H: in the Red Mill Merme\n",
            "\n",
            "Q: how long before?\n",
            "A: some weeks\n",
            "R:  that some weeks before\n",
            "PA: some weeks before\n",
            "PA_H: some weeks before\n",
            "\n",
            "Q: what were they esaping?\n",
            "A: the Gevangenhuis\n",
            "R: on their escape from the Gevangenhuis\n",
            "PA: a sword\n",
            "PA_H: the sword of Gevanghu\n",
            "\n",
            "Q: where was the lair?\n",
            "A: the Haarlemer Meer.\n",
            "R: lair in the Haarlemer Meer.\n",
            "PA: Haarlemerer\n",
            "PA_H: the Haarlemer Meer\n",
            "\n",
            "Q: who was ill?\n",
            "A: Foy\n",
            "R: Here he lay sick many days,\n",
            "PA: Lysbeth\n",
            "PA_H: Alfred\n",
            "\n",
            "Q: for how long?\n",
            "A: many days\n",
            "R: Here he lay sick many days\n",
            "PA: Some weeks\n",
            "PA_H: many days\n",
            "\n",
            "Q: why?\n",
            "A: the sword cut in his thigh\n",
            "R: for the sword cut in his thigh\n",
            "PA: because he was afraid of the danger\n",
            "PA_H: the sword cut off his thigh\n",
            "\n",
            "Q: was it serious?\n",
            "A: yes\n",
            "R:  so badly that at one time his life was threatened by gangrene\n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: how bad?\n",
            "A: his life was threatened by gangrene\n",
            "R: so badly that at one time his life was threatened by gangrene\n",
            "PA: he was severely severely wounded\n",
            "PA_H: he was severely crippled by gangrene\n",
            "\n",
            "Q: did he get better?\n",
            "A: yes\n",
            "R: cured him\n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: how?\n",
            "A: his own strength and healthy constitution, helped with Martha's simples\n",
            "R:  his own strength and healthy constitution, helped with Martha's simples\n",
            "PA: by the sword\n",
            "PA_H: he had Martha and Sam's health\n",
            "\n",
            "Q: where was he travelling to?\n",
            "A: Leyden\n",
            "R: he travelled into Leyden\n",
            "PA: Leiden\n",
            "PA_H: Leyden\n",
            "\n",
            "Q: why was it now safe?\n",
            "A: the Spaniards were driven from the town\n",
            "R:  safe enough for him to visit, since the Spaniards were driven from the town. \n",
            "PA: because he had been driven from the\n",
            "PA_H: because he was visiting the Spaniards\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 4:\n",
            "\n",
            "CHAPTER XVII: MACHINATIONS \n",
            "\n",
            "\"Baby born to woe.\" \n",
            "\n",
            "F. T. PALGRAVE. \n",
            "\n",
            "When Anne Woodford began to wake from the constant thought of the grief and horror she had left at Portchester, and to feel more alive to her surroundings and less as if they were a kind of dream, in which she only mechanically took her part, one thing impressed itself on her gradually, and that was disappointment. If the previous shock had not blunted all her hopes and aspirations, perhaps she would have felt it sooner and more keenly; but she could not help realising that she had put herself into an inferior position whence there did not seem to be the promotion she had once anticipated. Her companion rockers were of an inferior grade to herself. Jane Humphreys was a harmless but silly girl, not much wiser, though less spoilt, than poor little Madam, and full of Cockney vulgarities. Education was unfashionable just then, and though Hester Bridgeman was bettor born and bred, being the daughter of an attorney in the city, she was not much better instructed, and had no pursuits except that of her own advantage. Pauline Dunord was by far the best of the three, but she seemed to live a life apart, taking very little interest in her companions or anything around her except her devotions and the bringing them over to her Church. The nursery was quite a separate establishment; there was no mingling with the guests of royalty, who were only seen in excited peeps from the window, or when solemnly introduced to the presence chamber to pay their respects to the Prince. As to books, the only secular one that Anne saw while at Whitehall was an odd volume of Parthenissa. The late King's summary of the Roman controversy was to be had in plenty, and nothing was more evident than that the only road to favour or promotion was in being thereby convinced. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: who was Anne's companion\n",
            "A: rockers\n",
            "R: Her companion rockers \n",
            "PA: Jane Humphreys\n",
            "PA_H: rockers\n",
            "\n",
            "Q: what was the only secular book she saw while at whitehall?\n",
            "A: an odd volume of Parthenissa\n",
            "R: he only secular one that Anne saw while at Whitehall was an odd volume of Parthenissa\n",
            "PA: Parthenissa\n",
            "PA_H: Part of Parthenissa\n",
            "\n",
            "Q: what feeling impressed upon Anne?\n",
            "A: she had put herself into an inferior position\n",
            "R: but she could not help realising that she had put herself into an inferior position\n",
            "PA: satisfaction\n",
            "PA_H: satisfaction\n",
            "\n",
            "Q: what place had she left?\n",
            "A: t Portchester\n",
            "R: she had left at Portchester\n",
            "PA: Portchester\n",
            "PA_H: Portchester\n",
            "\n",
            "Q: did she think her companion was superior?\n",
            "A: no\n",
            "R: Her companion rockers were of an inferior grade to herself. \n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: what did she leave at Portchester?\n",
            "A: the constant thought of the grief and horror\n",
            "R: the constant thought of the grief and horror she had left at Portchester\n",
            "PA: the grief and horror and horror\n",
            "PA_H: the grief and horror\n",
            "\n",
            "Q: was the nursery connected or separate?\n",
            "A: separate\n",
            "R: The nursery was quite a separate establishment\n",
            "PA: yes\n",
            "PA_H: separated\n",
            "\n",
            "Q: what was Pauline devoted to?\n",
            "A: her Church\n",
            "R:  her Church\n",
            "PA: bringing her to the church\n",
            "PA_H: the whole of the Church\n",
            "\n",
            "Q: who used a lot of Cockney vulgarities?\n",
            "A: Jane Humphreys\n",
            "R:  Jane Humphreys\n",
            "PA: Jane Humphreys\n",
            "PA_H: Jane Humphreys\n",
            "\n",
            "Q: was she threatening?\n",
            "A: no\n",
            "R:  Jane Humphreys was a harmless but silly girl\n",
            "PA: no\n",
            "PA_H: yes\n",
            "\n",
            "Q: was she serious or silly?\n",
            "A: silly\n",
            "R:  Jane Humphreys was a harmless but silly girl,\n",
            "PA: serious\n",
            "PA_H: serious\n",
            "\n",
            "Q: how many rockers were there in total?\n",
            "A: four\n",
            "R:  Pauline Dunord was by far the best of the three,\n",
            "PA: her companion\n",
            "PA_H: three\n",
            "\n",
            "Q: was Heter Bridgeman one of them?\n",
            "A: .yes\n",
            "R: . Education was unfashionable just then, and though Hester Bridgeman was bettor born and bred\n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: who was she the daughter of?\n",
            "A: an attorney in the city,\n",
            "R: , being the daughter of an attorney in the city,\n",
            "PA: an attorney\n",
            "PA_H: an attorney\n",
            "\n",
            "Q: did she have any pursuits?\n",
            "A: only ones that were to her advantage\n",
            "R:  and had no pursuits except that of her own advantage.\n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: what is the name of chapter 17?\n",
            "A: MACHINATIONS\n",
            "R: MACHINATIONS \n",
            "PA: MACHIN VALVEVEVE\n",
            "PA_H: M. M.INATIONS\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 5:\n",
            "\n",
            "CHAPTER V. \n",
            "\n",
            "MESSRS. TREAT, JONES, WESTON & DOWD. \n",
            "\n",
            "The particular circle of society in which Ben and Johnny moved was shaken to its very centre by the news which was whispered from one to the other on the day after those young gentlemen and Paul had taken up their abode at Mrs. Green's. \n",
            "\n",
            "Early that morning the most exciting topic of conversation had been Master Spry's misfortune and Tim Dooley's perfidy; and that had hardly begun to be commented upon when the news spread that Ben and Johnny, since the coming of their guest, who was evidently a suspicious sort of a person, as was shown by his clothes and his entire ignorance of the slang of the street, were no longer proud of their neat little bit of real estate, but had made a change which would probably be the means of their financial ruin. That they had been so extravagant as to engage rooms at a regular boarding-house, where they were to spend their substance on three square meals each day, seemed like a reckless disregard of money; and the price which they were to pay for board was stated at various sums from five to ten dollars per week. But that was not the only bit of wonderful news. \n",
            "\n",
            "Jimmy Sullivan stated--and he was supported by several others as the time wore on--that Johnny himself had told him that they were to start a regular theatre, and had already engaged a hall, which would be converted into a first-class place of amusement as soon as possible. This would have been regarded simply as a rumor started for the purpose of injuring the credit of these young gentlemen, had it not come so directly from one of the parties concerned, and must therefore be true. \n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Who started to live at Mrs Green's?\n",
            "A: Ben, Johnny, and Paul\n",
            "R:  those young gentlemen and Paul had taken up their abode at Mrs. Green's.\n",
            "PA: Ben and Paul\n",
            "PA_H: Ben and Paul\n",
            "\n",
            "Q: How could you tell Paul was a person to be suspected?\n",
            "A: his clothes and his ignorance of the slang\n",
            "R: vidently a suspicious sort of a person, as was shown by his clothes and his entire ignorance of the slang\n",
            "PA: his clothes and shoes\n",
            "PA_H: his clothes\n",
            "\n",
            "Q: What kind of lingo was it?\n",
            "A: slang of the street\n",
            "R: slang of the street\n",
            "PA: the real real estate\n",
            "PA_H: street\n",
            "\n",
            "Q: Name one of the things that was rumored to lead to their money troubles?\n",
            "A: they had engaged rooms at a regular boarding-house\n",
            "R: That they had been so extravagant as to engage rooms at a regular boarding-house\n",
            "PA: the purpose of the use of the\n",
            "PA_H: Master Spiture and Spry's\n",
            "\n",
            "Q: Anot?her\n",
            "A: they were to pay for three square meals each day\n",
            "R: they were to spend their substance on three square meals each day\n",
            "PA: no\n",
            "PA_H: the price of the bill\n",
            "\n",
            "Q: What was the low estimate of the money they were spending?\n",
            "A: five dollars per week\n",
            "R: he price which they were to pay for board was stated at various sums from five to ten dollars\n",
            "PA: five to ten dollars per day\n",
            "PA_H: five to ten dollars per week\n",
            "\n",
            "Q: And the high estimate?\n",
            "A: ten dollars per week\n",
            "R: ten dollars per week\n",
            "PA: five to ten dollars\n",
            "PA_H: five to ten dollars\n",
            "\n",
            "Q: What had people been gossiping about before they started on Ben and Johnny?\n",
            "A: Master Spry's misfortune and Tim Dooley's perfidy\n",
            "R: Early that morning the most exciting topic of conversation had been Master Spry's misfortune and Tim Dooley's perfidy\n",
            "PA: Master Sparre's Spaniel's\n",
            "PA_H: Master Spry's misfortune and Sp\n",
            "\n",
            "Q: So were people having fun with this gossip?\n",
            "A: yes\n",
            "R: the most exciting topic\n",
            "PA: yes\n",
            "PA_H: no\n",
            "\n",
            "Q: What did they think of it?\n",
            "A: That it was wonderful news.\n",
            "R:  wonderful news\n",
            "PA: they were foolish\n",
            "PA_H: they were gleeful\n",
            "\n",
            "Q: How did they communicate with each other about it?\n",
            "A: whispered from one to the other\n",
            "R: the news which was whispered from one to the other\n",
            "PA: they spoke to one another\n",
            "PA_H: they talked to one another\n",
            "\n",
            "Q: What did Johnny tell Jimmy Sullivan about?\n",
            "A: that they were to start a regular theatre\n",
            "R: Johnny himself had told him that they were to start a regular theatre\n",
            "PA: that they were going to be a\n",
            "PA_H: that they were going to have a\n",
            "\n",
            "Q: Did people believe Jimmy?\n",
            "A: yes\n",
            "R:  he was supported by several others as the time wore on\n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: Would doing that be a bad thing?\n",
            "A: yes\n",
            "R: purpose of injuring the credit of these young gentlemen\n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: What had Johnny done about beginning the project?\n",
            "A: he had already engaged a hall\n",
            "R: had already engaged a hall\n",
            "PA: it was a regular theatre\n",
            "PA_H: attempted to acquire a hall\n",
            "\n",
            "Q: Was it going to be used as is?\n",
            "A: no\n",
            "R: a hall, which would be converted into a first-class place of amusement\n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: What was going to happen to it?\n",
            "A: it would be converted into a first-class place of amusement\n",
            "R: a hall, which would be converted into a first-class place of amusement\n",
            "PA: it was caused to be damaged\n",
            "PA_H: it was converted into a full-\n",
            "\n",
            "Q: When?\n",
            "A: as soon as possible\n",
            "R: converted into a first-class place of amusement as soon as possible\n",
            "PA: Early in the morning\n",
            "PA_H: as soon as soon as possible possible\n",
            "\n",
            "Q: Was this only a rumor?\n",
            "A: It was not thought to be so.\n",
            "R: This would have been regarded simply as a rumor\n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: Why not?\n",
            "A: Because it came directly from one of the parties concerned.\n",
            "R: had it not come so directly from one of the parties concerned\n",
            "PA: they were misinformed that they were\n",
            "PA_H: because of the purpose of the credit\n",
            "\n"
          ]
        }
      ],
      "id": "enKWyZR674dz"
    },
    {
      "cell_type": "code",
      "source": [
        "show_worst_5_passages_per_source('mctest', distilroberta_results, history = True, results_h = distilroberta_h_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c964b67c-2ad0-4eae-ff6b-a9b265cb7c45",
        "id": "--E3O4ip74dz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################  MCTEST  #####################\n",
            "\n",
            "\n",
            "PASSAGE 1:\n",
            "\n",
            "David and Lucy waited in their mom's truck. They all were going to the circus to see the clowns. Their Mom had to take their little brother back into the house to get more diapers and go to the bathroom. David was worried because he did not want to miss the wolf clown. Lucy watched her brother rub his chin. \"Why are you looking out the window?\" she asked. \"Mom needs to hurry. We might miss the clowns.\" \"I am going to get out of the truck and draw triangles in the sand.\" Lucy said. \"No! You might make us miss it.\" David cried. Before she could get out of the truck Lucy saw her mom and little brother walk out of the house. Her mom got in the truck and asked, \"Okay kids are we ready to go?\" David and Lucy yelled, \"Yes!\" When they got to the circus David and Lucy saw all the clowns. Their little brother was too young to go into the tent, so their mom stayed with him. The show ended and they walked to the truck. \"What do you have there?\" David asked Lucy \"It is a bag I found in the tent.\" Lucy said. \"We have to return that Lucy.\" Their mom said. \"Why? It has cool stuff in it. I found it.\" Lucy said. \"It is not ours.\" Their mom said. Lucy frowned as they walked to the lost and found and gave the bag back. Then they all went home. When they were washing up to go to bed Lucy said, \"I fear some of those clowns. The wolf one was scary.\" \"Lucy, I can save you. The wolf clown was a person in make-up.\" With that they shut off the bathroom light and went to bed. A great day had by all.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What was in it?\n",
            "A: cool stuff\n",
            "R: \"Why? It has cool stuff in it. I found it.\" Lucy said. \"It is not ours.\" Their mom said. Lucy fro\n",
            "PA: a bag\n",
            "PA_H: a bag\n",
            "\n",
            "Q: Who was in the truck?\n",
            "A: David and Lucy\n",
            "R: David and Lucy waited in their mom's truck\n",
            "PA: their mom and I\n",
            "PA_H: David and Lucy\n",
            "\n",
            "Q: why?\n",
            "A: They were going to the circus\n",
            "R:  They all were going to the circus to see the clowns\n",
            "PA: he didn't want to see the\n",
            "PA_H: because they didn't want the clown\n",
            "\n",
            "Q: Who's trruck was it?\n",
            "A: their mom's\n",
            "R: in their mom's truck\n",
            "PA: their mom and I\n",
            "PA_H: their mom's\n",
            "\n",
            "Q: was she there?\n",
            "A: No\n",
            "R: Their Mom had to take their little brother back into the house \n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: why not?\n",
            "A: She had to take their brother to the house\n",
            "R: Mom had to take their little brother back into the house to get more diapers and go to the bathroom.\n",
            "PA: he didn't want the wolf to\n",
            "PA_H: he didn't want to go to\n",
            "\n",
            "Q: why?\n",
            "A: to get diapers and go to the bathroom\n",
            "R: to get more diapers and go to the bathroom\n",
            "PA: he didn't want to see the\n",
            "PA_H: he didn't want to carry the\n",
            "\n",
            "Q: Did Lucy get out of the truck?\n",
            "A: No\n",
            "R:  Before she could get out of the truck Lucy saw her mom and little brother walk out of the house.\n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "Q: Did she want to?\n",
            "A: Yes\n",
            "R: \"I am going to get out of the truck and draw triangles in the sand.\" Lucy said.\n",
            "PA: no\n",
            "PA_H: Yes\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 2:\n",
            "\n",
            "One morning, I woke up and went downstairs for breakfast. I fixed my normal breakfast: hot pickles, marshmallows, and ketchup. First I heat up the pickles on the stove, then I put the marshmallows in the toaster as I pour out my small plate of ketchup. Delicious! I eat it every morning, often with a tall glass of lemonade. \n",
            "\n",
            "I sat down at the table to eat, and looked out the window at the birds in my yard. Sometimes I see skunks outside, and once I saw a small pot-bellied pig. But today there were only a few flamingos. I opened up the book I was reading, \"All About Birds.\" I had finished the chapter on pigeons, and was moving on to robins. \n",
            "\n",
            "While putting ketchup on my marshmallows, I felt something tickling my neck. I looked down, and there was a daddy long-legs, climbing up! I had never seen one so close to my face. \n",
            "\n",
            "\"Er, what's up?\" I asked. \n",
            "\n",
            "\"Not much,\" he said. \"I smelled the hot pickles, and wanted to have a taste.\" \n",
            "\n",
            "\"Sure, little guy,\" I said. I put a small piece of pickle on my finger and held it out. While I was doing that, that sneaky daddy-long-legs ran down my arm. He grabbed the rest of my pickle, and ran out the door! And that was the last time I gave a pickle to a spider at breakfast.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What happened one morning?\n",
            "A: woke up, went down for breakfast\n",
            "R: I woke up and went downstairs for breakfast\n",
            "PA: he went to bed\n",
            "PA_H: I woke up\n",
            "\n",
            "Q: Was it special?\n",
            "A: No\n",
            "R: I fixed my normal breakfast\n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: What was it?\n",
            "A: hot pickles, marshmallows, and ketchup\n",
            "R: hot pickles, marshmallows, and ketchup\n",
            "PA: a marshmallows\n",
            "PA_H: hot dog, marshmallows,\n",
            "\n",
            "Q: Was it good?\n",
            "A: Yes\n",
            "R: Delicious!\n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "Q: Where does he eat it?\n",
            "A: at the table\n",
            "R: I sat down at the table to eat,\n",
            "PA: at the table\n",
            "PA_H: at the table\n",
            "\n",
            "Q: What is near the table?\n",
            "A: a window\n",
            "R: looked out the window at the birds in my yard\n",
            "PA: birds\n",
            "PA_H: birds\n",
            "\n",
            "Q: Did he see anything?\n",
            "A: flamingos\n",
            "R: But today there were only a few flamingos\n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "Q: How many?\n",
            "A: a few\n",
            "R: only a few flamingos\n",
            "PA: Three\n",
            "PA_H: a few\n",
            "\n",
            "Q: What else were you doing?\n",
            "A: reading\n",
            "R: I opened up the book I was reading, \"All About Birds.\n",
            "PA: looking at the bird\n",
            "PA_H: reading a book\n",
            "\n",
            "Q: What?\n",
            "A: \"All About Birds.\"\n",
            "R: book I was reading, \"All About Birds.\"\n",
            "PA: a hot dog\n",
            "PA_H: All about birds\n",
            "\n",
            "Q: Which ones?\n",
            "A: pigeons and robins\n",
            "R: I had finished the chapter on pigeons, and was moving on to robins\n",
            "PA: hot dogs and marshmallows\n",
            "PA_H: a pigeons\n",
            "\n",
            "Q: What happened next?\n",
            "A: a tickle on my neck.\n",
            "R: I felt something tickling my neck.\n",
            "PA: he tripped into the window\n",
            "PA_H: he felt like a tickling-\n",
            "\n",
            "Q: from what?\n",
            "A: a daddy long-legs\n",
            "R: nd there was a daddy long-legs\n",
            "PA: strawberries\n",
            "PA_H: a kite-legs\n",
            "\n",
            "Q: What did you do?\n",
            "A: asked him what's up\n",
            "R: \"Er, what's up?\" I asked. \n",
            "PA: sat down on the table\n",
            "PA_H: climbed it up up\n",
            "\n",
            "Q: Was he close?\n",
            "A: Yes, very!\n",
            "R: I had never seen one so close to my face\n",
            "PA: no\n",
            "PA_H: No\n",
            "\n",
            "Q: What did he say?\n",
            "A: \"Not much. I smelled the hot pickles, and wanted to have a taste.\"\n",
            "R: \"Not much,\" he said. \"I smelled the hot pickles, and wanted to have a taste.\" \n",
            "PA: I don't like it.\n",
            "PA_H: \"Not much,\"\n",
            "\n",
            "Q: Did you share?\n",
            "A: yes\n",
            "R: I put a small piece of pickle on my finger and held it out. \n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "Q: How?\n",
            "A: held it out on my finger\n",
            "R: I put a small piece of pickle on my finger and held it out.\n",
            "PA: put it on the stove\n",
            "PA_H: he put it on his hand\n",
            "\n",
            "Q: What did the spider do?\n",
            "A: grabbed the rest of the pickle and ran off!\n",
            "R: He grabbed the rest of my pickle, and ran out the door!\n",
            "PA: gave him a pickle\n",
            "PA_H: gave it a pickle\n",
            "\n",
            "Q: Did you do that again?\n",
            "A: No!\n",
            "R: And that was the last time I gave a pickle to a spider at breakfast.\n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 3:\n",
            "\n",
            "Andrew waited for his granddaddy to show up. They were going fishing. His mom had packed them a lunch. She had made Peanut Butter and Jelly Sandwiches. She also packed a bottle of nice cool water to drink. Andrew had wanted something else for lunch. He wanted chicken or cold cuts or left over meat loaf, but his mom sent Peanut Butter and Jelly sandwiches. The best way to get to the river was along the path. When the path ended, they needed to go through the bushes to the river. On some days they would think about going another way. They could go over the bridge, or through the back of the house or through Uncle Tom's yard. Today they took the path to the river. Andrew found some animal poop along the path. He asked his Grandpa what kind of animal poop he had found. He wanted to know if it was a lion, a tiger or a bear that had made the poop. Grandpa pointed in the bushes. Andrew saw a small black and white animal looking at him with big eyes. Andrew knew that he had was looking at a raccoon. He was sure that the raccoon had made the poop. When Andrew and Grandpa got to the river they put their fishing poles into the water. Andrew caught his first fish right away. He caught a second fish before lunch. He got hungry and had his Peanut Butter and Jelly sandwich and cool water. He saw that Grandpa had fallen asleep. After eating his lunch, Andrew caught three more fish before he woke up his grandpa. When Andrew and Grandpa got back home they gave all the fish they had caught to Andrew's mother, so she could make a good dinner.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What was Andrew waiting for?\n",
            "A: His granddaddy.\n",
            "R: Andrew waited for his granddaddy to show up\n",
            "PA: his grandaddy\n",
            "PA_H: his granddaddy\n",
            "\n",
            "Q: Why?\n",
            "A: Because they were going fishing.\n",
            "R: They were going fishing\n",
            "PA: to get a grandaddy out of\n",
            "PA_H: They were going fishing.\n",
            "\n",
            "Q: What did his mom do?\n",
            "A: Packed them a lunch\n",
            "R:  His mom had packed them a lunch\n",
            "PA: gave them some lunch sandwiches and\n",
            "PA_H: She packed a lunch with Peanut\n",
            "\n",
            "Q: of what?\n",
            "A: Peanut Butter and Jelly Sandwiches and a bottle of nice cool water.\n",
            "R: She had made Peanut Butter and Jelly Sandwiches. She also packed a bottle of nice cool water to drink\n",
            "PA: strawberries\n",
            "PA_H: Peanut butter Sandwiches and\n",
            "\n",
            "Q: Did he like it?\n",
            "A: No.\n",
            "R: Andrew had wanted something else for lunch\n",
            "PA: yes\n",
            "PA_H: He wanted to eat some chicken or\n",
            "\n",
            "Q: What did he want?\n",
            "A: Chicken or cold cuts or left over meat loaf.\n",
            "R: He wanted chicken or cold cuts or left over meat loaf\n",
            "PA: a chicken or a hamburger\n",
            "PA_H: Chicken or cold cuts.\n",
            "\n",
            "Q: How did they get to the river?\n",
            "A: Along the path.\n",
            "R: The best way to get to the river was along the path\n",
            "PA: through the bushes\n",
            "PA_H: Along the road.\n",
            "\n",
            "Q: where there other ways?\n",
            "A: Yes.\n",
            "R: On some days they would think about going another way. They could go over the bridge, or through the back of the house or through Uncle Tom's yard\n",
            "PA: through the bridge\n",
            "PA_H: To the bridge.\n",
            "\n",
            "Q: What did he find on the path?\n",
            "A: Animal poop.\n",
            "R: Andrew found some animal poop along the path.\n",
            "PA: an animal poop\n",
            "PA_H: Animal poop.\n",
            "\n",
            "Q: what kind?\n",
            "A: Raccoon.\n",
            "R: He was sure that the raccoon had made the poop\n",
            "PA: an elephant\n",
            "PA_H: A lion.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 4:\n",
            "\n",
            "There was once a group of kittens who all liked to play together. Their names were Fluffy, Snowball, Cookie Monster, and Larry. Fluffy was the leader of the group, and whatever he said, the others did. One day he said, \"Hey, everybody, let's go chase some mice.\" Everyone said \"Yes!\" except Larry. Larry did not want to chase mice. Larry wanted to drink some milk. He told Fluffy, and Fluffy only said \"too bad.\" So everyone went off to chase mice. The next day Fluffy said, \"Hey, everybody, let's go annoy the dogs.\" Everyone said \"Yes\" except Larry. Larry thought it was a bad idea. He told Fluffy, and Fluffy only said, \"Too bad.\" So everyone went off to annoy the dogs. Except Larry. When the cats annoyed the dogs, the dogs became very angry. They started growling. They caught the kittens in their paws. The kittens yelled, \"Help us!\" Larry heard, and he ran over and hissed at the dogs. The dogs could not see Larry, only his shadow, which looked very big. The dogs got scared and ran away, and the kittens cheered. \"Yay Larry! We'll do anything you want!\"\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: How many kittens were there?\n",
            "A: Four\n",
            "R: heir names were Fluffy, Snowball, Cookie Monster, and Larry. \n",
            "PA: Three\n",
            "PA_H: Three\n",
            "\n",
            "Q: Who was in charge?\n",
            "A: Fluffy\n",
            "R: Fluffy was the leader of the group\n",
            "PA: Fluffy\n",
            "PA_H: Fluffy, Snowball, and\n",
            "\n",
            "Q: What did Larry desire?\n",
            "A: milk\n",
            "R: to drink some milk\n",
            "PA: to drink some milk\n",
            "PA_H: To drink some milk\n",
            "\n",
            "Q: Did he get it?\n",
            "A: no\n",
            "R: and Fluffy only said \"too bad.\"\n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "Q: What did the group do?\n",
            "A: chased mice\n",
            "R: So everyone went off to chase mice\n",
            "PA: screamed\n",
            "PA_H: played with him.\n",
            "\n",
            "Q: Did they do something else?\n",
            "A: yes\n",
            "R: So everyone went off to annoy the dogs\n",
            "PA: screamed\n",
            "PA_H: got into the dogs\n",
            "\n",
            "Q: What did they do?\n",
            "A: annoyed the dogs\n",
            "R: So everyone went off to annoy the dogs\n",
            "PA: screamed at them\n",
            "PA_H: got out of the dogs\n",
            "\n",
            "Q: Did they get hurt?\n",
            "A: no\n",
            "R: hen the cats annoyed the dogs, the dogs became very angry. They started growling. They caught the kittens in their paws\n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: How did they survive?\n",
            "A: Larry hissed at the dogs\n",
            "R: Larry heard, and he ran over and hissed at the dogs\n",
            "PA: they were scared\n",
            "PA_H: they were caught in the wagging\n",
            "\n",
            "Q: Did the dogs stay?\n",
            "A: no\n",
            "R: The dogs got scared and ran away,\n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 5:\n",
            "\n",
            "There once was a little old lady who had no friends. She rode her bike to and from the store and didn't say hello to anyone along the way. One day she saw a little girl crying in the street. No one else was around to take care of the girl so the little old lady stopped her bike and asked the girl, \"What is wrong, Dear?\" The little girl said, \"I hurt my knee when I fell down!\" \"Oh dear!\" cried the little old lady and bent down and looked at the girl's knee. \"It looks to me like you have a scrape,\" she said. The little girl cried and the little old lady said, \"Don't worry, all boo-boos hurt a little but it's the medicine that makes it go away.\" The little old lady bent down and blew on the girl's knee and whispered, \"Go away, go away, no boo-boos here today!\" The little girl smiled and that little old lady found her first friend.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Who was antosocial at first?\n",
            "A: the old lady\n",
            "R: old lady who had no friends. \n",
            "PA: little girl\n",
            "PA_H: a little old lady\n",
            "\n",
            "Q: WHo did she find?\n",
            "A: a little girl\n",
            "R: One day she saw a little girl \n",
            "PA: her friend\n",
            "PA_H: a little girl\n",
            "\n",
            "Q: Where her parents around?\n",
            "A: No one else was around to take care of the girl\n",
            "R: No one else was around to take care of the girl \n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: Was she ok?\n",
            "A: no\n",
            "R: One day she saw a little girl crying\n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: How badly was she hurt?\n",
            "A: no\n",
            "R: It looks to me like you have a scrape\n",
            "PA: she hurt her knee\n",
            "PA_H: she hurt her knee\n",
            "\n",
            "Q: How did the woman help her?\n",
            "A: she blew on her knee\n",
            "R: blew on the girl's knee\n",
            "PA: She pushed her bike down on her\n",
            "PA_H: she stopped her bike\n",
            "\n",
            "Q: What else did she do?\n",
            "A: whispered\n",
            "R: whispered, \"Go away, go away, no boo-boos here today!\n",
            "PA: she rode on the bike\n",
            "PA_H: dropped her bike down\n",
            "\n",
            "Q: WHat happened then?\n",
            "A: the girll smiled\n",
            "R: he little girl smiled \n",
            "PA: she fell on her knee\n",
            "PA_H: she fell down\n",
            "\n",
            "Q: Did she like the woman who helped?\n",
            "A: yes\n",
            "R: that little old lady found her first friend\n",
            "PA: yes\n",
            "PA_H: no\n",
            "\n",
            "Q: How many friends she have now?\n",
            "A: one\n",
            "R: old lady found her first friend\n",
            "PA: no\n",
            "PA_H: none\n",
            "\n"
          ]
        }
      ],
      "id": "--E3O4ip74dz"
    },
    {
      "cell_type": "code",
      "source": [
        "show_worst_5_passages_per_source('race', distilroberta_results, history = True, results_h = distilroberta_h_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa61419-34f1-45bc-898a-df54161d9f09",
        "id": "3-blgL9574d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################  RACE  #####################\n",
            "\n",
            "\n",
            "PASSAGE 1:\n",
            "\n",
            "Many of us use them several times a day without really noticing.And yet the way we in lifts,or elevators as they are known in the US,reveals a hidden anxiety. \n",
            "\n",
            "Passengers seem to know instinctively how to arrange themselves in an elevator,like the dots on a die.With each additional passenger, the bodies shift,going into the open spaces. \n",
            "\n",
            "On your own,you can do whatever you want--it's your own little box.If there are two of you,you take different corners, creating the greatest distance. When a third person enters, you will unconsciously form a triangle .And when there is a square, with someone I every corner.A fifth person is probable going to have to stand in the middle. \n",
            "\n",
            "Why are we so awkward in lifts? \n",
            "\n",
            "\"You don't have enough space,\" says Professor Babette Renneberg, a clinical psychologist at the Free University of Berlin.\"Usually when we meet other people we have about an arm's length of distance between us. And that's not possible in most elevators, so it's a very unusual setting. It's unnatural.\" \n",
            "\n",
            "But perhaps there is more to it than just social awkwardness. \n",
            "\n",
            "\"In the back of our minds we are a little anxious,\" says Nick White,an office in New York who was unfortunate enough to be trapped in a lift for41 hours. \"We don't like to be locked into a place.We want to get out of the elevator as soon as possible, you know, it's a frightening place to be.\" \n",
            "\n",
            "During his terrible experience, he began to think of another encloses space-a tomb. \n",
            "\n",
            "Dr.Lee Gray agrees that a sense of disempowerment is the main cause of life anxiety. \n",
            "\n",
            "\"You're in a machine that's moving, over which you have no control. You cannot see the elevator engine, you don't know how it's working,\" he says.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What is the primary device mentioned here?\n",
            "A: lifts,or elevators\n",
            "R:  lifts,or elevators\n",
            "PA: aerial engine\n",
            "PA_H: elevators\n",
            "\n",
            "Q: What can you do when you're in one alone?\n",
            "A: whatever you want\n",
            "R: whatever you want\n",
            "PA: Do your own thing\n",
            "PA_H: make your own box\n",
            "\n",
            "Q: Do two folks generally hang out close to each other?\n",
            "A: you take different corners\n",
            "R: you take different corners\n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: What about three folks?\n",
            "A: you will unconsciously form a triangle\n",
            "R:  you will unconsciously form a triangle\n",
            "PA: Yes.\n",
            "PA_H: they are a third person\n",
            "\n",
            "Q: And four?\n",
            "A: a square\n",
            "R: a square\n",
            "PA: three\n",
            "PA_H: a third person\n",
            "\n",
            "Q: What happens to a 5th?\n",
            "A: stand in the middle\n",
            "R: stand in the middle\n",
            "PA: it's a sign that there is\n",
            "PA_H: a stand in the middle in the\n",
            "\n",
            "Q: Why is it weirder than in other social places?\n",
            "A: You don't have enough space\n",
            "R: You don't have enough space\n",
            "PA: It is not safe.\n",
            "PA_H: It's not uncommon for an arm\n",
            "\n",
            "Q: According to whom?\n",
            "A: Babette Renneberg\n",
            "R: Babette Renneberg\n",
            "PA: Dr Lee Grayneberg\n",
            "PA_H: Professor Babette Renne\n",
            "\n",
            "Q: Are there any other reasons it's uncomfortable?\n",
            "A: we are a little anxious\n",
            "R: we are a little anxious\n",
            "PA: no\n",
            "PA_H: they are different from other humans\n",
            "\n",
            "Q: Who said that?\n",
            "A: Nick White\n",
            "R: Nick White\n",
            "PA: Dr Lee Grayneberg\n",
            "PA_H: Dr. Professor White\n",
            "\n",
            "Q: What can one of these devices feel like?\n",
            "A: a tomb\n",
            "R: a tomb\n",
            "PA: the dots on a die.\n",
            "PA_H: an uncanny place\n",
            "\n",
            "Q: How controlling can you be in one?\n",
            "A: you have no control.\n",
            "R: you have no control.\n",
            "PA: No one controls it\n",
            "PA_H: no\n",
            "\n",
            "Q: Do you feel empowered?\n",
            "A: a sense of disempowerment\n",
            "R: a sense of disempowerment\n",
            "PA: yes\n",
            "PA_H: no\n",
            "\n",
            "Q: Who makes this claim?\n",
            "A: Dr.Lee Gray\n",
            "R: Dr.Lee Gray\n",
            "PA: Dr Lee Grayne\n",
            "PA_H: Dr. Lee Gray\n",
            "\n",
            "Q: How often to a lot of folks ride these devices?\n",
            "A: several times a day\n",
            "R: several times a day \n",
            "PA: several times a day\n",
            "PA_H: several times a day a day\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 2:\n",
            "\n",
            "As we drove along, my spirits went up again, and I turned, with pleasure, to the thought of the new life which I was entering. But though it was not far past the middle of September, the heavy clouds and strong north-easterly wind combined to make the day extremely cold; and the journey seemed a very long one, so that it was nearly one o'clock before we reached the place of our destination. Yet when we entered the gateway, my heart failed me, and I wished it were a mile or two farther off. For the first time in my life I must stand alone: there was no retreating now. I must enter that house, and introduce myself among its strange people. But how was it to be done? True, I was near nineteen; but, thanks to the protecting care of my mother and sister, I well knew that many a girl of fifteen, or under, was gifted with a more womanly address, and greater ease and self-possession, than I was. Yet, anyway. I would do very well, after all; and the children, of course, I should soon be at ease with them. \n",
            "\n",
            "\"Be calm, be calm, whatever happens,\" I said within myself; and truly I was so fully occupied in steadying my nerves and keeping down the rebellious beat of my heart that when I was admitted into the hall and into the presence of Mrs. Bloomfield, I almost forgot to answer her polite greeting; and it afterwards struck me that the little I did say was spoken in the tone of one half-dead or half-asleep. \n",
            "\n",
            "With due politeness, however, she showed me my bedroom, and left me there to take a little refreshment for a little while and led me into the dining-room. Some beefsteaks and potatoes were set before me; and while I dined upon these, she sat opposite, watching me (as I thought) and trying to keep something like a conversation-- consisting chiefly of commonplace remarks. In fact, my attention was almost wholly absorbed in my dinner: not from appetite, but from the toughness of the beefsteaks, and the numbness of my hands. \n",
            "\n",
            "\"I have had so little time to attend to their education myself, but I think they are clever children, and very willing to learn, especially the little boy; he is, I think, the flower of the flock-- a generous, noble-spirited boy, one to be led, but not driven, and remarkable for always speaking the truth.\" \"His sister Mary Ann will require watching,\" continued she, \"but she is a very good girl on the whole, though I wish her to be kept out of the nursery as much as possible, as she is now almost six years old, and might acquire bad habits from the nurses. I have ordered her bed to be placed in your room, and if you will be so kind as to look after her washing and dressing, and take charge of her clothes, she needs to have nothing further to do with the nursery maid.\" \n",
            "\n",
            "I replied I was quite willing to do so; and at that moment the children entered the room. Tom Bloomfield was a well-grown boy of seven. Mary was a tall girl, for her age of six, somewhat dark like her mother. The second sister was Fanny, a very pretty little girl, looking little younger than Mary. The remaining one was Harriet, a little broad, fat, merry, playful thing of scarcely two, whom I had more desire for than all the rest -- but with her I had nothing to do.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What time of year was it?\n",
            "A: not far past the middle of September\n",
            "R: But though it was not far past the middle of September\n",
            "PA: September\n",
            "PA_H: September\n",
            "\n",
            "Q: how was the weather?\n",
            "A: extremely cold\n",
            "R:  the heavy clouds and strong north-easterly wind combined to make the day extremely cold\n",
            "PA: It was very cold\n",
            "PA_H: it was very cold\n",
            "\n",
            "Q: What time did they reach the destination?\n",
            "A: nearly one o'clock\n",
            "R: it was nearly one o'clock before we reached the place of our destination\n",
            "PA: nearly one o'clock\n",
            "PA_H: nearly one o'clock\n",
            "\n",
            "Q: What did she need to do first?\n",
            "A: enter the house\n",
            "R:  I must enter that house, and introduce myself\n",
            "PA: Stand alone\n",
            "PA_H: Stand alone\n",
            "\n",
            "Q: and then?\n",
            "A: introduce herself\n",
            "R: and introduce myself\n",
            "PA: to have a quiet moment.\n",
            "PA_H: began to enter the house\n",
            "\n",
            "Q: What was she doing there?\n",
            "A: to teach her children\n",
            "R: I have had so little time to attend to their education myself, but I think they are clever children, and very willing to learn\n",
            "PA: watching a little meal\n",
            "PA_H: sleeping\n",
            "\n",
            "Q: Who's children?\n",
            "A: Mrs. Bloomfield's\n",
            "R: the presence of Mrs. Bloomfield\n",
            "PA: his mother and sister\n",
            "PA_H: a little girl and little girl\n",
            "\n",
            "Q: How many?\n",
            "A: four\n",
            "R: Tom Bloomfield was a well-grown boy of seven. Mary was a tall girl, for her age of six, somewhat dark like her mother. The second sister was Fanny, a very pretty little girl, looking little younger than Mary. The remaining one was Harriet\n",
            "PA: fifteen\n",
            "PA_H: two\n",
            "\n",
            "Q: Which was known for speaking the truth?\n",
            "A: Tom\n",
            "R: a generous, noble-spirited boy, one to be led, but not driven, and remarkable for always speaking the truth\n",
            "PA: Mrs. Bloomfield\n",
            "PA_H: Mrs. Bloomfield\n",
            "\n",
            "Q: Does the story tell you who the youngest was?\n",
            "A: Yes\n",
            "R: he remaining one was Harriet, a little broad, fat, merry, playful thing of scarcely two,\n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: who?\n",
            "A: Harriet\n",
            "R: The remaining one was Harriet, a little broad, fat, merry, playful thing of scarcely two\n",
            "PA: her mother\n",
            "PA_H: Mrs. Brownfield\n",
            "\n",
            "Q: Who is six?\n",
            "A: Mary Ann\n",
            "R: as she is now almost six years old\n",
            "PA: a little girl\n",
            "PA_H: a little girl\n",
            "\n",
            "Q: and seven?\n",
            "A: Tom\n",
            "R: Tom Bloomfield was a well-grown boy of seven\n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: Whos bed is going to be in her room?\n",
            "A: Mary Ann\n",
            "R: I have ordered her bed to be placed in your room,\n",
            "PA: Mrs. Bloomfield\n",
            "PA_H: Mrs. Bloomfield\n",
            "\n",
            "Q: How old was this teacher?\n",
            "A: near nineteen\n",
            "R: True, I was near nineteen\n",
            "PA: fifteen\n",
            "PA_H: fifteen\n",
            "\n",
            "Q: What held her attention on the meal?\n",
            "A: the toughness of the beefsteaks\n",
            "R: not from appetite, but from the toughness of the beefsteaks\n",
            "PA: the beefsteaks\n",
            "PA_H: the beefsteaks\n",
            "\n",
            "Q: Where did they dine?\n",
            "A: the dining-room\n",
            "R: while and led me into the dining-room\n",
            "PA: the dining-room\n",
            "PA_H: the dining-room\n",
            "\n",
            "Q: Who dined together?\n",
            "A: the girl, while Mrs. Bloomfield watched\n",
            "R: and while I dined upon these, she sat opposite, watching me\n",
            "PA: her mother and father\n",
            "PA_H: the beefsteaks\n",
            "\n",
            "Q: HAs Mrs. Bloomfield been putting in a lot of time teaching the children?\n",
            "A: no\n",
            "R: I have had so little time to attend to their education myself\n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: Does she think they are unwilling to learn?\n",
            "A: no\n",
            "R: but I think they are clever children, and very willing to learn\n",
            "PA: yes\n",
            "PA_H: no\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 3:\n",
            "\n",
            "\"It's 8:15 on a Sunday morning,\" said the officer, Tidwell, \"and this sort of thing seems an unlikely adventure at such a time. Would you mind explaining?\" The man was astonished at the voice from behind. He turned about and said, \"I know what you're thinking, officer, but it isn't true. It's a very funny mistake.\" \"I think you've just left this house in a manner rather than the usual one. That may be quite all right, but I'd like to make sure.\" Tidwell took out his notebook and a pen. \"Name, address and occupation and then, please tell me your story.\" \"Charlie Crane, lorry driver, from Nottingham, 51 Brecon Street. My story?\" \"Yes, What were you doing like a fly on that wall, Mr. Crane?\" \"Well, I had a breakdown yesterday and had to stay the night here. The landlady's name is Mrs. Fern. She gave me breakfast at seven, and I was out of here in the right way and down at the lorry park by half past seven. It was only when I felt around for a cigarette that I realized I'd left $80 under the pillow here. It's a habit I've got into. I even do it almost every day.\" \"I see. Why didn't you miss it when you meant to pay Mrs.? What's her name?\" \"I paid her last night. You've got to pay when you take the room, see? So I came rushing back, but it's Sunday, and she'd gone back to bed. I rang the bell and banged on the front door for ten minutes before I came round here. Up I went this pipe and the money was still there. You know the rest, and I hope you believe it because....\" \"Mr. Crane, whatever are you doing here? I thought you'd gone an hour ago.\" It was Mrs. Fern.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Who was thought to have left earlier?\n",
            "A: Mr. Crane\n",
            "R: \"Mr. Crane, whatever are you doing here? I thought you'd gone an hour ago.\" It was Mrs. Fern.\n",
            "PA: Mrs. Crane\n",
            "PA_H: Mrs. Fern\n",
            "\n",
            "Q: When should he supposed to have left?\n",
            "A: an hour ago\n",
            "R: I thought you'd gone an hour ago.\" It was Mrs. Fern.\n",
            "PA: Sunday morning\n",
            "PA_H: 8:15\n",
            "\n",
            "Q: According to whom?\n",
            "A: Mrs. Fern.\n",
            "R:  It was Mrs. Fern.\n",
            "PA: Mr. Tidwell\n",
            "PA_H: the Tidwell\n",
            "\n",
            "Q: Were dollars left somewhere?\n",
            "A: yes\n",
            "R: I'd left $80 under the pillow here\n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: How many?\n",
            "A: $80\n",
            "R: $80 under the pillow here.\n",
            "PA: Two\n",
            "PA_H: Two\n",
            "\n",
            "Q: Where?\n",
            "A: under the pillow\n",
            "R:  $80 under the pillow here\n",
            "PA: on the lode park\n",
            "PA_H: at the pillow\n",
            "\n",
            "Q: What was he feeling for?\n",
            "A: a cigarette\n",
            "R: when I felt around for a cigarette\n",
            "PA: a cigarette\n",
            "PA_H: a cigarette\n",
            "\n",
            "Q: What time was it?\n",
            "A: It's 8:15\n",
            "R: It's 8:15 on a Sunday morning,\" said the officer, Tidwell, \n",
            "PA: 8:15\n",
            "PA_H: 8:15\n",
            "\n",
            "Q: Am or pm?\n",
            "A: morning\n",
            "R: \"It's 8:15 on a Sunday morning\n",
            "PA: m.\n",
            "PA_H: morning\n",
            "\n",
            "Q: According to whom?\n",
            "A: the officer\n",
            "R: said the officer, Tidwell,\n",
            "PA: Mr. Tidwell\n",
            "PA_H: the Tidwell\n",
            "\n",
            "Q: And his name?\n",
            "A: Tidwell\n",
            "R: \"It's 8:15 on a Sunday morning,\" said the officer, Tidwell\n",
            "PA: Mr. Crane\n",
            "PA_H: Tidwell\n",
            "\n",
            "Q: What did he take out?\n",
            "A: a notebook and a pen\n",
            "R: Tidwell took out his notebook and a pen.\n",
            "PA: his notebook and a notebook\n",
            "PA_H: his notebook and a notebook\n",
            "\n",
            "Q: Did somebody drive?\n",
            "A: Charlie was a lorry driver\n",
            "R: Charlie Crane, lorry driver,\n",
            "PA: yes\n",
            "PA_H: yes\n",
            "\n",
            "Q: What's his last name?\n",
            "A: Crane\n",
            "R: Charlie Crane,\n",
            "PA: Cranwell\n",
            "PA_H: Cranwell\n",
            "\n",
            "Q: Where is he from?\n",
            "A: Nottingham\n",
            "R:  lorry driver, from Nottingham\n",
            "PA: Birmingham\n",
            "PA_H: Newham\n",
            "\n",
            "Q: And what's his street address?\n",
            "A: 51 Brecon Street\n",
            "R: Charlie Crane, lorry driver, from Nottingham, 51 Brecon Street\n",
            "PA: Baton Brecon\n",
            "PA_H: 51 Brecon Street\n",
            "\n",
            "Q: Was he fed?\n",
            "A: yes\n",
            "R: She gave me breakfast \n",
            "PA: no\n",
            "PA_H: yes\n",
            "\n",
            "Q: Which meal?\n",
            "A: breakfast\n",
            "R: She gave me breakfast \n",
            "PA: breakfast\n",
            "PA_H: breakfast\n",
            "\n",
            "Q: By whom?\n",
            "A: Mrs. Fern\n",
            "R: Mrs. Fern. She gave me breakfast\n",
            "PA: Mr. Tidwell\n",
            "PA_H: the landlady\n",
            "\n",
            "Q: What's her position?\n",
            "A: landlady\n",
            "R:  The landlady's name is Mrs. Fern\n",
            "PA: the livery driver\n",
            "PA_H: landlady\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 4:\n",
            "\n",
            "My doorbell rings. On the step, I find the elderly Chinese lady, small and slight, holding the hand of a little boy. In her other hand, she holds a paper carrier bag. \n",
            "\n",
            "I know this lady. It is not her first visit. She is the boy's grandmother, and her daughter bought the house next door last October. \n",
            "\n",
            "Her daughter, Nicole, speaks fluent English. But she is now in Shanghai, and her parents are here with the little boy. Nicole has obviously told her mother that I am having heart surgery soon, so her mother has decided I need more nutrients. \n",
            "\n",
            "I know what is inside the bag--a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake. This has become an almost-daily practice. \n",
            "\n",
            "Communication between us is somewhat affected by the fact that she doesn't speak English and all I can say in Chinese is hello. Once, she brought an iPad as well as the food. She pointed to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty. I am not used to iPads, so she indicated I should go with her to her house. Then, she handed the iPad to her husband and almost immediately I found myself looking at Nicole in Shanghai and discussing her mother's cooking and salt intake. Instantly, tears welled in my eyes. \n",
            "\n",
            "\"Your mother just can't be bringing me meals like this all the time,\" I insisted. \"I can hardly do dishes in return.\" \n",
            "\n",
            "\"Oh, no, Lucy.\" Nicole said. \"Mum doesn't like western food. Don't worry about it; she has to cook for the three of them anyway, and she wants to do it.\" \n",
            "\n",
            "The doorbell keeps ringing and there is the familiar brown paper carrier bag, handed smilingly to me. \n",
            "\n",
            "I am now working on some more Chinese words--it's the least I can do after such display of kindness. \n",
            "\n",
            "\"Thank you\" is, of course, the first one. Somehow, it seems inadequate.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: Who is at the door?\n",
            "A: An elderly Chinese lady and a little boy\n",
            "R: On the step, I find the elderly Chinese lady, small and slight, holding the hand of a little boy\n",
            "PA: The elderly Chinese lady\n",
            "PA_H: the elderly Chinese lady\n",
            "\n",
            "Q: Is she carrying something?\n",
            "A: Yes\n",
            "R: she holds a paper carrier bag\n",
            "PA: a paper bag\n",
            "PA_H: Yes\n",
            "\n",
            "Q: What?\n",
            "A: a paper carrier bag\n",
            "R: a paper carrier bag\n",
            "PA: a hamburger\n",
            "PA_H: a paper bag\n",
            "\n",
            "Q: Do I know her?\n",
            "A: Yes\n",
            "R: I know this lady\n",
            "PA: no\n",
            "PA_H: Yes\n",
            "\n",
            "Q: Who is her daughter?\n",
            "A: Nicole\n",
            "R: Her daughter, Nicole\n",
            "PA: Nicole\n",
            "PA_H: Nicole's grandmother\n",
            "\n",
            "Q: Where does Nicole live?\n",
            "A: Shanghai\n",
            "R:  But she is now in Shanghai\n",
            "PA: Shanghai\n",
            "PA_H: Shanghai\n",
            "\n",
            "Q: How is she related to the boy?\n",
            "A: mother\n",
            "R: She is the boy's grandmother, and her daughter bought the house next door last October. \n",
            "PA: she's a grandmother\n",
            "PA_H: mother's grandmother\n",
            "\n",
            "Q: What is in the bag?\n",
            "A: food\n",
            "R: a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake\n",
            "PA: a hot hot-te bag\n",
            "PA_H: a hot hot soup with a hot\n",
            "\n",
            "Q: Has she done this before?\n",
            "A: Yes\n",
            "R: This has become an almost-daily practice. \n",
            "PA: no\n",
            "PA_H: No\n",
            "\n",
            "Q: Why?\n",
            "A: I am having heart surgery soon, so her mother has decided I need more nutrients\n",
            "R: Nicole has obviously told her mother that I am having heart surgery soon, so her mother has decided I need more nutrients\n",
            "PA: She doesn't have any heart surgery\n",
            "PA_H: she has a heart surgery\n",
            "\n",
            "Q: What has helped us communicate?\n",
            "A: an iPad\n",
            "R: an iPad\n",
            "PA: her screenreader\n",
            "PA_H: she doesn't speak English\n",
            "\n",
            "Q: What kind of dishes does she bring?\n",
            "A: hot soup and a container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake\n",
            "R: hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake\n",
            "PA: fast food and hot water\n",
            "PA_H: hot and spicy soup\n",
            "\n",
            "Q: What do I do to help communicate with her?\n",
            "A: I am now working on some more Chinese words\n",
            "R: I am now working on some more Chinese words\n",
            "PA: give her a letter\n",
            "PA_H: gives her a message\n",
            "\n",
            "Q: Do she continue bringing the bag?\n",
            "A: Yes\n",
            "R: The doorbell keeps ringing and there is the familiar brown paper carrier bag, handed smilingly to me. \n",
            "PA: no\n",
            "PA_H: no\n",
            "\n",
            "Q: What is the first phrase I learn?\n",
            "A: \"Thank you\"\n",
            "R: \"Thank you\"\n",
            "PA: Thank you\n",
            "PA_H: Thank you\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 5:\n",
            "\n",
            "Register in person, by phone 264-8833, or by mail. Use form given. \n",
            "\n",
            "178 IN Winchester St., Chicago \n",
            "\n",
            "Basic Photography This is an eight-hour course for beginners who want to learn how to use a 35mm camera. The teacher will cover such areas as kinds of film, light and lenses . Bring your own 35mm camera to class. Course charge: $50.Jan. 10,12,17,19, Tues & Thurs. 6:00-8:00 pm. Marianne Adams is a professional photographer whose photographs appear in many magazines. \n",
            "\n",
            "Understanding Computers This twelve-hour course is for people who do not know much about computers, but need to learn about them. You will learn what computers are, what they can and can't do, and how to use them. Course charge: $75. Equipment charge: $10. Jan.14, 21, 28, Sats. 7:00-10:30pm. Joseph Saimders is Professor of Computer Science at New Urban University. He has over twelve years of experience in the computer field. \n",
            "\n",
            "Stop Smoking Do you want to stop smoking? Have you already tried to stop and failed? Now it's the time to stop smoking using the latest methods. You can stop smoking, and this twelve-hour course will help you do it. Course charge: $30. Jan. 4, 11, 18, 25, Wedns. 4:00-7:00pm. Dr John Goode is a practicing psychologist who has helped hundreds of people stop smoking. \n",
            "\n",
            "Typing This course on week-days is for those who want to learn to type, as well as those who want to improve their typing. You are tested in the first class and practice at one of eight different skill levels. This allows you to learn at your own speed. Each program lasts 20 hours. Bring your own paper. Course charge: $125. Material charge: $25. Two hours each evening for two weeks. New classes begin every two weeks. This course is taught by a number of business education teachers who have successfully taught typing courses before. \n",
            "\n",
            "Oil painting Oil paint is easy to use once you learn the basics. When you enroll at this oil painting course, you will learn to draw and learn to paint using many oil painting techniques under complete guidance and instruction. Together with the teacher's knowledge and your passion-we'll unlock your creativity and develop your potential! Course charge: $35. Jan. 5, 12, 19, 26, Thurs. 2:00-5:00 pm. Ralf Ericssion has taught beginners to masters and he has learned that everything builds on just a few basic concepts that he will show you here. \n",
            "\n",
            "Singing This course shows you how to deliver an accomplished vocal performance on stage and in the studio. Develop your vocal talents with professional warm-up routines and learn vocal techniques to gain confidence in your performance. You'll learn to perform classic songs before exploring your own songwriting ideas with a tutor. And finally you'll get the chance to record in a professional studio. Singing tuition may be in groups or one-to-one. We have Choral singing, Gospel singing, Folk singing and many other styles of song. All styles are welcome and no previous experience is required. Please read on for course contents and reviews from our students. Course charge: $90. Jan. 10, 12, 17, 19, Tues. & Thurs. 5:30-8:30pm. Peter Syrus is a Grammy award winning tutor.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: How long is the basic photography course?\n",
            "A: eight-hours\n",
            "R: eight-hou\n",
            "PA: eight hours\n",
            "PA_H: eight hours\n",
            "\n",
            "Q: What days is the typing course on?\n",
            "A: week-days\n",
            "R:  week-days\n",
            "PA: Weekends and Thursdays\n",
            "PA_H: Wednesdays and Thurs.\n",
            "\n",
            "Q: Who teaches the stop smoking course?\n",
            "A: . Dr John Goode\n",
            "R: . Dr John Goode\n",
            "PA: Dr. John Goode\n",
            "PA_H: Dr. John Goode\n",
            "\n",
            "Q: What kind of painting is being taught?\n",
            "A: Oil painting\n",
            "R: Oil painting\n",
            "PA: Oil painting\n",
            "PA_H: Oil painting\n",
            "\n",
            "Q: What will you learn in the course?\n",
            "A: to draw and to paint\n",
            "R: o draw and learn to paint\n",
            "PA: how to paint, paint, and\n",
            "PA_H: How to use a 35mm camera\n",
            "\n",
            "Q: What is the price of that course?\n",
            "A: $35\n",
            "R: $35\n",
            "PA: $50,000\n",
            "PA_H: $50\n",
            "\n",
            "Q: What is the course charge for Understanding computers?\n",
            "A: $75\n",
            "R:  $75\n",
            "PA: $25.00\n",
            "PA_H: $75,000\n",
            "\n",
            "Q: Is there an equipment charge for that course?\n",
            "A: Yes\n",
            "R:  Equipment charge: $10\n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "Q: How much is it?\n",
            "A: $10\n",
            "R: $10\n",
            "PA: $25.00\n",
            "PA_H: $50\n",
            "\n",
            "Q: How long is the course?\n",
            "A: twelve hours\n",
            "R: twelve-hour \n",
            "PA: twelve hours\n",
            "PA_H: twelve hours\n",
            "\n",
            "Q: Who teaches the singing course?\n",
            "A: Peter Syrus\n",
            "R: Peter Syrus \n",
            "PA: Dr. John Goode\n",
            "PA_H: Dr. John Goode\n",
            "\n",
            "Q: What are the dates of that course?\n",
            "A: Jan. 10, 12, 17, 19,\n",
            "R: Jan. 10, 12, 17, 19,\n",
            "PA: Jan. 14 and Jan. 14\n",
            "PA_H: Jan. 14, 18, 14\n",
            "\n",
            "Q: What is that courses price?\n",
            "A: $90\n",
            "R:  $90\n",
            "PA: $50,000\n",
            "PA_H: $50\n",
            "\n",
            "Q: How long are the typing classes?\n",
            "A: Two hours\n",
            "R: Two hours\n",
            "PA: twelve hours\n",
            "PA_H: 20 hours\n",
            "\n",
            "Q: And what is the courses duration?\n",
            "A: two weeks\n",
            "R:  two weeks\n",
            "PA: twelve hours\n",
            "PA_H: 12 hours\n",
            "\n",
            "Q: Is there a material charge for that course?\n",
            "A: Yes\n",
            "R: Material charge: $25\n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "Q: What is it?\n",
            "A: $25\n",
            "R: $25\n",
            "PA: Instruction course\n",
            "PA_H: Self-defense\n",
            "\n",
            "Q: Who teaches oil painting?\n",
            "A: Ralf Ericssion\n",
            "R:  Ralf Ericssion\n",
            "PA: Dr John Goode\n",
            "PA_H: a business education teacher\n",
            "\n",
            "Q: What day of the week is his course?\n",
            "A: Thurs\n",
            "R: Thurs\n",
            "PA: Wednesdays\n",
            "PA_H: Wednesdays\n",
            "\n",
            "Q: during what time?\n",
            "A: 2:00-5:00 pm\n",
            "R:  2:00-5:00 pm\n",
            "PA: 7:00 to 8:00\n",
            "PA_H: Nov. 8, 2008\n",
            "\n"
          ]
        }
      ],
      "id": "3-blgL9574d0"
    },
    {
      "cell_type": "code",
      "source": [
        "show_worst_5_passages_per_source('wikipedia', distilroberta_results, history = True, results_h = distilroberta_h_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b42578-ec18-4497-f391-6338a1c3c497",
        "id": "xUnC6QMC74d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#####################  WIKIPEDIA  #####################\n",
            "\n",
            "\n",
            "PASSAGE 1:\n",
            "\n",
            "A thorough understanding of adolescence in society depends on information from various perspectives, including psychology, biology, history, sociology, education, and anthropology. Within all of these perspectives, adolescence is viewed as a transitional period between childhood and adulthood, whose cultural purpose is the preparation of children for adult roles. It is a period of multiple transitions involving education, training, employment and unemployment, as well as transitions from one living circumstance to another. \n",
            "\n",
            "Puberty occurs through a long process and begins with a surge in hormone production, which in turn causes a number of physical changes. It is the stage of life characterized by the appearance and development of secondary sex characteristics (for example, a deeper voice and larger adam's apple in boys, and development of breasts and more curved and prominent hips in girls) and a strong shift in hormonal balance towards an adult state. This is triggered by the pituitary gland, which secretes a surge of hormonal agents into the blood stream, initiating a chain reaction to occur. The male and female gonads are subsequently activated, which puts them into a state of rapid growth and development; the triggered gonads now commence the mass production of the necessary chemicals. The testes primarily release testosterone, and the ovaries predominantly dispense estrogen. The production of these hormones increases gradually until sexual maturation is met. Some boys may develop gynecomastia due to an imbalance of sex hormones, tissue responsiveness or obesity.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What connect childhood and adulthood?\n",
            "A: adolescence\n",
            "R: A thorough understanding of adolescence in society depends on information from various perspectives, including psychology, biology, history, sociology, education, and anthropology. Within all of these perspectives, adolescence is viewed as a transitional period between childhood and adulthood, whose cultural purpose is the preparation of children for adult roles. It is a period of multiple transitions involving education, training, employment and unemployment, as well as transitions from one living circumstance to another. \n",
            "PA: a transition period between adolescence and adolescence\n",
            "PA_H: a transition between cultural and developmental periods\n",
            "\n",
            "Q: What non-biological purpose does it have?\n",
            "A: cultural\n",
            "R: Within all of these perspectives, adolescence is viewed as a transitional period between childhood and adulthood, whose cultural purpose is the preparation of children for adult roles. \n",
            "PA: preparing for the preparation of adult\n",
            "PA_H: preparing for children's role roles\n",
            "\n",
            "Q: What's the long biological process that happens during it?\n",
            "A: puberty\n",
            "R: Puberty occurs through a long process and begins with a surge in hormone production,\n",
            "PA: a surge in hormonal production\n",
            "PA_H: a surge in hormonal production\n",
            "\n",
            "Q: What develops during this?\n",
            "A: secondary sex characeristics\n",
            "R:  It is the stage of life characterized by the appearance and development of secondary sex characteristics (for example, a deeper voice and larger adam's apple in boys, and development of breasts and more curved and prominent hips in girls)\n",
            "PA: genetic and hormonal changes\n",
            "PA_H: a surge in hormonal production\n",
            "\n",
            "Q: What's an example?\n",
            "A: a deeper voice in boys\n",
            "R: for example, a deeper voice and larger adam's apple in boys, and development of breasts and more curved and prominent hips in girls)\n",
            "PA: diversification of male and female\n",
            "PA_H: a deeper voice and a deeper voice\n",
            "\n",
            "Q: What causes these developments?\n",
            "A: the pituitary gland\n",
            "R: This is triggered by the pituitary gland,\n",
            "PA: it has an effect on the sex\n",
            "PA_H: a pit gland, a pit gland\n",
            "\n",
            "Q: What does the pituitary gland alter?\n",
            "A: the male and female gonads\n",
            "R: . The male and female gonads are subsequently activated,\n",
            "PA: a surge of hormonal agents\n",
            "PA_H: a hormonal effect of hormonal agents\n",
            "\n",
            "Q: What alters those?\n",
            "A: hormonal agents\n",
            "R:  This is triggered by the pituitary gland, which secretes a surge of hormonal agents into the blood stream, initiating a chain reaction to occur\n",
            "PA: the vasogenary glands\n",
            "PA_H: a surge of surge of hormonal agents\n",
            "\n",
            "Q: What can happen if the balance of those agents change too much?\n",
            "A: gynecomastia\n",
            "R: Some boys may develop gynecomastia due to an imbalance of sex hormones, tissue responsiveness or obesity.\n",
            "PA: an adult state will become an adult\n",
            "PA_H: gypsycomasia\n",
            "\n",
            "Q: What else can cause that?\n",
            "A: tissue responsiveness or obesity\n",
            "R: tissue responsiveness or obesity.\n",
            "PA: the vasoconstary glands\n",
            "PA_H: an imbalance of sex responsiveness, hormonal\n",
            "\n",
            "Q: What carries the hormones?\n",
            "A: the testes and the ovaries\n",
            "R: The testes primarily release testosterone, and the ovaries predominantly dispense estrogen. \n",
            "PA: the pitary glandary glands\n",
            "PA_H: the testicular glands\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 2:\n",
            "\n",
            "Windows 8 introduced major changes to the operating system's platform and user interface to improve its user experience on tablets, where Windows was now competing with mobile operating systems, including Android and iOS. In particular, these changes included a touch-optimized Windows shell based on Microsoft's \"Metro\" design language, the Start screen (which displays programs and dynamically updated content on a grid of tiles), a new platform for developing apps with an emphasis on touchscreen input, integration with online services (including the ability to sync apps and settings between devices), and Windows Store, an online store for downloading and purchasing new software. Windows 8 added support for USB 3.0, Advanced Format hard drives, near field communications, and cloud computing. Additional security features were introduced, such as built-in antivirus software, integration with Microsoft SmartScreen phishing filtering service and support for UEFI Secure Boot on supported devices with UEFI firmware, to prevent malware from infecting the boot process.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: what prevents malware for infecting the boot process ?\n",
            "A: UEFI firmware\n",
            "R: with UEFI firmware, to prevent malware from infecting the boot process.\n",
            "PA: U.S. UEFI\n",
            "PA_H: U.S.FI BIOS\n",
            "\n",
            "Q: Windows 8 introduced what ?\n",
            "A: major changes\n",
            "R: Windows 8 introduced major changes\n",
            "PA: major graphical interface and graphical interface\n",
            "PA_H: major changes to the operating system and\n",
            "\n",
            "Q: to what ?\n",
            "A: operating system's platform\n",
            "R: Windows 8 introduced major changes to the operating system's platform\n",
            "PA: improved graphical experience and graphical experience\n",
            "PA_H: the operating system's platform and platform\n",
            "\n",
            "Q: why ?\n",
            "A: to improve its user experience on tablets\n",
            "R: Windows 8 introduced major changes to the operating system's platform and user interface to improve its user experience on tablets\n",
            "PA: to improve mobile experience of mobile devices\n",
            "PA_H: to improve its users on its users\n",
            "\n",
            "Q: what does the smart screen do ?\n",
            "A: phishing filtering service\n",
            "R:  integration with Microsoft SmartScreen phishing filtering service\n",
            "PA: sisters programs and updated content on\n",
            "PA_H: visualizes programs and protects content content\n",
            "\n",
            "Q: what is the online store for ?\n",
            "A: downloading and purchasing new software\n",
            "R: an online store for downloading and purchasing new software\n",
            "PA: copying and managing software\n",
            "PA_H: buying and managing software\n",
            "\n",
            "Q: how many things did windows add support for ?\n",
            "A: four\n",
            "R:  Windows 8 added support for USB 3.0, Advanced Format hard drives, near field communications, and cloud computing\n",
            "PA: Two\n",
            "PA_H: Two\n",
            "\n",
            "Q: how many Additional security features were introduced ?\n",
            "A: three\n",
            "R: Additional security features were introduced, such as built-in antivirus software, integration with Microsoft SmartScreen phishing filtering service and support for UEFI Secure Boot on supported devices with UEFI firmware, to prevent malware from infecting the boot process.\n",
            "PA: Three-in-browser software\n",
            "PA_H: Two\n",
            "\n",
            "Q: what harm would malware do ?\n",
            "A: infecting the boot process\n",
            "R: to prevent malware from infecting the boot process.\n",
            "PA: prevent infecting bootloader\n",
            "PA_H: encourages the bootloader\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 3:\n",
            "\n",
            "An atoll (, , , , or ), sometimes called a coral atoll, is a ring-shaped coral reef including a coral rim that encircles a lagoon partially or completely. There may be coral islands/cays on the rim. The coral of the atoll often sits atop the rim of an extinct seamount or volcano which has eroded or subsided partially beneath the water. The lagoon forms over the volcanic crater or caldera while the higher rim remains above water or at shallow depths that permit the coral to grow and form the reefs. For the atoll to persist, continued erosion or subsidence must be at a rate slow enough to permit reef growth upwards and outwards to replace the lost height. \n",
            "\n",
            "The word \"atoll\" comes from the Dhivehi (an Indo-Aryan language spoken on the Maldive Islands) word \"atholhu\" (Dhivehi: , ), meaning an administrative subdivision. Its first recorded use in English was in 1625 as \"atollon\" – Charles Darwin recognized its indigenous origin and coined, in his \"The Structure and Distribution of Coral Reefs\", the definition of atolls as \"circular groups of coral islets\" that is synonymous with \"lagoon-island\". \n",
            "\n",
            "More modern definitions of \"atoll\" describe them as \"annular reefs enclosing a lagoon in which there are no promontories other than reefs and islets composed of reef detritus\" or \"in an exclusively morphological sense, [as] a ring-shaped ribbon reef enclosing a lagoon\".\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: what kind of coral reef is discussed?\n",
            "A: a coral atoll\n",
            "R:  a coral atoll, is a ring-shaped coral reef\n",
            "PA: a ring-shaped shape\n",
            "PA_H: An o'shaped\n",
            "\n",
            "Q: what shape is it?\n",
            "A: a ring\n",
            "R: is a ring-shaped coral reef\n",
            "PA: ring-shaped shape\n",
            "PA_H: ring-shaped shape\n",
            "\n",
            "Q: what is another attribute?\n",
            "A: it encircles a lagoon\n",
            "R: that encircles a lagoon\n",
            "PA: it's the longest-lived stream\n",
            "PA_H: a coral archipelago\n",
            "\n",
            "Q: where is the coral?\n",
            "A: atop the rim\n",
            "R: The coral of the atoll often sits atop the rim of an extinct seamount\n",
            "PA: on the edge of the volcano\n",
            "PA_H: on the edge of the archipel\n",
            "\n",
            "Q: of what?\n",
            "A: an extinct seamount or volcano\n",
            "R: the rim of an extinct seamount or volcano\n",
            "PA: an extinct volcano\n",
            "PA_H: an extinct seapount or se\n",
            "\n",
            "Q: above the water?\n",
            "A: partially\n",
            "R: or subsided partially beneath the water\n",
            "PA: no\n",
            "PA_H: seated beneath the cal calder\n",
            "\n",
            "Q: what language does atoll come from?\n",
            "A: Dhivehi\n",
            "R: \"atoll\" comes from the Dhivehi \n",
            "PA: the Dhāhi language\n",
            "PA_H: the Dhāhi language\n",
            "\n",
            "Q: what word is it derived from?\n",
            "A: \"atholhu\"\n",
            "R: from the Dhivehi (an Indo-Aryan language spoken on the Maldive Islands) word \"atholhu\"\n",
            "PA: the Dhivehhihi\n",
            "PA_H: the Indo-Aryan language\n",
            "\n",
            "Q: what does that mean?\n",
            "A: an administrative subdivision\n",
            "R: meaning an administrative subdivision\n",
            "PA: an administrative subdivion\n",
            "PA_H: an administrative subdivision\n",
            "\n",
            "Q: where is that language spoken?\n",
            "A: the Maldive Islands\n",
            "R: spoken on the Maldive Islands\n",
            "PA: on the Maldive Islands\n",
            "PA_H: on the island of Maldive Islands\n",
            "\n",
            "Q: what is a modern meaning of atoll?\n",
            "A: annular reefs enclosing a lagoon\n",
            "R: modern definitions of \"atoll\" describe them as \"annular reefs enclosing a lagoon\n",
            "PA: an administrative clacention\n",
            "PA_H: \"annular reefs\"\n",
            "\n",
            "Q: when was it first used in English?\n",
            "A: in 1625\n",
            "R: Its first recorded use in English was in 1625\n",
            "PA: 1625\n",
            "PA_H: 1625\n",
            "\n",
            "Q: by who?\n",
            "A: Charles Darwin\n",
            "R: Charles Darwin\n",
            "PA: Charles Darwin\n",
            "PA_H: Charles Darwin\n",
            "\n",
            "Q: how did he describe it?\n",
            "A: \"circular groups of coral islets\"\n",
            "R: \"circular groups of coral islets\"\n",
            "PA: \"annular reefs\" or \"\n",
            "PA_H: \"circular origin\" and \"\n",
            "\n",
            "Q: what is it similar to?\n",
            "A: \"lagoon-island\"\n",
            "R: that is synonymous with \"lagoon-island\"\n",
            "PA: a coral reef\n",
            "PA_H: the \"lide-island\n",
            "\n",
            "Q: what word did he use for it?\n",
            "A: \"atollon\"\n",
            "R: \"atollon\" – Charles Darwin\n",
            "PA: \"hollhuhu\"\n",
            "PA_H: \" structure of coral reefs\"\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 4:\n",
            "\n",
            "The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of \"one world, one dream\". Plans for the relay were announced on April 26, 2007, in Beijing, China. The relay, also called by the organizers as the \"Journey of Harmony\", lasted 129 days and carried the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics. \n",
            "\n",
            "After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the event.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: What is the topic?\n",
            "A: The 2008 Summer Olympics torch relay\n",
            "R: The 2008 Summer Olympics torch relay \n",
            "PA: one of the world's one dream\n",
            "PA_H: The 2008 Summer Olympics torch relay\n",
            "\n",
            "Q: When did it occur?\n",
            "A: March 24 until August 8, 2008\n",
            "R: The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008\n",
            "PA: March 24, 2008\n",
            "PA_H: March 8, 2008\n",
            "\n",
            "Q: What was the theme\n",
            "A: \"one world, one dream\".\n",
            "R: 2008 Summer Olympics, with the theme of \"one world, one dream\". \n",
            "PA: one world, one dream\n",
            "PA_H: one world, one world.\n",
            "\n",
            "Q: What was the length of the race?\n",
            "A: 137,000 km\n",
            "R: and carried the torch 137,000 km\n",
            "PA: 137,000 km (137,\n",
            "PA_H: 137,000 km\n",
            "\n",
            "Q: Was it larger than previous ones?\n",
            "A: No\n",
            "R: the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics. \n",
            "PA: yes\n",
            "PA_H: No\n",
            "\n",
            "Q: Where did the race begin?\n",
            "A: Olympia, Greece\n",
            "R: After being lit at the birthplace of the Olympic Games in Olympia, Greece\n",
            "PA: Olympia\n",
            "PA_H: Olympia\n",
            "\n",
            "Q: Is there anything notable about that place?\n",
            "A: birthplace of Olympic Games\n",
            "R: After being lit at the birthplace of the Olympic Games\n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "Q: Where did they go after?\n",
            "A: Athens\n",
            "R:  the torch traveled to the Panathinaiko Stadium in Athens\n",
            "PA: Panathinaiko Stadium\n",
            "PA_H: Panathinaiko Stadium\n",
            "\n",
            "Q: How many days was the race?\n",
            "A: seven\n",
            "R: After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. \n",
            "PA: 129\n",
            "PA_H: 129\n",
            "\n",
            "Q: Did they visit any notable landmarks?\n",
            "A: Panathinaiko Stadium\n",
            "R:  the torch traveled to the Panathinaiko Stadium in Athens\n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "Q: Did they visit any ancient Chinese sites?\n",
            "A: Silk Road\n",
            "R:  the Silk Road, symbolizing ancient links between China and the rest of the world.\n",
            "PA: yes\n",
            "PA_H: Yes\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PASSAGE 5:\n",
            "\n",
            "Endemism is the ecological state of a species being unique to a defined geographic location, such as an island, nation, country or other defined zone, or habitat type; organisms that are indigenous to a place are not endemic to it if they are also found elsewhere. The extreme opposite of endemism is cosmopolitan distribution. An alternative term for a species that is endemic is precinctive, which applies to species (and subspecific categories) that are restricted to a defined geographical area. \n",
            "\n",
            "The word \"endemic\" is from New Latin \"endēmicus\", from Greek ενδήμος, \"endēmos\", \"native\". \"Endēmos\" is formed of \"en\" meaning \"in\", and \"dēmos\" meaning \"the people\". The term \"precinctive\" has been suggested by some scientists, and was first used in botany by MacCaughey in 1917. It is the equivalent of \"endemism\". \"Precinction\" was perhaps first used by Frank and McCoy. \"Precinctive\" seems to have been coined by David Sharp when describing the Hawaiian fauna in 1900: \"I use the word precinctive in the sense of 'confined to the area under discussion' ... 'precinctive forms' means those forms that are confined to the area specified.\" That definition excludes artificial confinement of examples by humans in far-off botanical gardens or zoological parks. \n",
            "\n",
            "Physical, climatic, and biological factors can contribute to endemism. The orange-breasted sunbird is exclusively found in the fynbos vegetation zone of southwestern South Africa. The glacier bear is found only in limited places in Southeast Alaska. Political factors can play a part if a species is protected, or actively hunted, in one jurisdiction but not another.\n",
            "\n",
            "\n",
            "CONVERSATION:\n",
            "\n",
            "Q: How many factors contribute to endemism?\n",
            "A: Three.\n",
            "R: Physical, climatic, and biological factors can contribute to endemism\n",
            "PA: Two\n",
            "PA_H: Two\n",
            "\n",
            "Q: Is psychological one of those?\n",
            "A: No.\n",
            "R: Physical, climatic, and biological factors can contribute to endemism. \n",
            "PA: no\n",
            "PA_H: No.\n",
            "\n",
            "Q: What about biological?\n",
            "A: Yes.\n",
            "R: Physical, climatic, and biological factors can contribute to endemism\n",
            "PA: It is a climatic climatic\n",
            "PA_H: Physical and climatic.\n",
            "\n",
            "Q: What are the other two?\n",
            "A: Physical and climatic\n",
            "R: Physical, climatic, and biological factors can contribute to endemism\n",
            "PA: worldwide, national, and national\n",
            "PA_H: Physical, climatic, and\n",
            "\n",
            "Q: In what country is the orange breasted sunbird found?\n",
            "A: South Africa\n",
            "R: The orange-breasted sunbird is exclusively found in the fynbos vegetation zone of southwestern South Africa\n",
            "PA: Southeastern Africa\n",
            "PA_H: South Africa.\n",
            "\n",
            "Q: Is it found throughout the country?\n",
            "A: No\n",
            "R: The orange-breasted sunbird is exclusively found in the fynbos vegetation zone of southwestern South Africa\n",
            "PA: no\n",
            "PA_H: No.\n",
            "\n",
            "Q: Which region of South Africa?\n",
            "A: southwestern\n",
            "R: The orange-breasted sunbird is exclusively found in the fynbos vegetation zone of southwestern South Africa\n",
            "PA: Southeastern South Africa\n",
            "PA_H: Seynbosia forests.\n",
            "\n",
            "Q: And what zone?\n",
            "A: Fynbos vegetation zone.\n",
            "R: The orange-breasted sunbird is exclusively found in the fynbos vegetation zone of southwestern South Africa\n",
            "PA: in southwestern South South Africa\n",
            "PA_H: Fynbosbosia forest\n",
            "\n",
            "Q: Does the term endemic have a latin root?\n",
            "A: Yes\n",
            "R: The word \"endemic\" is from New Latin \"endēmicus\",\n",
            "PA: no\n",
            "PA_H: No.\n",
            "\n",
            "Q: What else?\n",
            "A: Greek\n",
            "R: The word \"endemic\" is from New Latin \"endēmicus\", from Greek ενδήμος, \"endēmos\", \"native\". \"Endēmos\" is formed of \"en\" meaning \"in\", and \"dēmos\" meaning \"the people\n",
            "PA: biodiversity\n",
            "PA_H: A glacier.\n",
            "\n",
            "Q: What would \"en\" translate to?\n",
            "A: \"in\"\n",
            "R: \" is formed of \"en\" meaning \"in\"\n",
            "PA: \"in the world\"\n",
            "PA_H: \"People\".\n",
            "\n",
            "Q: And \"demos\"?\n",
            "A: \"the people\"\n",
            "R: and \"dēmos\" meaning \"the people\"\n",
            "PA: \"dēme\"\n",
            "PA_H: \"dēme\"\n",
            "\n",
            "Q: What term means the same thing?\n",
            "A: Precinctive\n",
            "R: The term \"precinctive\" has been suggested by some scientists, and was first used in botany by MacCaughey in 1917. It is the equivalent of \"endemism\".\n",
            "PA: \"endemem\"\n",
            "PA_H: \"endēme\"\n",
            "\n",
            "Q: Who first used that?\n",
            "A: MacCaughey\n",
            "R:  The term \"precinctive\" has been suggested by some scientists, and was first used in botany by MacCaughey\n",
            "PA: Frank McCaughey and Mac McC\n",
            "PA_H: Frank and Chernhey\n",
            "\n",
            "Q: Was he studying animal species?\n",
            "A: No.\n",
            "R: and was first used in botany by MacCaughey in 1917. It is the equivalent of \"endemism\".\n",
            "PA: yes\n",
            "PA_H: Yes.\n",
            "\n",
            "Q: Can something be indiginous but not endemic?\n",
            "A: Yes.\n",
            "R: organisms that are indigenous to a place are not endemic to it if they are also found elsewhere\n",
            "PA: no\n",
            "PA_H: Yes.\n",
            "\n",
            "Q: What would cause that?\n",
            "A: They are also found elsewhere.\n",
            "R: organisms that are indigenous to a place are not endemic to it if they are also found elsewhere.\n",
            "PA: physical, climatic and climatic\n",
            "PA_H: The presence of a population.\n",
            "\n",
            "Q: What is its opposite?\n",
            "A: Cosmopolitan distribution\n",
            "R: The extreme opposite of endemism is cosmopolitan distribution\n",
            "PA: cosmopolitan distribution\n",
            "PA_H: Cosmopolitan distribution.\n",
            "\n",
            "Q: What is the definition of endemism?\n",
            "A: The ecological state of a species being unique to a defined geographic location\n",
            "R: Endemism is the ecological state of a species being unique to a defined geographic location\n",
            "PA: Ecological state of nature\n",
            "PA_H: The ecological state of a unique state\n",
            "\n",
            "Q: Give me one example of a geographic location in this sense.\n",
            "A: An island\n",
            "R:  to a defined geographic location, such as an island, nation,\n",
            "PA: an island\n",
            "PA_H: An island, a island, and\n",
            "\n"
          ]
        }
      ],
      "id": "xUnC6QMC74d0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's interesting to notice that also DistilRoBERTa model, even when missing the correct answer, in most of the cases it's able to predict something at least plausible, showing the capability to understand the question type and answer coherently.\n",
        "\n",
        "Also, the model shows to really predict passage-related answers and it can be observed that among all different question types, it finds it easier to answer to lexical matches and paraphrasing than pragmatics ones, as the former contain explicit or implicit clues on what portion of the passage the question relates to, while the latter require a common sense knowledge to answer properly.\n",
        "\n",
        "In addition, it emerges that DistilRoBERTa history version performs better than the baseline on questions which refer back to conversational history."
      ],
      "metadata": {
        "id": "p2s7xjmcabPT"
      },
      "id": "p2s7xjmcabPT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Effects of passages truncation analysis"
      ],
      "metadata": {
        "id": "pPOl6I28rFMl"
      },
      "id": "pPOl6I28rFMl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The $512$ input tokens limit required by the models forces the truncation of longer passages, sometimes excluding key portions of text. Then, let's check which kind of impact this phenomenon may have."
      ],
      "metadata": {
        "id": "JqaQVlLPeM8W"
      },
      "id": "JqaQVlLPeM8W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2AJtPJZs1iS"
      },
      "outputs": [],
      "source": [
        "def truncation_rationale(df):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns a dictionary {passage_id : Q&A_pairs_ids}, where the Q&A_pairs_ids is a list of Q&A ids whose rationale has been truncated because of the 512 tokens limit\n",
        "    and the passage_id is the id of Q&A pairs related passage. It also returns the total number of Q&A pairs whose rationale suffers from truncation.\n",
        "    \"\"\"\n",
        "\n",
        "    max_length = 512\n",
        "    questions_per_passage = []\n",
        "    dict_passage_questions = {}\n",
        "    last_id = ''\n",
        "    n_truncated = 0\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "\n",
        "      passage_till_rationale = row['passage'][:row['rationale_end']]\n",
        "      length = len(passage_till_rationale.split(\" \"))\n",
        "\n",
        "      if length > max_length:\n",
        "        n_truncated += 1\n",
        "\n",
        "        if last_id != row['id']:\n",
        "          last_id  = row['id']\n",
        "          questions_per_passage = []\n",
        "\n",
        "        questions_per_passage.append(row['question'])\n",
        "        dict_passage_questions[last_id] = questions_per_passage\n",
        "\n",
        "    return dict_passage_questions, n_truncated\n",
        "\n",
        "\n",
        "dict_passage_questions_train, n_truncated_train = truncation_rationale(df_train)\n",
        "dict_passage_questions_val, n_truncated_val = truncation_rationale(df_val)\n",
        "dict_passage_questions_test, n_truncated_test = truncation_rationale(df_test)"
      ],
      "id": "j2AJtPJZs1iS"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The number of train Q&A pairs whose passage has been truncated before the rationale end is \" + str(n_truncated_train))\n",
        "print(\"The number of validation Q&A pairs whose passage has been truncated before the rationale end is \" + str(n_truncated_val))\n",
        "print(\"The number of test Q&A pairs whose passage has been truncated before the rationale end is \" + str(n_truncated_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtZCyYHCpFiG",
        "outputId": "067ffeba-dcdd-4b67-a839-d098c797d0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of train Q&A pairs whose passage has been truncated before the rationale end is 83\n",
            "The number of validation Q&A pairs whose passage has been truncated before the rationale end is 38\n",
            "The number of test Q&A pairs whose passage has been truncated before the rationale end is 14\n"
          ]
        }
      ],
      "id": "dtZCyYHCpFiG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It could be interesting to compute, for each passage whose Q&A pairs are affected by rationale truncation, which percentage of the whole conversation those turns cover."
      ],
      "metadata": {
        "id": "tsQTPX9Te93Y"
      },
      "id": "tsQTPX9Te93Y"
    },
    {
      "cell_type": "code",
      "source": [
        "def show_truncation_percentage(df, dict_passage_questions):\n",
        "\n",
        "  \"\"\"\n",
        "  For each passage in `dict_passage_questions`, it computes the percentage of related Q&A pairs whose rationale suffers from truncation.\n",
        "  \"\"\"\n",
        "\n",
        "  df_truncation = df.groupby(['id', 'passage'], as_index=False).mean().drop(['rationale_end'], axis = 1)\n",
        "  df_truncation['truncation_percentage'] = np.zeros(df_truncation.shape[0])\n",
        "\n",
        "  for id_passage, questions in dict_passage_questions.items():\n",
        "\n",
        "    passage = df[df['id']==id_passage]\n",
        "    percentage_truncation = int(np.round(len(questions)/passage.shape[0]*100))\n",
        "    df_truncation.loc[df_truncation['id'] == id_passage, 'truncation_percentage'] = str(percentage_truncation) + '%'\n",
        "\n",
        "  return df_truncation"
      ],
      "metadata": {
        "id": "UM76hNpY5rZC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "UM76hNpY5rZC"
    },
    {
      "cell_type": "code",
      "source": [
        "df_truncation_train = show_truncation_percentage(df_train, dict_passage_questions_train)\n",
        "df_truncation_train[df_truncation_train['truncation_percentage'] != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "9SJddkD-mVzM",
        "outputId": "04f85f89-e0b0-4a77-a5ea-a35b2eabee84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  id  \\\n",
              "96    30lb5cdzncau778s2e7bvp843w70za   \n",
              "455   32zkvd547fnu6149fn9rb5z8f63b3a   \n",
              "678   33ppungg385i71srwrqqfl9rcnyrzt   \n",
              "767   34hjijklp5wuxbljki5ammllwxf4vs   \n",
              "1111  36wlnqg78zaxgzk647qnuw356vdebo   \n",
              "1340  38f71oa9gtwl54ozq702quzzuubmfg   \n",
              "1341  38f71oa9gtwl54ozq702quzzuvxmf4   \n",
              "1416  39ghhavomfrl6glp3trrjyar1bu4jv   \n",
              "1491  39o5d9o87tsdg6wftn5mmp5qxq13c5   \n",
              "1738  3befod78w6tb7ora6q4jzq285414m9   \n",
              "2032  3dy4fpooa1o1yhnhvu1nufwvocgvrl   \n",
              "2255  3f1567xtnw53p9vefe7rx7xt1yzq93   \n",
              "2469  3glb5jmzfxvofaehoy7hppchmvldgq   \n",
              "2519  3h7xdtshkcrnoge85tc7hd12tb0gwb   \n",
              "2626  3hvvdcpgtesviqve4ut21t17uerytm   \n",
              "2656  3i02618ya06g9pi2dcnttyux9mzpuc   \n",
              "3203  3leiz60cdjzc31w52aq4o09x60xz9h   \n",
              "3256  3lozaj85yddcymbrgjn4hsl8su42xa   \n",
              "3448  3mtmreqs4vimep15jtkxlrqzvh6wa0   \n",
              "3468  3myyfcxhj37bfevovn6omlib9ing4b   \n",
              "3486  3n2bf7y2vqu5j0f5lxo2tfbca9vhmg   \n",
              "3765  3oswbblg1exz1w97d87ldbccplsdxt   \n",
              "3935  3pmby0ye273zv8lvaw6wd28cxidc9x   \n",
              "4184  3r3yrb5grf39mlc0ot5w3352a1juan   \n",
              "4548  3tdxmtx3cbu3qs5x4zz64vf5kn76i2   \n",
              "4703  3txd01zld4hukwwjfsv5q0j2iqku47   \n",
              "5027  3vw04l3zlt6dz2eo488x7if453nxxy   \n",
              "5260  3wz36bjev3gz5i23u2fiti3694ctb3   \n",
              "5461  3yhh42uu5bfa2irondg2nax6oodl0s   \n",
              "5705  3zr9aiqjub9e4ak3hlhl1tvv22004w   \n",
              "\n",
              "                                                passage truncation_percentage  \n",
              "96    Billie Holiday was an American jazz singer and...                  100%  \n",
              "455   Old Behrman was a painter who lived on the gro...                    6%  \n",
              "678   Scrooge looked around him angrily. It was Chri...                   25%  \n",
              "767   A disheveled man appeared in court Thursday on...                   13%  \n",
              "1111  ROME: The legend of Leonardo da Vinci is cover...                   10%  \n",
              "1340  Not so long ago, most people didn't know who S...                   30%  \n",
              "1341  The revolution was over by the time Tom Ford s...                    6%  \n",
              "1416  Old Behrman was a painter who lived on the gro...                   20%  \n",
              "1491  Mark Twain has been called the inventor of the...                   19%  \n",
              "1738  Consult the page adapted from an English dicti...                   10%  \n",
              "2032  As a young boy, I sometimes traveled the count...                   11%  \n",
              "2255  I call my story the story of a bad boy, partly...                   20%  \n",
              "2469  Oprah Winfrey has come a long way from her poo...                   20%  \n",
              "2519  _ , by the U.S. education system. Remarkably, ...                   43%  \n",
              "2626  It was Saturday when the entire summer world w...                   30%  \n",
              "2656  Harry Houdini was a man who astonished and att...                   10%  \n",
              "3203  Joe came to New York from the Middle West, dre...                    5%  \n",
              "3256  The children's Theater was actually an old mil...                   24%  \n",
              "3448  On an August afternoon last year, Pamela River...                    5%  \n",
              "3468  Thirty years ago, the Earnshaw family lived at...                   20%  \n",
              "3486  As Amy Hagadorn rounded the corner across the ...                   15%  \n",
              "3765  .\" Those words were some of the last penned by...                   40%  \n",
              "3935  Below are reviews for three books and two book...                   35%  \n",
              "4184  Neil Armstrong, the first man to walk on the m...                   18%  \n",
              "4548  This is VOA. The National Cryptologic Museum i...                   10%  \n",
              "4703  It appears that the police now have a device t...                   10%  \n",
              "5027  The Great Gatsby was not well received when it...                   29%  \n",
              "5260  \"Christmas won't be Christmas without any pres...                   44%  \n",
              "5461  Mr. Hungerton, her father, really was absolute...                   33%  \n",
              "5705  BEIJING --- Since Mo Yan won the Nobel Prize i...                   20%  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4865c7c9-8f1f-4ea1-8f8c-a0d09a84c5a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>passage</th>\n",
              "      <th>truncation_percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>30lb5cdzncau778s2e7bvp843w70za</td>\n",
              "      <td>Billie Holiday was an American jazz singer and...</td>\n",
              "      <td>100%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>32zkvd547fnu6149fn9rb5z8f63b3a</td>\n",
              "      <td>Old Behrman was a painter who lived on the gro...</td>\n",
              "      <td>6%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>678</th>\n",
              "      <td>33ppungg385i71srwrqqfl9rcnyrzt</td>\n",
              "      <td>Scrooge looked around him angrily. It was Chri...</td>\n",
              "      <td>25%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>34hjijklp5wuxbljki5ammllwxf4vs</td>\n",
              "      <td>A disheveled man appeared in court Thursday on...</td>\n",
              "      <td>13%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1111</th>\n",
              "      <td>36wlnqg78zaxgzk647qnuw356vdebo</td>\n",
              "      <td>ROME: The legend of Leonardo da Vinci is cover...</td>\n",
              "      <td>10%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1340</th>\n",
              "      <td>38f71oa9gtwl54ozq702quzzuubmfg</td>\n",
              "      <td>Not so long ago, most people didn't know who S...</td>\n",
              "      <td>30%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1341</th>\n",
              "      <td>38f71oa9gtwl54ozq702quzzuvxmf4</td>\n",
              "      <td>The revolution was over by the time Tom Ford s...</td>\n",
              "      <td>6%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1416</th>\n",
              "      <td>39ghhavomfrl6glp3trrjyar1bu4jv</td>\n",
              "      <td>Old Behrman was a painter who lived on the gro...</td>\n",
              "      <td>20%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1491</th>\n",
              "      <td>39o5d9o87tsdg6wftn5mmp5qxq13c5</td>\n",
              "      <td>Mark Twain has been called the inventor of the...</td>\n",
              "      <td>19%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1738</th>\n",
              "      <td>3befod78w6tb7ora6q4jzq285414m9</td>\n",
              "      <td>Consult the page adapted from an English dicti...</td>\n",
              "      <td>10%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2032</th>\n",
              "      <td>3dy4fpooa1o1yhnhvu1nufwvocgvrl</td>\n",
              "      <td>As a young boy, I sometimes traveled the count...</td>\n",
              "      <td>11%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2255</th>\n",
              "      <td>3f1567xtnw53p9vefe7rx7xt1yzq93</td>\n",
              "      <td>I call my story the story of a bad boy, partly...</td>\n",
              "      <td>20%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2469</th>\n",
              "      <td>3glb5jmzfxvofaehoy7hppchmvldgq</td>\n",
              "      <td>Oprah Winfrey has come a long way from her poo...</td>\n",
              "      <td>20%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2519</th>\n",
              "      <td>3h7xdtshkcrnoge85tc7hd12tb0gwb</td>\n",
              "      <td>_ , by the U.S. education system. Remarkably, ...</td>\n",
              "      <td>43%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2626</th>\n",
              "      <td>3hvvdcpgtesviqve4ut21t17uerytm</td>\n",
              "      <td>It was Saturday when the entire summer world w...</td>\n",
              "      <td>30%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2656</th>\n",
              "      <td>3i02618ya06g9pi2dcnttyux9mzpuc</td>\n",
              "      <td>Harry Houdini was a man who astonished and att...</td>\n",
              "      <td>10%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3203</th>\n",
              "      <td>3leiz60cdjzc31w52aq4o09x60xz9h</td>\n",
              "      <td>Joe came to New York from the Middle West, dre...</td>\n",
              "      <td>5%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3256</th>\n",
              "      <td>3lozaj85yddcymbrgjn4hsl8su42xa</td>\n",
              "      <td>The children's Theater was actually an old mil...</td>\n",
              "      <td>24%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3448</th>\n",
              "      <td>3mtmreqs4vimep15jtkxlrqzvh6wa0</td>\n",
              "      <td>On an August afternoon last year, Pamela River...</td>\n",
              "      <td>5%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3468</th>\n",
              "      <td>3myyfcxhj37bfevovn6omlib9ing4b</td>\n",
              "      <td>Thirty years ago, the Earnshaw family lived at...</td>\n",
              "      <td>20%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3486</th>\n",
              "      <td>3n2bf7y2vqu5j0f5lxo2tfbca9vhmg</td>\n",
              "      <td>As Amy Hagadorn rounded the corner across the ...</td>\n",
              "      <td>15%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3765</th>\n",
              "      <td>3oswbblg1exz1w97d87ldbccplsdxt</td>\n",
              "      <td>.\" Those words were some of the last penned by...</td>\n",
              "      <td>40%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3935</th>\n",
              "      <td>3pmby0ye273zv8lvaw6wd28cxidc9x</td>\n",
              "      <td>Below are reviews for three books and two book...</td>\n",
              "      <td>35%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4184</th>\n",
              "      <td>3r3yrb5grf39mlc0ot5w3352a1juan</td>\n",
              "      <td>Neil Armstrong, the first man to walk on the m...</td>\n",
              "      <td>18%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4548</th>\n",
              "      <td>3tdxmtx3cbu3qs5x4zz64vf5kn76i2</td>\n",
              "      <td>This is VOA. The National Cryptologic Museum i...</td>\n",
              "      <td>10%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4703</th>\n",
              "      <td>3txd01zld4hukwwjfsv5q0j2iqku47</td>\n",
              "      <td>It appears that the police now have a device t...</td>\n",
              "      <td>10%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5027</th>\n",
              "      <td>3vw04l3zlt6dz2eo488x7if453nxxy</td>\n",
              "      <td>The Great Gatsby was not well received when it...</td>\n",
              "      <td>29%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5260</th>\n",
              "      <td>3wz36bjev3gz5i23u2fiti3694ctb3</td>\n",
              "      <td>\"Christmas won't be Christmas without any pres...</td>\n",
              "      <td>44%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5461</th>\n",
              "      <td>3yhh42uu5bfa2irondg2nax6oodl0s</td>\n",
              "      <td>Mr. Hungerton, her father, really was absolute...</td>\n",
              "      <td>33%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5705</th>\n",
              "      <td>3zr9aiqjub9e4ak3hlhl1tvv22004w</td>\n",
              "      <td>BEIJING --- Since Mo Yan won the Nobel Prize i...</td>\n",
              "      <td>20%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4865c7c9-8f1f-4ea1-8f8c-a0d09a84c5a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4865c7c9-8f1f-4ea1-8f8c-a0d09a84c5a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4865c7c9-8f1f-4ea1-8f8c-a0d09a84c5a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "id": "9SJddkD-mVzM"
    },
    {
      "cell_type": "code",
      "source": [
        "df_truncation_val = show_truncation_percentage(df_val, dict_passage_questions_val)\n",
        "df_truncation_val[df_truncation_val['truncation_percentage'] != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Kh3S3WlxpIB1",
        "outputId": "29545d15-5f55-4c1e-95d0-20f25a8625ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  id  \\\n",
              "39    3137onmdkg5t7gshkti1v7u2m7uegs   \n",
              "223   34z02eimisdylvztwmit917ni1j0tu   \n",
              "512   3dbqwde4y6yzlpgaww2thxxmaq55ne   \n",
              "788   3kibxj1wd5uklt1p4y6cybg9xrnkod   \n",
              "882   3mx2nq3yc9u4xjuey2p2fzokc4y5xh   \n",
              "1013  3q5c1wp23m1w7i9lr94vkqkj7p1517   \n",
              "1141  3strjbfxowr0yl6x0fsbslmwwz5kt0   \n",
              "1200  3u0srxb7cd5oqce8t3fwky2i28trnd   \n",
              "1294  3wleiwsyhohfcwbcbf5ie6xe45ch2p   \n",
              "1353  3y5140z9dxgb0yn2jvyfav6mfrupim   \n",
              "\n",
              "                                                passage truncation_percentage  \n",
              "39    Tom appeared on the sidewalk with a bucket of ...                   20%  \n",
              "223   Colleges taking another look at value of merit...                   26%  \n",
              "512   It never occurred to Sun Yukun that the decisi...                    6%  \n",
              "788   Buck did not read the newspapers,or he would h...                   11%  \n",
              "882   After much thought,I came up with a brilliant ...                   30%  \n",
              "1013  Doctor Manette had suddenly disappeared. Every...                   18%  \n",
              "1141  The Chinese put up with a lot living in the wo...                    5%  \n",
              "1200  Caught in the Web A few months ago, it wasn't ...                   46%  \n",
              "1294  What Is Today's American Dream \\n\\nThey may no...                   40%  \n",
              "1353  British writer John Bunyan was born at Elstow,...                   10%  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab273b29-da53-445d-ac68-9c72fb9ca4a2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>passage</th>\n",
              "      <th>truncation_percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>3137onmdkg5t7gshkti1v7u2m7uegs</td>\n",
              "      <td>Tom appeared on the sidewalk with a bucket of ...</td>\n",
              "      <td>20%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>34z02eimisdylvztwmit917ni1j0tu</td>\n",
              "      <td>Colleges taking another look at value of merit...</td>\n",
              "      <td>26%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>3dbqwde4y6yzlpgaww2thxxmaq55ne</td>\n",
              "      <td>It never occurred to Sun Yukun that the decisi...</td>\n",
              "      <td>6%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>788</th>\n",
              "      <td>3kibxj1wd5uklt1p4y6cybg9xrnkod</td>\n",
              "      <td>Buck did not read the newspapers,or he would h...</td>\n",
              "      <td>11%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>882</th>\n",
              "      <td>3mx2nq3yc9u4xjuey2p2fzokc4y5xh</td>\n",
              "      <td>After much thought,I came up with a brilliant ...</td>\n",
              "      <td>30%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1013</th>\n",
              "      <td>3q5c1wp23m1w7i9lr94vkqkj7p1517</td>\n",
              "      <td>Doctor Manette had suddenly disappeared. Every...</td>\n",
              "      <td>18%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1141</th>\n",
              "      <td>3strjbfxowr0yl6x0fsbslmwwz5kt0</td>\n",
              "      <td>The Chinese put up with a lot living in the wo...</td>\n",
              "      <td>5%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1200</th>\n",
              "      <td>3u0srxb7cd5oqce8t3fwky2i28trnd</td>\n",
              "      <td>Caught in the Web A few months ago, it wasn't ...</td>\n",
              "      <td>46%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1294</th>\n",
              "      <td>3wleiwsyhohfcwbcbf5ie6xe45ch2p</td>\n",
              "      <td>What Is Today's American Dream \\n\\nThey may no...</td>\n",
              "      <td>40%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1353</th>\n",
              "      <td>3y5140z9dxgb0yn2jvyfav6mfrupim</td>\n",
              "      <td>British writer John Bunyan was born at Elstow,...</td>\n",
              "      <td>10%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab273b29-da53-445d-ac68-9c72fb9ca4a2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab273b29-da53-445d-ac68-9c72fb9ca4a2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab273b29-da53-445d-ac68-9c72fb9ca4a2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "id": "Kh3S3WlxpIB1"
    },
    {
      "cell_type": "code",
      "source": [
        "df_truncation_test = show_truncation_percentage(df_test, dict_passage_questions_test)\n",
        "df_truncation_test[df_truncation_test['truncation_percentage'] != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EuuluwFZpIdS",
        "outputId": "5ecf5083-d85a-4da6-ffb6-78aeaee728a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 id  \\\n",
              "36   33cusnvvnncx50c8oskdbkhinmz88p   \n",
              "57   34z02eimisdylvztwmit917ni020tb   \n",
              "94   37m28k1j0qd08516cu1iw1wrtkcajw   \n",
              "314  3p59jyt76lk5h527b9m7sp02fzkt2o   \n",
              "431  3w92k5rlwuhctupjynokrerzwu05vq   \n",
              "\n",
              "                                               passage truncation_percentage  \n",
              "36   My trip with my sister and my daughter to Manh...                   10%  \n",
              "57   When I was young, I went looking for gold in C...                    5%  \n",
              "94   As we drove along, my spirits went up again, a...                   20%  \n",
              "314  Register in person, by phone 264-8833, or by m...                   10%  \n",
              "431  The Board Meeting had come to an end. Bob star...                   25%  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0240af1e-5425-4615-8511-e74dacf07f57\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>passage</th>\n",
              "      <th>truncation_percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>33cusnvvnncx50c8oskdbkhinmz88p</td>\n",
              "      <td>My trip with my sister and my daughter to Manh...</td>\n",
              "      <td>10%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>34z02eimisdylvztwmit917ni020tb</td>\n",
              "      <td>When I was young, I went looking for gold in C...</td>\n",
              "      <td>5%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>37m28k1j0qd08516cu1iw1wrtkcajw</td>\n",
              "      <td>As we drove along, my spirits went up again, a...</td>\n",
              "      <td>20%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>3p59jyt76lk5h527b9m7sp02fzkt2o</td>\n",
              "      <td>Register in person, by phone 264-8833, or by m...</td>\n",
              "      <td>10%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>3w92k5rlwuhctupjynokrerzwu05vq</td>\n",
              "      <td>The Board Meeting had come to an end. Bob star...</td>\n",
              "      <td>25%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0240af1e-5425-4615-8511-e74dacf07f57')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0240af1e-5425-4615-8511-e74dacf07f57 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0240af1e-5425-4615-8511-e74dacf07f57');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "id": "EuuluwFZpIdS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's also meaningful to observe that among the $25$ test passages with the worst SQuAD F1-score, according to DistilRoBERTa performances, for two of them the truncation cut out the rationale of at least one of their Q&A pairs."
      ],
      "metadata": {
        "id": "KxNaQgVYf5zD"
      },
      "id": "KxNaQgVYf5zD"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Among the 25 test passages with the worst SQuAD F1-score, for\",\n",
        "      list(set(dict_passage_questions_test.keys()).intersection(list(worst_5_passages['id']))),\n",
        "      \"ones\\nthe truncation cut out the rationale of at least one of their Q&A pairs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHC_LcqIzWZp",
        "outputId": "109b71c1-1423-49f2-f1d4-a76ba920c9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Among the 25 test passages with the worst SQuAD F1-score, for ['3p59jyt76lk5h527b9m7sp02fzkt2o', '37m28k1j0qd08516cu1iw1wrtkcajw']\n",
            " ones the truncation cut out the rationale of at least one of their Q&A pairs.\n"
          ]
        }
      ],
      "id": "YHC_LcqIzWZp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's see how models perform on truncated passages."
      ],
      "metadata": {
        "id": "v0ENavT4giaR"
      },
      "id": "v0ENavT4giaR"
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_f1_truncation(df_truncation, results):\n",
        "\n",
        "  \"\"\"\n",
        "  For each passage contained in `df_truncation` computes the SQuAD F1-score, according to `results`.\n",
        "  \"\"\"\n",
        "\n",
        "  df_grouped_results = results.groupby(['id'], as_index=False).mean().drop(['rationale_end', '__index_level_0__'], axis = 1)\n",
        "  df_merged = pd.merge(df_truncation, df_grouped_results, how='left', on = 'id')\n",
        "\n",
        "  return df_merged"
      ],
      "metadata": {
        "id": "fHq7KvIEFLzq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fHq7KvIEFLzq"
    },
    {
      "cell_type": "code",
      "source": [
        "df_f1_truncation = correlation_f1_truncation(df_truncation_test, bert2bert_results)\n",
        "df_f1_truncation[df_f1_truncation['truncation_percentage'] != 0.0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jZMetdh8Okbm",
        "outputId": "f1505982-4748-4482-fe66-76cedd74738c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 id  \\\n",
              "36   33cusnvvnncx50c8oskdbkhinmz88p   \n",
              "57   34z02eimisdylvztwmit917ni020tb   \n",
              "94   37m28k1j0qd08516cu1iw1wrtkcajw   \n",
              "314  3p59jyt76lk5h527b9m7sp02fzkt2o   \n",
              "431  3w92k5rlwuhctupjynokrerzwu05vq   \n",
              "\n",
              "                                               passage truncation_percentage  \\\n",
              "36   My trip with my sister and my daughter to Manh...                   10%   \n",
              "57   When I was young, I went looking for gold in C...                    5%   \n",
              "94   As we drove along, my spirits went up again, a...                   20%   \n",
              "314  Register in person, by phone 264-8833, or by m...                   10%   \n",
              "431  The Board Meeting had come to an end. Bob star...                   25%   \n",
              "\n",
              "     f1_squad  \n",
              "36   0.050000  \n",
              "57   0.315914  \n",
              "94   0.074286  \n",
              "314  0.137500  \n",
              "431  0.332692  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-933a5397-e9ac-4e53-acf6-495d7ad4ecfd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>passage</th>\n",
              "      <th>truncation_percentage</th>\n",
              "      <th>f1_squad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>33cusnvvnncx50c8oskdbkhinmz88p</td>\n",
              "      <td>My trip with my sister and my daughter to Manh...</td>\n",
              "      <td>10%</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>34z02eimisdylvztwmit917ni020tb</td>\n",
              "      <td>When I was young, I went looking for gold in C...</td>\n",
              "      <td>5%</td>\n",
              "      <td>0.315914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>37m28k1j0qd08516cu1iw1wrtkcajw</td>\n",
              "      <td>As we drove along, my spirits went up again, a...</td>\n",
              "      <td>20%</td>\n",
              "      <td>0.074286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>3p59jyt76lk5h527b9m7sp02fzkt2o</td>\n",
              "      <td>Register in person, by phone 264-8833, or by m...</td>\n",
              "      <td>10%</td>\n",
              "      <td>0.137500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>3w92k5rlwuhctupjynokrerzwu05vq</td>\n",
              "      <td>The Board Meeting had come to an end. Bob star...</td>\n",
              "      <td>25%</td>\n",
              "      <td>0.332692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-933a5397-e9ac-4e53-acf6-495d7ad4ecfd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-933a5397-e9ac-4e53-acf6-495d7ad4ecfd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-933a5397-e9ac-4e53-acf6-495d7ad4ecfd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "id": "jZMetdh8Okbm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTTiny seems to not be affected from truncation. A possible explanation is that it rarely predicts passage-related answers, thus the impact of truncation is less evident."
      ],
      "metadata": {
        "id": "AXjoZFpwhFTU"
      },
      "id": "AXjoZFpwhFTU"
    },
    {
      "cell_type": "code",
      "source": [
        "df_f1_truncation = correlation_f1_truncation(df_truncation_test, distilroberta_results)\n",
        "df_f1_truncation[df_f1_truncation['truncation_percentage'] != 0.0]"
      ],
      "metadata": {
        "id": "rKVqEIBmO3SY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "3e862625-7077-4776-9769-629e35b9dbb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 id  \\\n",
              "36   33cusnvvnncx50c8oskdbkhinmz88p   \n",
              "57   34z02eimisdylvztwmit917ni020tb   \n",
              "94   37m28k1j0qd08516cu1iw1wrtkcajw   \n",
              "314  3p59jyt76lk5h527b9m7sp02fzkt2o   \n",
              "431  3w92k5rlwuhctupjynokrerzwu05vq   \n",
              "\n",
              "                                               passage truncation_percentage  \\\n",
              "36   My trip with my sister and my daughter to Manh...                   10%   \n",
              "57   When I was young, I went looking for gold in C...                    5%   \n",
              "94   As we drove along, my spirits went up again, a...                   20%   \n",
              "314  Register in person, by phone 264-8833, or by m...                   10%   \n",
              "431  The Board Meeting had come to an end. Bob star...                   25%   \n",
              "\n",
              "     f1_squad  \n",
              "36   0.353571  \n",
              "57   0.541126  \n",
              "94   0.155952  \n",
              "314  0.315000  \n",
              "431  0.469048  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3daadaa9-c3d0-4ac0-a88d-166930a3ad32\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>passage</th>\n",
              "      <th>truncation_percentage</th>\n",
              "      <th>f1_squad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>33cusnvvnncx50c8oskdbkhinmz88p</td>\n",
              "      <td>My trip with my sister and my daughter to Manh...</td>\n",
              "      <td>10%</td>\n",
              "      <td>0.353571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>34z02eimisdylvztwmit917ni020tb</td>\n",
              "      <td>When I was young, I went looking for gold in C...</td>\n",
              "      <td>5%</td>\n",
              "      <td>0.541126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>37m28k1j0qd08516cu1iw1wrtkcajw</td>\n",
              "      <td>As we drove along, my spirits went up again, a...</td>\n",
              "      <td>20%</td>\n",
              "      <td>0.155952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>3p59jyt76lk5h527b9m7sp02fzkt2o</td>\n",
              "      <td>Register in person, by phone 264-8833, or by m...</td>\n",
              "      <td>10%</td>\n",
              "      <td>0.315000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>3w92k5rlwuhctupjynokrerzwu05vq</td>\n",
              "      <td>The Board Meeting had come to an end. Bob star...</td>\n",
              "      <td>25%</td>\n",
              "      <td>0.469048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3daadaa9-c3d0-4ac0-a88d-166930a3ad32')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3daadaa9-c3d0-4ac0-a88d-166930a3ad32 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3daadaa9-c3d0-4ac0-a88d-166930a3ad32');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "id": "rKVqEIBmO3SY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "DistilRoBERTa instead is a bit influenced by truncation problems. Indeed, on those test passages whose truncation cut out the rationale of at least one of the Q&A pairs, it obtains an average SQuAD F1-score way lower than the one computed on the whole test set."
      ],
      "metadata": {
        "id": "jhFsJTKahjpH"
      },
      "id": "jhFsJTKahjpH"
    },
    {
      "cell_type": "code",
      "source": [
        "def show_truncated_passage(results, id_passage, dict_passage_questions):\n",
        "\n",
        "  \"\"\"\n",
        "  For each passage in `dict_passage_questions`, it shows the Q&A pairs whose rationale suffers from truncation.\n",
        "  \"\"\"\n",
        "\n",
        "  questions = dict_passage_questions.get(id_passage)\n",
        "  passage = results[results['id']==id_passage]\n",
        "\n",
        "  print(f\"PASSAGE:\\n\")\n",
        "  print(f\"{list(passage['passage'])[0]}\")\n",
        "  print()\n",
        "  print(\"TRUNCATED CONVERSATION:\\n\")\n",
        "\n",
        "  for q in questions:\n",
        "\n",
        "    i = passage.index[passage['question'] == q].tolist()[0]\n",
        "    question = passage['question'][i]\n",
        "    answer = passage['answer'][i]\n",
        "    pred_answer = passage['pred_answer'][i]\n",
        "    rationale = passage['rationale'][i]\n",
        "\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer}\")\n",
        "    print(f\"R: {rationale}\")\n",
        "    print(f\"PA: {pred_answer}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "hUTGWX-DPCbB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hUTGWX-DPCbB"
    },
    {
      "cell_type": "code",
      "source": [
        "show_truncated_passage(pd.DataFrame(bert2bert_results), '3p59jyt76lk5h527b9m7sp02fzkt2o', dict_passage_questions_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZFXVxRPPDF8",
        "outputId": "447650cd-2f4c-456d-a90e-676b03c8037d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Register in person, by phone 264-8833, or by mail. Use form given. \n",
            "\n",
            "178 IN Winchester St., Chicago \n",
            "\n",
            "Basic Photography This is an eight-hour course for beginners who want to learn how to use a 35mm camera. The teacher will cover such areas as kinds of film, light and lenses . Bring your own 35mm camera to class. Course charge: $50.Jan. 10,12,17,19, Tues & Thurs. 6:00-8:00 pm. Marianne Adams is a professional photographer whose photographs appear in many magazines. \n",
            "\n",
            "Understanding Computers This twelve-hour course is for people who do not know much about computers, but need to learn about them. You will learn what computers are, what they can and can't do, and how to use them. Course charge: $75. Equipment charge: $10. Jan.14, 21, 28, Sats. 7:00-10:30pm. Joseph Saimders is Professor of Computer Science at New Urban University. He has over twelve years of experience in the computer field. \n",
            "\n",
            "Stop Smoking Do you want to stop smoking? Have you already tried to stop and failed? Now it's the time to stop smoking using the latest methods. You can stop smoking, and this twelve-hour course will help you do it. Course charge: $30. Jan. 4, 11, 18, 25, Wedns. 4:00-7:00pm. Dr John Goode is a practicing psychologist who has helped hundreds of people stop smoking. \n",
            "\n",
            "Typing This course on week-days is for those who want to learn to type, as well as those who want to improve their typing. You are tested in the first class and practice at one of eight different skill levels. This allows you to learn at your own speed. Each program lasts 20 hours. Bring your own paper. Course charge: $125. Material charge: $25. Two hours each evening for two weeks. New classes begin every two weeks. This course is taught by a number of business education teachers who have successfully taught typing courses before. \n",
            "\n",
            "Oil painting Oil paint is easy to use once you learn the basics. When you enroll at this oil painting course, you will learn to draw and learn to paint using many oil painting techniques under complete guidance and instruction. Together with the teacher's knowledge and your passion-we'll unlock your creativity and develop your potential! Course charge: $35. Jan. 5, 12, 19, 26, Thurs. 2:00-5:00 pm. Ralf Ericssion has taught beginners to masters and he has learned that everything builds on just a few basic concepts that he will show you here. \n",
            "\n",
            "Singing This course shows you how to deliver an accomplished vocal performance on stage and in the studio. Develop your vocal talents with professional warm-up routines and learn vocal techniques to gain confidence in your performance. You'll learn to perform classic songs before exploring your own songwriting ideas with a tutor. And finally you'll get the chance to record in a professional studio. Singing tuition may be in groups or one-to-one. We have Choral singing, Gospel singing, Folk singing and many other styles of song. All styles are welcome and no previous experience is required. Please read on for course contents and reviews from our students. Course charge: $90. Jan. 10, 12, 17, 19, Tues. & Thurs. 5:30-8:30pm. Peter Syrus is a Grammy award winning tutor.\n",
            "\n",
            "TRUNCATED CONVERSATION:\n",
            "\n",
            "Q: Who teaches the singing course?\n",
            "A: Peter Syrus\n",
            "R: Peter Syrus \n",
            "PA: a student teacher\n",
            "\n",
            "Q: What are the dates of that course?\n",
            "A: Jan. 10, 12, 17, 19,\n",
            "R: Jan. 10, 12, 17, 19,\n",
            "PA: a computer system\n",
            "\n"
          ]
        }
      ],
      "id": "CZFXVxRPPDF8"
    },
    {
      "cell_type": "code",
      "source": [
        "show_truncated_passage(pd.DataFrame(distilroberta_results), '3p59jyt76lk5h527b9m7sp02fzkt2o', dict_passage_questions_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9R-XupK8mE3",
        "outputId": "57b4e76a-04db-4c2a-b6b7-51c83156f770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "Register in person, by phone 264-8833, or by mail. Use form given. \n",
            "\n",
            "178 IN Winchester St., Chicago \n",
            "\n",
            "Basic Photography This is an eight-hour course for beginners who want to learn how to use a 35mm camera. The teacher will cover such areas as kinds of film, light and lenses . Bring your own 35mm camera to class. Course charge: $50.Jan. 10,12,17,19, Tues & Thurs. 6:00-8:00 pm. Marianne Adams is a professional photographer whose photographs appear in many magazines. \n",
            "\n",
            "Understanding Computers This twelve-hour course is for people who do not know much about computers, but need to learn about them. You will learn what computers are, what they can and can't do, and how to use them. Course charge: $75. Equipment charge: $10. Jan.14, 21, 28, Sats. 7:00-10:30pm. Joseph Saimders is Professor of Computer Science at New Urban University. He has over twelve years of experience in the computer field. \n",
            "\n",
            "Stop Smoking Do you want to stop smoking? Have you already tried to stop and failed? Now it's the time to stop smoking using the latest methods. You can stop smoking, and this twelve-hour course will help you do it. Course charge: $30. Jan. 4, 11, 18, 25, Wedns. 4:00-7:00pm. Dr John Goode is a practicing psychologist who has helped hundreds of people stop smoking. \n",
            "\n",
            "Typing This course on week-days is for those who want to learn to type, as well as those who want to improve their typing. You are tested in the first class and practice at one of eight different skill levels. This allows you to learn at your own speed. Each program lasts 20 hours. Bring your own paper. Course charge: $125. Material charge: $25. Two hours each evening for two weeks. New classes begin every two weeks. This course is taught by a number of business education teachers who have successfully taught typing courses before. \n",
            "\n",
            "Oil painting Oil paint is easy to use once you learn the basics. When you enroll at this oil painting course, you will learn to draw and learn to paint using many oil painting techniques under complete guidance and instruction. Together with the teacher's knowledge and your passion-we'll unlock your creativity and develop your potential! Course charge: $35. Jan. 5, 12, 19, 26, Thurs. 2:00-5:00 pm. Ralf Ericssion has taught beginners to masters and he has learned that everything builds on just a few basic concepts that he will show you here. \n",
            "\n",
            "Singing This course shows you how to deliver an accomplished vocal performance on stage and in the studio. Develop your vocal talents with professional warm-up routines and learn vocal techniques to gain confidence in your performance. You'll learn to perform classic songs before exploring your own songwriting ideas with a tutor. And finally you'll get the chance to record in a professional studio. Singing tuition may be in groups or one-to-one. We have Choral singing, Gospel singing, Folk singing and many other styles of song. All styles are welcome and no previous experience is required. Please read on for course contents and reviews from our students. Course charge: $90. Jan. 10, 12, 17, 19, Tues. & Thurs. 5:30-8:30pm. Peter Syrus is a Grammy award winning tutor.\n",
            "\n",
            "TRUNCATED CONVERSATION:\n",
            "\n",
            "Q: Who teaches the singing course?\n",
            "A: Peter Syrus\n",
            "R: Peter Syrus \n",
            "PA: Dr. John Goode\n",
            "\n",
            "Q: What are the dates of that course?\n",
            "A: Jan. 10, 12, 17, 19,\n",
            "R: Jan. 10, 12, 17, 19,\n",
            "PA: Jan. 14 and Jan. 14\n",
            "\n"
          ]
        }
      ],
      "id": "b9R-XupK8mE3"
    },
    {
      "cell_type": "code",
      "source": [
        "show_truncated_passage(pd.DataFrame(bert2bert_results), '37m28k1j0qd08516cu1iw1wrtkcajw', dict_passage_questions_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D1p7BiZ8f6W",
        "outputId": "41019d73-caf6-4930-d25f-13747a9d412a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "As we drove along, my spirits went up again, and I turned, with pleasure, to the thought of the new life which I was entering. But though it was not far past the middle of September, the heavy clouds and strong north-easterly wind combined to make the day extremely cold; and the journey seemed a very long one, so that it was nearly one o'clock before we reached the place of our destination. Yet when we entered the gateway, my heart failed me, and I wished it were a mile or two farther off. For the first time in my life I must stand alone: there was no retreating now. I must enter that house, and introduce myself among its strange people. But how was it to be done? True, I was near nineteen; but, thanks to the protecting care of my mother and sister, I well knew that many a girl of fifteen, or under, was gifted with a more womanly address, and greater ease and self-possession, than I was. Yet, anyway. I would do very well, after all; and the children, of course, I should soon be at ease with them. \n",
            "\n",
            "\"Be calm, be calm, whatever happens,\" I said within myself; and truly I was so fully occupied in steadying my nerves and keeping down the rebellious beat of my heart that when I was admitted into the hall and into the presence of Mrs. Bloomfield, I almost forgot to answer her polite greeting; and it afterwards struck me that the little I did say was spoken in the tone of one half-dead or half-asleep. \n",
            "\n",
            "With due politeness, however, she showed me my bedroom, and left me there to take a little refreshment for a little while and led me into the dining-room. Some beefsteaks and potatoes were set before me; and while I dined upon these, she sat opposite, watching me (as I thought) and trying to keep something like a conversation-- consisting chiefly of commonplace remarks. In fact, my attention was almost wholly absorbed in my dinner: not from appetite, but from the toughness of the beefsteaks, and the numbness of my hands. \n",
            "\n",
            "\"I have had so little time to attend to their education myself, but I think they are clever children, and very willing to learn, especially the little boy; he is, I think, the flower of the flock-- a generous, noble-spirited boy, one to be led, but not driven, and remarkable for always speaking the truth.\" \"His sister Mary Ann will require watching,\" continued she, \"but she is a very good girl on the whole, though I wish her to be kept out of the nursery as much as possible, as she is now almost six years old, and might acquire bad habits from the nurses. I have ordered her bed to be placed in your room, and if you will be so kind as to look after her washing and dressing, and take charge of her clothes, she needs to have nothing further to do with the nursery maid.\" \n",
            "\n",
            "I replied I was quite willing to do so; and at that moment the children entered the room. Tom Bloomfield was a well-grown boy of seven. Mary was a tall girl, for her age of six, somewhat dark like her mother. The second sister was Fanny, a very pretty little girl, looking little younger than Mary. The remaining one was Harriet, a little broad, fat, merry, playful thing of scarcely two, whom I had more desire for than all the rest -- but with her I had nothing to do.\n",
            "\n",
            "TRUNCATED CONVERSATION:\n",
            "\n",
            "Q: How many?\n",
            "A: four\n",
            "R: Tom Bloomfield was a well-grown boy of seven. Mary was a tall girl, for her age of six, somewhat dark like her mother. The second sister was Fanny, a very pretty little girl, looking little younger than Mary. The remaining one was Harriet\n",
            "PA: three\n",
            "\n",
            "Q: Does the story tell you who the youngest was?\n",
            "A: Yes\n",
            "R: he remaining one was Harriet, a little broad, fat, merry, playful thing of scarcely two,\n",
            "PA: yes\n",
            "\n",
            "Q: who?\n",
            "A: Harriet\n",
            "R: The remaining one was Harriet, a little broad, fat, merry, playful thing of scarcely two\n",
            "PA: his mother\n",
            "\n",
            "Q: and seven?\n",
            "A: Tom\n",
            "R: Tom Bloomfield was a well-grown boy of seven\n",
            "PA: no\n",
            "\n"
          ]
        }
      ],
      "id": "3D1p7BiZ8f6W"
    },
    {
      "cell_type": "code",
      "source": [
        "show_truncated_passage(pd.DataFrame(distilroberta_results), '37m28k1j0qd08516cu1iw1wrtkcajw', dict_passage_questions_test)"
      ],
      "metadata": {
        "id": "608vY1H04kAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc58a52-d95d-456b-d8af-ca8e757e2178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSAGE:\n",
            "\n",
            "As we drove along, my spirits went up again, and I turned, with pleasure, to the thought of the new life which I was entering. But though it was not far past the middle of September, the heavy clouds and strong north-easterly wind combined to make the day extremely cold; and the journey seemed a very long one, so that it was nearly one o'clock before we reached the place of our destination. Yet when we entered the gateway, my heart failed me, and I wished it were a mile or two farther off. For the first time in my life I must stand alone: there was no retreating now. I must enter that house, and introduce myself among its strange people. But how was it to be done? True, I was near nineteen; but, thanks to the protecting care of my mother and sister, I well knew that many a girl of fifteen, or under, was gifted with a more womanly address, and greater ease and self-possession, than I was. Yet, anyway. I would do very well, after all; and the children, of course, I should soon be at ease with them. \n",
            "\n",
            "\"Be calm, be calm, whatever happens,\" I said within myself; and truly I was so fully occupied in steadying my nerves and keeping down the rebellious beat of my heart that when I was admitted into the hall and into the presence of Mrs. Bloomfield, I almost forgot to answer her polite greeting; and it afterwards struck me that the little I did say was spoken in the tone of one half-dead or half-asleep. \n",
            "\n",
            "With due politeness, however, she showed me my bedroom, and left me there to take a little refreshment for a little while and led me into the dining-room. Some beefsteaks and potatoes were set before me; and while I dined upon these, she sat opposite, watching me (as I thought) and trying to keep something like a conversation-- consisting chiefly of commonplace remarks. In fact, my attention was almost wholly absorbed in my dinner: not from appetite, but from the toughness of the beefsteaks, and the numbness of my hands. \n",
            "\n",
            "\"I have had so little time to attend to their education myself, but I think they are clever children, and very willing to learn, especially the little boy; he is, I think, the flower of the flock-- a generous, noble-spirited boy, one to be led, but not driven, and remarkable for always speaking the truth.\" \"His sister Mary Ann will require watching,\" continued she, \"but she is a very good girl on the whole, though I wish her to be kept out of the nursery as much as possible, as she is now almost six years old, and might acquire bad habits from the nurses. I have ordered her bed to be placed in your room, and if you will be so kind as to look after her washing and dressing, and take charge of her clothes, she needs to have nothing further to do with the nursery maid.\" \n",
            "\n",
            "I replied I was quite willing to do so; and at that moment the children entered the room. Tom Bloomfield was a well-grown boy of seven. Mary was a tall girl, for her age of six, somewhat dark like her mother. The second sister was Fanny, a very pretty little girl, looking little younger than Mary. The remaining one was Harriet, a little broad, fat, merry, playful thing of scarcely two, whom I had more desire for than all the rest -- but with her I had nothing to do.\n",
            "\n",
            "TRUNCATED CONVERSATION:\n",
            "\n",
            "Q: How many?\n",
            "A: four\n",
            "R: Tom Bloomfield was a well-grown boy of seven. Mary was a tall girl, for her age of six, somewhat dark like her mother. The second sister was Fanny, a very pretty little girl, looking little younger than Mary. The remaining one was Harriet\n",
            "PA: fifteen\n",
            "\n",
            "Q: Does the story tell you who the youngest was?\n",
            "A: Yes\n",
            "R: he remaining one was Harriet, a little broad, fat, merry, playful thing of scarcely two,\n",
            "PA: no\n",
            "\n",
            "Q: who?\n",
            "A: Harriet\n",
            "R: The remaining one was Harriet, a little broad, fat, merry, playful thing of scarcely two\n",
            "PA: her mother\n",
            "\n",
            "Q: and seven?\n",
            "A: Tom\n",
            "R: Tom Bloomfield was a well-grown boy of seven\n",
            "PA: no\n",
            "\n"
          ]
        }
      ],
      "id": "608vY1H04kAl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, it's evident that both models, despite huge differences in performances, really struggle when trying to predict an answer whose rationale has been cut because of truncation."
      ],
      "metadata": {
        "id": "6j4kozD5iNbO"
      },
      "id": "6j4kozD5iNbO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations and improvements\n",
        "\n",
        "In general, performances seem to be far from great, especially BERTTiny's. However, neither the models versions receiving as input the rationale instead of the passage can reach really high performances, as a proof that generative question answering is really an hard task.\n",
        "\n",
        "Future works might focus on the use of larger models, that, even if more computational costly, might lead to better performances."
      ],
      "metadata": {
        "id": "Q4-eHETVi-os"
      },
      "id": "Q4-eHETVi-os"
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "collapsed_sections": [
        "11ada8c8",
        "66cfee64",
        "d26d68b7",
        "UEEVjcYD3tnD",
        "YBnRBeC5NUEQ",
        "40e42311",
        "92kAux4-gqG5",
        "VAqg1ZT7Msrg",
        "Aq0CvJFukyha",
        "eHrBUUP14zqI",
        "LUOqCeaztQF6",
        "cJpiUbwYRxqN",
        "2HIgfC44Ei4K",
        "GMkVJfxAElJa",
        "9fzyGGfMEnE-",
        "q2GIvfwbL_MA",
        "b0HzTWLjL_MB",
        "hUl5KjgNNpdd",
        "jhWlr0yUNpsi",
        "KVPiSEhEPhTy",
        "Zyw9ky-qqEAO",
        "6FpNby15vLRc",
        "9OKP5RbRQGQk",
        "eUTk_S79QGQl",
        "hn6ocq8gQuwc",
        "5s_CrbQzQu9R",
        "7jZutYVkQGQp",
        "kdSbTQanQGQp",
        "KVOfcErPRWvo",
        "-B4rZj6yRW3Z",
        "THzdUvRCQGQt",
        "TLI7HQCqcT05",
        "-fklSymEsbin",
        "WAJftVjk74dy",
        "pPOl6I28rFMl"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}